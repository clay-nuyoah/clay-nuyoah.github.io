<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Transformer</title>
    <url>/2021/07/11/Transformer/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/12/10-29-28-c9a0a3715bc67b36854c12ff8ab42c7c-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210712102914-14187c.png"></p>
<a id="more"></a>

<h1 id="0、前置知识"><a href="#0、前置知识" class="headerlink" title="0、前置知识"></a>0、前置知识</h1><h2 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h2><p>Seq2Seq（是 Sequence-to-sequence 的缩写），就如字面意思，输入一个序列，输出另一个序列。这种结构最重要的地方在于输入序列和输出序列的长度是可变的。Seq2Seq 也不特指具体方法，满足「输入序列、输出序列」的目的，都可以统称为 Seq2Seq 模型。</p>
<h2 id="Encoder-Decoder"><a href="#Encoder-Decoder" class="headerlink" title="Encoder-Decoder"></a>Encoder-Decoder</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/pic1.zhimg.com/80/2021/07/11/19-50-00-9b69bc95acbc87b96043bd04da582459-v2-d84b05caae8a93164e92a1c3b7f998bc_1440w-fc8997.jpeg"></p>
<p>Decoder 的通用的框架如上</p>
<pre><code>Encoder 又称作编码器。它的作用就是「将现实问题转化为数学问题」
Decoder 又称作解码器，他的作用是「求解数学问题，并转化为现实世界的解决方案」</code></pre>
<p>Encoder-Decoder 这个框架很好的诠释了机器学习的核心思路</p>
<pre><code>将现实问题转化为数学问题，通过求解数学问题，从而解决现实问题</code></pre>
<p>其中需要注意的是：</p>
<pre><code>1、不论输入和输出的长度是什么，中间的向量 C 长度都是固定的
2、根据不同的任务可以选择不同的编码器和解码器，Encoder-Decoder 两部分都可以在 RNN、LSTM、GRU 模型中任意挑选。</code></pre>
<h2 id="Encoder-Decoder-和-Seq2Seq-的区别："><a href="#Encoder-Decoder-和-Seq2Seq-的区别：" class="headerlink" title="Encoder-Decoder 和 Seq2Seq 的区别："></a>Encoder-Decoder 和 Seq2Seq 的区别：</h2><pre><code>1、Seq2seq 是应用层的概念，即序列到序列，强调应用场景。
2、Encoder-decoder 是网络架构层面的概念，是现在主流框架，特指同时具有 encoder 模块和 decode 模块的结构。
3、encoder-decoder 模型是一种应用于 seq2seq 问题的模型。
4、目前，Seq2Seq 使用的具体方法基本都属于 Encoder-Decoder 模型的范畴</code></pre>
<h1 id="1、Transformer-模型结构"><a href="#1、Transformer-模型结构" class="headerlink" title="1、Transformer 模型结构"></a>1、Transformer 模型结构</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/19-54-16-908c1dbbdbec9473e697311ffd10c9c6-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210711193825-da9da9.png"></p>
<p>Transformer 可以看作是一个 Encoder-Decoder 模型，黑框表示出了整个 Transformer 的模型结构，在红线左侧是 Encoder 部分，在红线的右侧是 Decoder 部分。</p>
<h1 id="2、Encoder-部分"><a href="#2、Encoder-部分" class="headerlink" title="2、Encoder 部分"></a>2、Encoder 部分</h1><h2 id="Encoder-结构"><a href="#Encoder-结构" class="headerlink" title="Encoder 结构"></a>Encoder 结构</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-00-41-a068fcf4f4e46a088d1a5ad120dd2b96-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210711195749-dc68f6.png"></p>
<p>这里需要注意到两个地方，一个是红框框出来的地方，这里说明 Transformer 是由多个该结构组成的，每个该结构称为一个 block。黄框框出来的地方说明在 Transformer 中加入了位置信息。</p>
<h2 id="执行过程"><a href="#执行过程" class="headerlink" title="执行过程"></a>执行过程</h2><h2 id="Self-Attention"><a href="#Self-Attention" class="headerlink" title="Self Attention"></a>Self Attention</h2><p>下面需要计算 Self Attention，Self Attention 的计算见 <a href="https://clay-nuyoah.club/2021/07/10/self-attention/" title="Self-Attention">https://clay-nuyoah.club/2021/07/10/self-attention/</a></p>
<h2 id="Layer-Norm"><a href="#Layer-Norm" class="headerlink" title="Layer Norm"></a>Layer Norm</h2><p>下面的计算中会用到 Layer Norm，先进行介绍。</p>
<p>对于向量 x = [1,2,3,4] 来说，首先获取 x 的均值，获取 x 的标准差，然后用下面的公式计算新的向量 $x_i’$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-15-23-3310ad5905aaa944026d39edd2058fc2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210711201516-8de4a7.png"></p>
<p>代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line">x &#x3D; np.array([1,2,3,4])</span><br><span class="line">(x - x.mean()) &#x2F; x.std()</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h2 id="详细过程"><a href="#详细过程" class="headerlink" title="详细过程"></a>详细过程</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-21-46-fead93e0f044be2885f7c43662589886-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210711200408-31671d.png"></p>
<p>红线部分框出了 Encoder 的输入，先执行左侧黄色框，然后执行右侧黄色框。</p>
<pre><code>1、根据输入 b 计算 Self Attention，得到新向量 a
(向量 b 有两个来源，一个来源是起始时的输入，另一个来源是上一层的 block 块的输出)
2、根据 Self Attention 的输出 a 与输入向量 b 相加得到新的向量 c
3、向量 c 按照上面介绍的 Layer Norm 做归一化处理得到向量 d
4、向量 d 经过全神经网络得到新的向量 e
5、向量 e 与向量 d 向量相加得到新的向量 f
6、向量 f 做 Layer Norm 得到新的向量 g
7、当前的 block 计算结束，向量 g 可能作为下一个 block 的输入，或者是直接作为 Encoder 的输出
8、通过上面的步骤依次计算每一个输入的输出</code></pre>
<h2 id="Encoder-总结"><a href="#Encoder-总结" class="headerlink" title="Encoder 总结"></a>Encoder 总结</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-26-45-22e8ab6fe46224cc765283ceb15b750f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210711202532-1c8140.png"></p>
<p>上图是 Encoder 部分的简化图，红框部分就是步骤 4，黄框部分是 步骤 2，3 和步骤 5，6。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-29-49-4d29ba88042c9701c3f60e474cb80f05-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210711202945-4eb249.png"></p>
<p>Transformer 的 Encoder 部分有很多结构，上图中左侧是上面讲的 Encoder 结构，右侧是另外一种结构，而且还有很多其他的结构。</p>
<h1 id="3、Decoder-部分"><a href="#3、Decoder-部分" class="headerlink" title="3、Decoder 部分"></a>3、Decoder 部分</h1><p>Decoder 部分有两种结构，分别是 Auto regressive (AT) 和 Non-auto regressive (NAT)。</p>
<h2 id="Self-Attention-And-Masked-Self-Attention"><a href="#Self-Attention-And-Masked-Self-Attention" class="headerlink" title="Self Attention And Masked Self Attention"></a>Self Attention And Masked Self Attention</h2><p>下面的内容会用到 Self Attention 和 Masked Self Attention，先做介绍。</p>
<h3 id="Self-Attention-1"><a href="#Self-Attention-1" class="headerlink" title="Self Attention"></a>Self Attention</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-43-33-e66373c218f6cdca86425aaee21eb2fc-QQ%E6%88%AA%E5%9B%BE20210711204325-bc9e81.png"></p>
<p>Self Attention 是每个输入彼此之间做 Attention。</p>
<p>Self Attention 的计算见 <a href="https://clay-nuyoah.club/2021/07/10/self-attention/" title="Self-Attention">https://clay-nuyoah.club/2021/07/10/self-attention/</a></p>
<h3 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self Attention"></a>Masked Self Attention</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-37-22-c378b5c7bf650c05f3c4fedbb9a70ef4-QQ%E6%88%AA%E5%9B%BE20210711203657-063648.png"></p>
<p>正如上图所显示的那样，Decoder 输入并不是一次性进行输入的，只有在预测完一个词，才能用这个词的信息和 Encoder 的输出信息一起输入到 Decoder 中来预测下一个词，所以在 Decoder 中只有当前输入和以前的输入信息。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-45-00-3eaee08234325cf81860dd743b46f318-QQ%E6%88%AA%E5%9B%BE20210711204450-99dd12.png"></p>
<p>由于存在上面的问题，所以 Masked Self Attention 对 Self Attention 做了修改，做 Attention 的时候只有当前向量和以前的输入向量。</p>
<h2 id="Auto-regressive"><a href="#Auto-regressive" class="headerlink" title="Auto regressive"></a>Auto regressive</h2><p>Auto regressive 是将 Decoder 前期产生的输出信息，输入到 Decoder 中，然后根据 Encoder 的输出，使用 Decoder 预测下一个输出。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-33-03-fc46738da7d5c984843d94d7724b6ae5-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210711203257-b9a2a8.png"></p>
<p>Auto regressive 首先会给 Decoder 一个特殊的 token，表示开始进行预测，然后 Decoder 根据 Decoder 自己的输入和 Encoder 的输入，预测出了 <em>机</em> 这个输出。由于用到了表示预测开始的这个 token，所以在词表中需要添加这个 token。 </p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-37-22-c378b5c7bf650c05f3c4fedbb9a70ef4-QQ%E6%88%AA%E5%9B%BE20210711203657-063648.png"></p>
<p>每次的输出都会根据上一次的输出和 Encoder 的输出产生新的输出。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/12/09-19-34-59d48775783fe27e6bc077d24cebc1b2-QQ%E6%88%AA%E5%9B%BE20210712091921-71a792.png"></p>
<p>直到遇到表示结束的特殊 token 来结束预测。所以在词表中也会添加表示预测结束的 token。有时也在词表中用同一个 token 来表示预测开始和预测结束。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-38-46-57cc9ab84f82433711638a3c95d9644b-QQ%E6%88%AA%E5%9B%BE20210711203840-a8b755.png"></p>
<p>Decoder 的内部结构如上图所示，看着非常的复杂，其实在 Encoder 中已经学习了绝大部分。这里也需要注意，在 Decoder 中也是由非常多的 block 组成,而且也加入了位置信息。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-39-28-1e2c16322b0ee1cdc1b14a9d7a46554f-QQ%E6%88%AA%E5%9B%BE20210711203919-fc5672.png"></p>
<p>上面的图展示了 Encoder 和 Decoder 的区别，除了灰色覆盖区域外和红框区域，其他部分和 Encoder 是一样的。其中 Masked Self Attention 也已经做了介绍。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-49-43-ed7fbb5149193547c5650459d6d68c0c-QQ%E6%88%AA%E5%9B%BE20210711204930-058769.png"></p>
<p>红框是 Cross Attention 部分，该部分的两个蓝圈表示 Multi-Head Attention 部分会受到来自于 Encoder 的两个输入，红圈表示 Multi-Head Attention 部分会受到一个来自 Decoder 内部的输入。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/20-51-32-6507c33f2d083ea62975af98fa63109a-QQ%E6%88%AA%E5%9B%BE20210711205126-3ad650.png"></p>
<p>上图中 $q$ 就是来自 Decoder 内部的输入，$k^i、v^i$是来自 Encoder 的两个输入，会把 Encoder 的输出变成两个向量 $k^i、v^i$，这两个向量会跟 Decoder 内部传过来的向量做 Self Attention,得到 $v$，<em>然后 $v$ 和 $q$ 相加，做 Layer Norm</em> (这一块图中没有给出，但是根据上上图是有这么一个操作,而且图中做的是 Self Attention，而不是 Multi-Head Attention)，最入输出到全连接网络中。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/21-01-08-4386851eaa677905698c09cb26134ec4-QQ%E6%88%AA%E5%9B%BE20210711210100-a0934d.png"></p>
<p>生成第二个输出的过程和生成第一个输出的过程是一样的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/21-02-29-10949b499ed8a79114ddaef310a5447a-QQ%E6%88%AA%E5%9B%BE20210711210222-699d94.png"></p>
<p>做 Cross Attention 有很多的形式，上图的上部分是上面讨论的 Cross Attention，将 Encoder 中产生的输出，输入到 Decoder 中的每一个块中。下部分也列举了各种 Cross Attention。</p>
<h2 id="AT-v-s-NAT"><a href="#AT-v-s-NAT" class="headerlink" title="AT v.s. NAT"></a>AT v.s. NAT</h2><p>AT 是给定一个输入，会有一个输出，而 NAT，是给定 N 个输入，会有 N 个输出。NAT 给定输入的方式有 2 种。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/12/09-10-55-dbd9f29658292df90d158a76191e2bd0-QQ%E6%88%AA%E5%9B%BE20210712091001-3e41a3.png"></p>
<p>一种是用其他的模型当预测模型，来预测输出的长度，根据输出的长度，给定输入。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/12/09-15-09-bc7fdf4cb31c0cce00a7dcd7de68f40f-QQ%E6%88%AA%E5%9B%BE20210712091459-5a1e58.png"></p>
<p>另一种是不管三七二十一给定非常多的输入，然后根据给定的 N 个输入，产生 N 个输出。从 $W_1$ 开始，直到出现结尾符结束，第一个结尾符后面的字符全都不要。</p>
<p>NAT 与 NT 相比 NT 能够进行并行计算，而现在 NAT 的表现还没有 NT 的表现好。</p>
<h1 id="4、技巧"><a href="#4、技巧" class="headerlink" title="4、技巧"></a>4、技巧</h1><h2 id="添加噪声"><a href="#添加噪声" class="headerlink" title="添加噪声"></a>添加噪声</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/21-04-23-53c4143fd405c0d111ccb5fd7f3bef2d-QQ%E6%88%AA%E5%9B%BE20210711210415-031054.png"></p>
<p>在训练时每次都给 Decoder 正确的输入，会使 Decoder 在看到哪几个关键字时，应该给出怎样的输出。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/21-06-50-d140b55ca2d7de2dafb09a0ebea86804-QQ%E6%88%AA%E5%9B%BE20210711210638-ef118f.png"></p>
<p>但是这样也会存在一些问题，问题是在测试时，输出中的某个词有错误，因为在训练的时候，模型没有见过这种错误，导致模型在接下来可能所有的输出都是错误的。</p>
<p>所以要给模型适当的输入少量的错误信息，能让模型拥有更强的容错性。</p>
<h2 id="学会复制"><a href="#学会复制" class="headerlink" title="学会复制"></a>学会复制</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/21-10-40-610b5d4c41fd6afd64840e577f0c6d54-QQ%E6%88%AA%E5%9B%BE20210711211032-ac2c19.png"></p>
<p>在对话机器人任务中，要让机器人学会复制，对于* 库洛洛* 这样的名字，模型有很大的概率是没有见过的，所以当机器人在看到 你好，我是 XXX 时，要学会复制。</p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Attention</tag>
        <tag>Transformer</tag>
        <tag>Self-Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>self attention</title>
    <url>/2021/07/10/self-attention/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/10-42-53-a108b67dae803d8778d8ac314a49c12b-QQ%E6%88%AA%E5%9B%BE20210711104245-c097df.png"></p>
<a id="more"></a>

<h1 id="Self-Attention-模型"><a href="#Self-Attention-模型" class="headerlink" title="Self-Attention 模型"></a>Self-Attention 模型</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/20-41-53-bc4b4863894f89b227d543d405254f23-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210710204145-2b706e.png"></p>
<p>Self Attention 是一种给定 n 个输入，给出 n 个输出的模型，通过全连接层进行预测和分类。Self Attention 内部的架构会在下面进行讲解。</p>
<h1 id="全连接网络的局限性"><a href="#全连接网络的局限性" class="headerlink" title="全连接网络的局限性"></a>全连接网络的局限性</h1><h2 id="不擅长处理多个向量作为输入"><a href="#不擅长处理多个向量作为输入" class="headerlink" title="不擅长处理多个向量作为输入"></a>不擅长处理多个向量作为输入</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/20-52-42-2e0f49e09333deaa05798d3200f5378b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210710204145-99c345.png"></p>
<p>全连接网络不擅长处理将多个向量作为输入，产生数值或者分类。但存在很多的任务需要输入多个向量，用来产生数值或者是分类。</p>
<p><strong>多向量作为输入</strong></p>
<p><em>文字处理</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/20-56-28-37c8e804ac3cc58c78201d30d75d9dfb-QQ%E6%88%AA%E5%9B%BE20210710205621-d8f85a.png"></p>
<p>文字处理任务需要将多个向量输入到模型，输入的每个向量可以是通过 One-Hot 编码过的向量(向量之间的相关性无法体现)，也可以是通过词嵌入形成的向量(在高维空间中相关的单词会汇聚在一起)。</p>
<p><em>语音处理</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/20-59-18-60e66b49cfead699087f4fb89c8c3c2c-QQ%E6%88%AA%E5%9B%BE20210710205910-a1116f.png"></p>
<p>语音处理也是把多个向量作为输入，一般是每 25ms 对音频采样一次，但是会向右移动 10ms 位置（这就意味着会有 15ms 的音频信息会被重复采样），然后利用 400 sample points (16KHz)、39-dim MFCC 或者是 80-dim filter bank output 3 种方法提取声学特征。这样每 25ms 提取到的声学特征组成一个向量，这样的一个向量称为一个 frame。</p>
<p><em>图</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-02-54-37c8e804ac3cc58c78201d30d75d9dfb-QQ%E6%88%AA%E5%9B%BE20210710205621-3ee2e2.png"></p>
<p>图信息是把每个节点作为一个向量进行输入，一个图存在多个节点，所以是多输入问题。</p>
<p><strong>输出的形式</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-44-58-c839da5ed69f1dea08f289537c218332-QQ%E6%88%AA%E5%9B%BE20210711094429-7b6e0a.png"></p>
<p>输出向量的个数可以是跟输入向量的个数相同的，例如在词性标注问题上，输入 N 个单词，就要给出这 N 个单词的词性。在语音识别问题上，输入 n 个向量就要给出这 n 个向量的 Grapheme 表示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-47-48-cc32c4f0a0fc209645bbf48d79c196dd-QQ%E6%88%AA%E5%9B%BE20210711094740-c06f5e.png"></p>
<p>可以是输入 n 个向量，输出一个向量。例如在情感预测上，给出一个句子，判断句子的情感是消极的还是积极的。在语者识别任务中，给出一段录音，识别这断录音是谁讲的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-51-07-30f4d6e6126368c63e209cd7ee465aa9-QQ%E6%88%AA%E5%9B%BE20210711095056-188ff4.png"></p>
<p>也可以是输入 n 个向量，输出 m 个向量，例如机器翻译任务。</p>
<h2 id="上下文无关"><a href="#上下文无关" class="headerlink" title="上下文无关"></a>上下文无关</h2><p><strong>上下文无关</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-06-29-86dcf0b556dd19bf89d39d7b3ca77294-QQ%E6%88%AA%E5%9B%BE20210710210615-187011.png"></p>
<p>当前的输入没有依赖上下文的输入，上图中的两个 saw 在不依赖上下文的情况下是一样的，但是在原文中，第一个 saw 是动词，而第二个 saw 是名词，所以上下的信息是重要的信息。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-11-06-15be5b20ac6e217bc84d1fafb340f0a7-QQ%E6%88%AA%E5%9B%BE20210710211059-42489a.png"></p>
<p>在这种情况下虽然把上下文信息加入到了当前输入，但是窗口的大小是无法确定的，一方面是由于句子的大小无法确定，另一方面是因为窗口太小，上下文信息考虑的不全面，窗口太大，导致参数过多，训练困难。</p>
<h1 id="Self-Attention-的优势"><a href="#Self-Attention-的优势" class="headerlink" title="Self Attention 的优势"></a>Self Attention 的优势</h1><h2 id="全局性"><a href="#全局性" class="headerlink" title="全局性"></a>全局性</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-14-54-46a2eaa1bde06cd4066562a72b121c2c-QQ%E6%88%AA%E5%9B%BE20210710211441-5ef35a.png"></p>
<p>Self Attention 在产生一个输出的时候，会考虑到所有的输入。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-16-46-00e45d1afc464e33fba91261e33c5b73-QQ%E6%88%AA%E5%9B%BE20210710211635-301377.png"></p>
<p>Self Attention 在经过全连接层之后可以再接多个 Self Attention</p>
<h2 id="并行运算"><a href="#并行运算" class="headerlink" title="并行运算"></a>并行运算</h2><p>Self Attention 可以轻松的做到并行运算，提升效率。</p>
<h1 id="Self-Attention-的计算"><a href="#Self-Attention-的计算" class="headerlink" title="Self Attention 的计算"></a>Self Attention 的计算</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-17-54-f4aa8d6525be14e0256e58a47f229999-QQ%E6%88%AA%E5%9B%BE20210710211746-75bb07.png"></p>
<p>Self Attention 中的每个向量都会跟其他向量计算产生一个 $\alpha$ 值，在上图中只显示了 $a^1$ 与 $a^2$、$a^3$、$a^4$ 的计算，实际上 $a^1 与 a^1 $ 自己也会计算 $\alpha$ 值。另外 $a^2、a^3、a^4$ 都会与每个向量计算 $\alpha$ 值，图中也没有画出，下面会讲解。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-25-26-6fcaa513fcb7acad0a0d40606209b42f-QQ%E6%88%AA%E5%9B%BE20210710212519-68e876.png"></p>
<p>$\alpha$ 值的计算主要有两种方法，但是下面的计算使用的是第一种方法。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-28-08-1bc39a07ee88b0d19d7e5f801ee93eef-QQ%E6%88%AA%E5%9B%BE20210710212652-c2f715.png"></p>
<p>首先按照红框计算出 $q^1$,按照黄色框计算出 $k^1、k^2、k^3、k^4$，然后$q^1 和 k^1、k^2、k^3、k^4$ 分别做内积计算出 $\alpha_{1,1},\alpha_{1,2},\alpha_{1,3},\alpha_{1,4}$(上面说的步骤都是根据 <strong>Dot-product</strong> 计算出来的)，$\alpha_{1,1},\alpha_{1,2},\alpha_{1,3},\alpha_{1,4}$ 每个值都称为 Attention 得分，将 Attention 得分输入到 softmax，得到概率值$\alpha_{1,1}’、\alpha_{1,2}’、\alpha_{1,3}’、\alpha_{1,4}’$。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-38-18-9c52016803b28856590fac5d308bf011-QQ%E6%88%AA%E5%9B%BE20210710213354-917930.png"></p>
<p>按照橙色框计算出 $v_1、v_2、v_3、v_4$，然后按照粉色框计算出 $b^1$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/21-41-08-be2b8e8348ebc6d08284ac4c85d3c90e-QQ%E6%88%AA%E5%9B%BE20210710214059-37fb04.png"></p>
<p>$b^2$ 的计算与 $b^1$ 是一样的，按照同样的步骤计算出 $b^3、b^4$。</p>
<h1 id="Self-Attention-的并行计算"><a href="#Self-Attention-的并行计算" class="headerlink" title="Self Attention 的并行计算"></a>Self Attention 的并行计算</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/08-34-30-56cb3c0841a38cb6f66f9cd8311d7094-QQ%E6%88%AA%E5%9B%BE20210711083349-bf9734.png"></p>
<p>并行获取 $q^i、k^i、v^i$的值</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/08-34-58-e4b4adf31606dade3e5a2b61ee9ad705-QQ%E6%88%AA%E5%9B%BE20210711083448-3dd5eb.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/08-35-37-60fc6177a8c430b4576889176a448c59-QQ%E6%88%AA%E5%9B%BE20210711083528-9bfad5.png"></p>
<p>并行获取 $\alpha_{i,j} 和 \alpha_{i,j}’$ 的值。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/08-36-02-0b71727a867583b907c9e8849916e392-QQ%E6%88%AA%E5%9B%BE20210711083554-d82922.png"></p>
<p>并行的获取 $b^i$ 的值</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/08-36-23-3d2089643778fdd8c5de066b5d112c5e-QQ%E6%88%AA%E5%9B%BE20210711083618-d645db.png"></p>
<p>上图的 3 个步骤对 Self Attention 做了总结,在整个运算过程中只有$W^q、W^k、W^v$ 是未知的，是需要求解的参数。</p>
<h1 id="Multi-head-Self-attention"><a href="#Multi-head-Self-attention" class="headerlink" title="Multi-head Self-attention"></a>Multi-head Self-attention</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/08-54-50-fe0b51cead857253fa1d20abe4f20d87-QQ%E6%88%AA%E5%9B%BE20210711084059-219fd7.png"></p>
<p>根据红框部分的公式计算产生 $q^i 和 q^j$,根据黄框部分的公式计算产生 $q^{i,1},q^{i,2},q^{j,1},q^{j,2}$,根据绿框部分的公式计算产生$v^{i,1},v^{i,2},v^{j,1},v^{j,2}$,根据粉框部分的公式计算产生$k^{i,1},k^{i,2},k^{j,1},k^{j,2}$,接下来就跟前面的一样了，使用下面公式计算 $\alpha_{i,1}，\alpha_{j,1}$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/10-11-59-406f694611015eab01b218c8d3947ed6-QQ%E6%88%AA%E5%9B%BE20210711101149-173d5d.png"></p>
<p>让 $\alpha_{i,1}，\alpha_{j,1}$ 通过 softmax 计算 $\alpha_{i,1}’，\alpha_{j,1}’$,根据下面公式计算 $b^{i,1}$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/10-38-17-fb0845cdbdf9050804690b404d0c4656-QQ%E6%88%AA%E5%9B%BE20210711103811-82c52d.png"></p>
<p>可以将看出在计算 $b^{i,1}$ 的时候是与 $k^{i,2},k^{j,2},v^{i,2},v^{j,2}$ 无关的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-13-58-8230e434f7390aef8c2ba70e78d2753d-QQ%E6%88%AA%E5%9B%BE20210711091329-ad4949.png"></p>
<p>计算 $b^{i,2}$ 与计算 $b^{i,1}$ 的步骤是一样的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-15-30-b611fefd9ab9fe55a708b95daf8677b0-QQ%E6%88%AA%E5%9B%BE20210711091457-121fb9.png"></p>
<p>最后将两个向量合并起来，通过矩阵乘法，求取 $b^i$。</p>
<h1 id="位置编码"><a href="#位置编码" class="headerlink" title="位置编码"></a>位置编码</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-16-41-9665a8d9eafcceda4be244f37733f065-QQ%E6%88%AA%E5%9B%BE20210711091635-f4a54e.png"></p>
<p>上面介绍的 Self Attention 都没有包含位置信息，只包含了相似性信息，所以可以通过在输入时加入位置信息来提升 Self Attention 的表现。</p>
<h1 id="Self-attention-v-s-CNN"><a href="#Self-attention-v-s-CNN" class="headerlink" title="Self-attention v.s. CNN"></a>Self-attention v.s. CNN</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-19-35-c074763669c756ca84eeb4158d381baa-QQ%E6%88%AA%E5%9B%BE20210711091926-cd5515.png"></p>
<p>只用 Self Attention 是拿着一个像素信息，和整张图片的其他像素做 Self Attention，而是用了 CNN 之后相当于一个像素与接收视野内的像素做 Self Attention。</p>
<p>所以 Self Attention 是复杂版本的 CNN，CNN 是简化版本的 Self Attention。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-23-35-e141b772eae59edcd0a32b008b9e7783-QQ%E6%88%AA%E5%9B%BE20210711092324-317c93.png"></p>
<p>从上图可以看出 Self Attention 是包含 CNN 的，相对来说 Self Attention 模型是比 CNN 模型复杂的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-24-52-8119bd8398e67db2eb1fcdb0448170ad-QQ%E6%88%AA%E5%9B%BE20210711092445-a7ead7.png"></p>
<p>所以在数据量较少时，Self Attention 会出现欠拟合。当数据量达到一定的数量时，Self Attention 的表现优于 CNN，这是因为 Self Attention 的模型复杂，可以更好的拟合数据。</p>
<h1 id="Self-attention-v-s-RNN"><a href="#Self-attention-v-s-RNN" class="headerlink" title="Self-attention v.s. RNN"></a>Self-attention v.s. RNN</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/11/09-26-38-799ed025494169ddbe0c988dcfebe304-QQ%E6%88%AA%E5%9B%BE20210711092633-545da0.png"></p>
<pre><code>1、单向的 RNN 网络是只能捕获上文信息，而无法捕获下文信息，而 Self Attention 能够非常容易的捕获上下文信息。
2、虽然双向 RNN 也能够捕获到上下文信息，但是存在梯度消失和梯度爆炸问题，
当两个信息距离非常远时，是很难捕获有用信息的，但是在 Self Attention 中，
捕获上下文信息是通过相似程度来实现的，无论两条信息距离多远，
只要两条信息的相似度越高，那就能越多的捕获到两者的信息。
3、上面已经介绍过 Self Attention 是可以做并行计算的，而 RNN 无法实现并行计算。</code></pre>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>Attention</tag>
        <tag>Self Attention</tag>
      </tags>
  </entry>
  <entry>
    <title>CNN</title>
    <url>/2021/07/09/cnn/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/19-13-18-10b27b1a14748b41dc2d7eb390a95b7d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709191301-85fce7.png"></p>
<a id="more"></a>

<h1 id="0、前置知识"><a href="#0、前置知识" class="headerlink" title="0、前置知识"></a>0、前置知识</h1><h2 id="单通道图"><a href="#单通道图" class="headerlink" title="单通道图"></a>单通道图</h2><p>俗称灰度图，每个像素点只能有有一个值表示颜色，它的像素值在0到255之间，0是黑色，255是白色，中间值是一些不同等级的灰色。（也有3通道的灰度图，3通道灰度图只有一个通道有值，其他两个通道的值都是零）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/20-06-28-5bcbd4fc0769f5fe8368cb19d53fc848-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709200538-6b1804.png"></p>
<p>上面是一张单通道图。</p>
<h2 id="三通道图"><a href="#三通道图" class="headerlink" title="三通道图"></a>三通道图</h2><p>三通道图，每个像素点都有3个值表示 ，所以就是3通道。也有4通道的图。例如RGB图片即为三通道图片，RGB色彩模式是工业界的一种颜色标准，是通过对红(R)、绿(G)、蓝(B)三个颜色通道的变化以及它们相互之间的叠加来得到各式各样的颜色的，RGB即是代表红、绿、蓝三个通道的颜色，这个标准几乎包括了人类视力所能感知的所有颜色，是目前运用最广的颜色系统之一。总之，每一个点由三个值表示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/20-03-14-59c105758e0daa9f26bf0af2776bb2da-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709200301-023887.png"></p>
<p>上图显示了一张图片是由 RGB 三个通道组成的。</p>
<h1 id="1、CNN-模型结构"><a href="#1、CNN-模型结构" class="headerlink" title="1、CNN 模型结构"></a>1、CNN 模型结构</h1><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/upload-images.jianshu.io/upload_images/2021/07/09/14-13-28-588f68dfcf7c2c618f374e027c0e1d0c-2256672-a36210f89c7164a7-17861a.png"></p>
<p>如上图所示，一个卷积神经网络由若干卷积层、Pooling 层、全连接层组成。你可以构建各种不同的卷积神经网络，它的常用架构模式为：</p>
<pre><code>INPUT -&gt; [[CONV] * N -&gt; POOL?] * M -&gt; [FC] * K</code></pre>
<p>也就是 N 个卷积层叠加，然后(可选)叠加一个 Pooling 层，重复这个结构 M 次，最后叠加 K 个全连接层。</p>
<p>对于上图所展示的卷积神经网络：</p>
<pre><code>INPUT -&gt; CONV -&gt; POOL -&gt; CONV -&gt; POOL -&gt; FC -&gt; FC</code></pre>
<p>按照上述模式可以表示为：</p>
<pre><code>INPUT -&gt; [[CONV]*1 -&gt; POOL]*2 -&gt; [FC]*2</code></pre>
<p>也就是：</p>
<pre><code>N = 1, M = 2, K = 2。</code></pre>
<h1 id="2、全连接网络处理图片的局限性"><a href="#2、全连接网络处理图片的局限性" class="headerlink" title="2、全连接网络处理图片的局限性"></a>2、全连接网络处理图片的局限性</h1><p><strong>参数数量太多</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/14-16-30-4546c89fc47f69e33b9bc8cb39cbbc8a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709141622-9e3738.png"></p>
<p>如果一张图片有三个通道，每个通道是 100 * 100 的矩阵，将这张图片每个通道中的矩阵拉平，每个通道可以获得一个 1 万维的向量，然后将三个通道拼接，获得 3 万维的向量。接下来跟隐层中的 100 个节点做全连接，会出现 3 * 100 * 100 * 100 = 3 百万个参数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/14-27-59-3ee2fe8de8bc27ef285e61d9cf080591-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709142233-3352ca.png"></p>
<p>在上面的图片中，红色框圈出了鸟嘴、鸟眼、鸟脚三个特征，选择其中的一个特征不太可能确定是鸟类，但是将 3 个特征结合起来，就有非常大的概率可以确定是鸟类了。这样就不需要让神经元看到整张图片的信息。</p>
<p><strong>相同特征可能出现在不同区域</strong> </p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/14-46-29-6fe4f72110679f9f9dc75b600c163692-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709144621-334fdc.png"></p>
<p>对于鸟嘴这个特征，他可能会出现在图片的左上角，也可能出现在图片的中间，但是无论出现在什么位置，它都是识别鸟类的重要特征，所以特征出现的区域不应该影响鸟类的识别。</p>
<h1 id="3、CNN的优点"><a href="#3、CNN的优点" class="headerlink" title="3、CNN的优点"></a>3、CNN的优点</h1><p>为了解决上面提到的全连接网络的不足，CNN 提出了下面的解决方法。</p>
<h2 id="接收视野"><a href="#接收视野" class="headerlink" title="接收视野"></a>接收视野</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/14-52-47-a8206a4426ba1b5fa4724e94282dd4b5-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709145241-f6e2f9.png"></p>
<p>为了解决 <em>参数数量太多</em> 的问题,使用了<em>接收视野</em> 来将一块比较小的区域输入到神经元中。假如使用 3 * 3 * 3 的 Filter，将 <em>接收视野</em> 中的每个矩阵拉直，再拼接，输入到一个神经元中，会有 3 * 3 * 3 = 27 的参数数量</p>
<h2 id="下采样"><a href="#下采样" class="headerlink" title="下采样"></a>下采样</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/15-02-00-eb7da89cacc862c9f33c74a6d4082db1-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709150153-6e63bd.png"></p>
<p>下采样操作也是解决参数过多的手段，上图中左侧是原始图像，右侧是对左侧图片每隔一个像素点取样一次得到的图片，右图对于我们的识别是没有影响的，这样通过改变输入的数量，改变了参数的数量。</p>
<h2 id="权值共享"><a href="#权值共享" class="headerlink" title="权值共享"></a>权值共享</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/14-58-34-a81e0bf44d8617fd0369c88ff5af3230-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709145804-8511de.png"></p>
<p>为了解决 <em>相同特征可能出现在不同区域</em> 的问题，通过权值共享让具有相同特征的区域在输出时具有基本相同的输出。</p>
<h1 id="4、CNN-网络架构"><a href="#4、CNN-网络架构" class="headerlink" title="4、CNN 网络架构"></a>4、CNN 网络架构</h1><p>前面的 <em>CNN 模型结构</em> 已经介绍了一个卷积神经网络由若干卷积层、Pooling层、全连接层组成。下面是对卷积层和 Pooling 层的介绍。</p>
<h2 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h2><p>所有的数据和图片来自于 <a href="https://www.zybuluo.com/hanbingtao/note/485480" title="零基础入门深度学习(4) - 卷积神经网络">https://www.zybuluo.com/hanbingtao/note/485480</a></p>
<h3 id="单通道单-Filter-卷积"><a href="#单通道单-Filter-卷积" class="headerlink" title="单通道单 Filter 卷积"></a>单通道单 Filter 卷积</h3><p>卷积层对应的是卷积操作。</p>
<p>假设有一个 5 * 5 的图像，使用一个 3 * 3 的 filter 进行卷积，想得到一个3 * 3的Feature Map，如下所示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/upload-images.jianshu.io/upload_images/2021/07/09/15-15-43-1ba77d89601b81e6bd097767dc4c9f4c-2256672-548b82ccd7977294-e026b0.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/15-26-20-664724827a67debcdd4e47a6eb85c6a1-202107091522391-d865b1.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/15-30-48-f2a93b6c784c7e4cc4984409b19b7fd5-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709153041-dd11b7.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/16-22-15-3569270401c65d677e41084594de59c7-2256672-318017ad134effc5-1e53e2.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/16-29-32-78ad4fa09fb40412188d331f6e618bda-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709162531-b7b0e8.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/upload-images.jianshu.io/upload_images/2021/07/09/16-33-55-a51a38103e3ad616437a1e7a2f7a1765-2256672-b05427072f4c548d-aace0e.png"></p>
<p>可以依次计算出Feature Map中所有元素的值。下面的动画显示了整个Feature Map的计算过程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/upload-images.jianshu.io/upload_images/2021/07/09/16-37-27-6428cf505ac1e9e1cf462e1ec8fe9a68-2256672-19110dee0c54c0b2-0f0e6a.gif"></p>
<p>上面的计算过程中，步幅(stride)为1。步幅可以设为大于1的数。例如，当步幅为2时，Feature Map计算如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/upload-images.jianshu.io/upload_images/2021/07/09/16-38-42-689c698ee4902dd3e83ceab97cdf0072-2256672-273e3d9cf9dececb-a01b68.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/upload-images.jianshu.io/upload_images/2021/07/09/16-39-18-ddad421687bbad2655518588d53053c2-2256672-7f362ea9350761d9-73cca3.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/upload-images.jianshu.io/upload_images/2021/07/09/16-39-53-d36160dea827df695c2aaf8328adca9a-2256672-f5fa1e904cb0287e-3c411d.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/upload-images.jianshu.io/upload_images/2021/07/09/16-40-19-8182dbf86c612e5a31968263b7a53e23-2256672-7919cabd375b4cfd-3a5a1a.png"></p>
<h3 id="多通道单-Filter-卷积"><a href="#多通道单-Filter-卷积" class="headerlink" title="多通道单 Filter 卷积"></a>多通道单 Filter 卷积</h3><p>上面讲的都是单通道的卷积操作，也就是假设图片只有一个通道，下面列举有 3 个通道的图片。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/09/20-19-33-3e72ac63bcb3ae3c5dcb174ee0d24052-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709200853-260583.png"></p>
<p>将 $X$ 分为 3 个 7 * 7 的矩阵，分别为 $X1、X2、X3$，将 Filter1 分为 3 个 3 * 3 的矩阵，分别为 $W_1$、$W_2$、$W_3$，将 $X1$ 与 $W_1$ 做卷积操作（单通道的卷积操作）得到$C1$，$X2$ 与 $W_2$ 做卷积操作得到$C2$，$X3$ 与 $W_3$ 做卷积操作得到 $C3$,然后将 $C1$、$C2$、$C3$ 的对应元素相加，最后加上偏置 $W_b = 1$,得到 $O1$。</p>
<p>下面的代码对 O1 进行了实现，feature_map 就是 O1 所表示的矩阵</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">import numpy as np</span><br><span class="line"></span><br><span class="line"># 定义图片的 3 个通道信息</span><br><span class="line"></span><br><span class="line">X1 &#x3D; np.array([[0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">               [0, 0, 1, 1, 0, 2, 0],</span><br><span class="line">               [0, 2, 2, 2, 2, 1, 0],</span><br><span class="line">               [0, 1, 0, 0, 2, 0, 0],</span><br><span class="line">               [0, 0, 1, 1, 0, 0, 0],</span><br><span class="line">               [0, 1, 2, 0, 0, 2, 0],</span><br><span class="line">               [0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line"></span><br><span class="line">X2 &#x3D; np.array([[0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">               [0, 1, 0, 2, 2, 0, 0],</span><br><span class="line">               [0, 0, 0, 0, 2, 0, 0],</span><br><span class="line">               [0, 1, 2, 1, 2, 1, 0],</span><br><span class="line">               [0, 1, 0, 0, 0, 0, 0],</span><br><span class="line">               [0, 1, 2, 1, 1, 1, 0],</span><br><span class="line">               [0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line"></span><br><span class="line">X3 &#x3D; np.array([[0, 0, 0, 0, 0, 0, 0],</span><br><span class="line">               [0, 2, 1, 2, 0, 0, 0],</span><br><span class="line">               [0, 1, 0, 0, 1, 0, 0],</span><br><span class="line">               [0, 0, 2, 1, 0, 1, 0],</span><br><span class="line">               [0, 0, 1, 2, 2, 2, 0],</span><br><span class="line">               [0, 2, 1, 0, 0, 1, 0],</span><br><span class="line">               [0, 0, 0, 0, 0, 0, 0]])</span><br><span class="line"># 定义 3 个 filter 信息</span><br><span class="line">image_filter1 &#x3D; np.array([[-1, 1, 0],[0, 1, 0],[0, 1, 1]])</span><br><span class="line">image_filter2 &#x3D; np.array([[-1, -1, 0],[0, 0, 0],[0, -1, 0]])</span><br><span class="line">image_filter3 &#x3D; np.array([[0, 0, -1],[0, 1, 0],[1, -1, -1]])</span><br><span class="line"></span><br><span class="line">width &#x3D; X1.shape[-1]</span><br><span class="line">height &#x3D; X1.shape[0]</span><br><span class="line"></span><br><span class="line"># 步长</span><br><span class="line">stride &#x3D; 2</span><br><span class="line"># 图片信息</span><br><span class="line">Xs &#x3D; [X1,X2,X3]</span><br><span class="line"># filter 信息</span><br><span class="line">image_filters &#x3D; [image_filter1,image_filter2,image_filter3]</span><br><span class="line"># 通道大小</span><br><span class="line">channel_number &#x3D; len(Xs)</span><br><span class="line">filter_size &#x3D; len(image_filters)</span><br><span class="line"># 偏置</span><br><span class="line">b &#x3D; 1</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">feature_map_width &#x3D; (width - filter_size) &#x2F;&#x2F;  stride + 1 </span><br><span class="line">feature_map_height &#x3D; (height - filter_size) &#x2F;&#x2F;  stride + 1 </span><br><span class="line"></span><br><span class="line">feature_map &#x3D; np.zeros((feature_map_height,feature_map_width))</span><br><span class="line">for k in range(channel_number):</span><br><span class="line">    for i in range(0,width - filter_size + 1,stride):</span><br><span class="line">        for j in range(0,height - filter_size + 1,stride):</span><br><span class="line">            X &#x3D; Xs[k]</span><br><span class="line">            image_filter &#x3D; image_filters[k]</span><br><span class="line">            feature_map[i  &#x2F;&#x2F; stride][j &#x2F;&#x2F; stride] +&#x3D; (np.sum(X[i : i + filter_size, j : j + filter_size] * image_filter))</span><br><span class="line"># 添加偏置</span><br><span class="line">feature_map +&#x3D;b</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="多通道多Filter卷积"><a href="#多通道多Filter卷积" class="headerlink" title="多通道多Filter卷积"></a>多通道多Filter卷积</h3><p>上面讲述的是单通道单 Filter，在单通道单 Filter 中，只能够产生一个输出，但是在多通道多 Filter 中会产生多个输出，即一个 Filter 会产生一个输出。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/10/09-59-16-fa4eb7e19135d8c151de61bbc789b8dd-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210709203057-0fb4d1.png"></p>
<p>与单通道单 Filter 产生 O1 的方式一样，可以通过 $X1、X2、X3$ 与 Filter2 中的 $W_4$、$W_5$、$W_6$ 得到 $O2$,这样就能通过 2 个 Filter 获得 2 个输出，因此存在几个 Filter 就能产生几个输出。</p>
<p>以上就是卷积层的计算方法。这里面体现了局部连接和权值共享：每层神经元只和上一层部分神经元相连(卷积计算规则)，且 filter 的权值对于上一层所有神经元都是一样的。对于包含两个 3 * 3 * 3 的 fitler 的卷积层来说，其参数数量仅有(3 * 3 * 3 + 1) * 2 = 56 个，且参数数量与上一层神经元个数无关。与全连接神经网络相比，其参数数量大大减少了。</p>
<h2 id="Pooling层"><a href="#Pooling层" class="headerlink" title="Pooling层"></a>Pooling层</h2><h3 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/upload-images.jianshu.io/upload_images/2021/07/09/17-08-26-6eabc03098da799d01d575001b69fd07-2256672-03bfc7683ad2e3ad-626f59.png"></p>
<p>Max Pooling实际上就是在 n * n 的样本中取最大值，上面是在一个 2 * 2 的样本中选取最大的值。分别在红色、绿色、黄色、蓝色区域选取最大的值，组成了右侧的矩阵。</p>
<h3 id="Mean-Pooling"><a href="#Mean-Pooling" class="headerlink" title="Mean Pooling"></a>Mean Pooling</h3><p>Mean Pooling实际上就是在 n * n 的样本中取最平均值</p>
<h1 id="5、卷积神经网络的训练"><a href="#5、卷积神经网络的训练" class="headerlink" title="5、卷积神经网络的训练"></a>5、卷积神经网络的训练</h1><p>卷积神经网络的训练过程与推到见 <a href="https://www.zybuluo.com/hanbingtao/note/485480" title="零基础入门深度学习(4) - 卷积神经网络">https://www.zybuluo.com/hanbingtao/note/485480</a></p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>CNN</tag>
        <tag>卷积神经网络</tag>
      </tags>
  </entry>
  <entry>
    <title>end-to-end speech recognition</title>
    <url>/2021/06/30/end-to-end-speech-recognition/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/06/09-17-37-7aadc7cdf5911ab44e6edd0703223145-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210706091731-ba3f68.png"></p>
<a id="more"></a>

<h1 id="Automatic-Speech-Recognition（ASR）"><a href="#Automatic-Speech-Recognition（ASR）" class="headerlink" title="Automatic Speech Recognition（ASR）"></a>Automatic Speech Recognition（ASR）</h1><p>ASR就是将声学信号转化为文本的系统</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/30/15-08-20-a1cab161816ce6211cdadbe2436ab015-006Fmjmcly1fh46pi9l5rj31em0feaee-50d65b.jpeg"></p>
<p>语音是一种自然的用户接口</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/30/15-08-52-a5c819ff2d7dc4a3226a0538b7c84465-006Fmjmcly1fh46raonxhj30lc0563zx-573b45.jpeg"></p>
<h2 id="传统ASR"><a href="#传统ASR" class="headerlink" title="传统ASR"></a>传统ASR</h2><p>传统做法的主体是生成式语言模型，建模声学信号与文本的发音特征的联合概率，但pipeline的不同部分掺杂了不同的机器学习模型：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/30/15-09-37-d60602adbbe294a758cb2d337a54a60c-006Fmjmcly1fh46ynd1l7j31b80goq9z-984b59.jpeg"></p>
<h1 id="ASR模型"><a href="#ASR模型" class="headerlink" title="ASR模型"></a>ASR模型</h1><h2 id="0、前置知识"><a href="#0、前置知识" class="headerlink" title="0、前置知识"></a>0、前置知识</h2><h3 id="token"><a href="#token" class="headerlink" title="token"></a>token</h3><p>下面会频繁用到 token 这个单词，下面列举 4 类常用的 token。</p>
<p><strong>Phoneme</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/21-09-58-27d1044f97418806eddb4daf8de98b87-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705210941-006674.png"></p>
<p>Phoneme 是声学的基本单位。</p>
<p><strong>Grapheme</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/21-10-49-b5da781bc02892ef278fb69e3fead986-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705211041-fe99da.png"></p>
<p>对于英文就是一个英文字母，加上标点符号还有空格。对于中文就是一个汉字。</p>
<p><strong>word</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/21-11-28-6b952c4963a6ee1a1e1a8b631029cfa9-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705211119-a527a8.png"></p>
<p>对于英文就是一个英文单词，对于中文就是一个一个词组。</p>
<p><strong>Morpheme</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/21-12-42-ba17a4421d4a322d6a34c14023b1eb5d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705211235-c01bde.png"></p>
<p>Morpheme 是对 word 的更细的划分。</p>
<p>下面提到的都是 Grapheme 这个token</p>
<h3 id="token-在论文中使用情况"><a href="#token-在论文中使用情况" class="headerlink" title="token 在论文中使用情况"></a>token 在论文中使用情况</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/21-15-47-890521842dbe69533c9ce57f00991d3f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705211351-038ba9.png"></p>
<p>可以看出 Grapheme 在论文中所占的比例非常高。</p>
<h3 id="将音频信息转化为声学特征"><a href="#将音频信息转化为声学特征" class="headerlink" title="将音频信息转化为声学特征"></a>将音频信息转化为声学特征</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/06/09-13-44-40a0e629602890dbfa8bb98090d3fbaf-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210706091306-1b6ced.png"></p>
<p>音频信息可以通过上面的方法转化为声学特征。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/06/09-15-26-165358696e3cd41bc04315f5d850c11b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210706091520-fba357.png"></p>
<p>各种提取声学特征的方法在以往论文中所占的比例。</p>
<h2 id="1、LAS"><a href="#1、LAS" class="headerlink" title="1、LAS"></a>1、LAS</h2><h3 id="1、LAS-模型"><a href="#1、LAS-模型" class="headerlink" title="1、LAS 模型"></a>1、LAS 模型</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img-blog.csdnimg.cn/2021/07/05/19-36-50-7425c42b43d88b3d23662427741a389f-20191216220112519-86321e.png"></p>
<p>LAS 是 Listen, Attention, and Spell 的缩写，它利用了注意力机制来进行有效对齐</p>
<p>工作流程：</p>
<pre><code>将语音信号的特征输入到 Encoder 中（Listen部分）
做 Attention，在不同的时刻关注输入的不同部分（Attention部分）
通过 Decoder 进行解码(Spell部分)</code></pre>
<h3 id="2、Listen"><a href="#2、Listen" class="headerlink" title="2、Listen"></a>2、Listen</h3><p>Listen 部分对应的是 Encode-Decoder 模型中的 Encoder 部分，可以选择 RNN,CNN,CNN+RNN,Self-Attention</p>
<p><strong>Encoder 使用 RNN 模型</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/15-39-03-094f470acb6a1d7d6a34d3f13d5b9841-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704153829-e7e3ab.png"></p>
<p><strong>Encoder 使用 CNN 模型</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/06/09-23-55-e591d715829f2a8d8f17411f5c0b6e1e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210706092349-081fb9.png"></p>
<p><strong>Encoder 使用 Self-attention 模型</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/19-40-50-d00102a16f7ae9248b1fd37e3b81eef7-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705194043-f7f176.png"></p>
<p>如果每 10ms 对样本进行采样一次，那么 1s 就会采集到 100 个样本，这会导致两个问题，一个问题是会采集到大量的重复样本，另一个问题是如果采集数据的时间过长，会导致样本数目变的非常庞大。所以通常需要对声音进行降采样，以减少重复样本和样本数目。降采样的主要方法有 Pyramid RNN，Pooling over time，Time-delay DNN (TDNN)，Truncated Self-attention，前三种降采样一方面可以减少样本的数量，另一方面也可以合并重复的样本。</p>
<p><strong>Pyramid RNN</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/15-48-12-4bd734f799e27d7fd25184313a403ed9-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704154804-925817.png"></p>
<p>在每一层的RNN输出后，都做一个聚合操作。把两个向量加起来，变成一个向量。</p>
<p><strong>Pooling over time</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/15-49-15-0c6839ebf7cc4cb60fbbe5b354261060-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704154910-8ef8d2.png"></p>
<p>两个 time step 的向量，只选其中一个，输入到下一层。</p>
<p><strong>Time-delay DNN (TDNN)</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/15-55-41-0cb3ca6d2d46fc0c66e13d000f7908e9-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704155535-f62f70.png"></p>
<p>因为相邻的编码信息差不太多，为了减少运算量，只取两端的样本。</p>
<p><strong>Truncated Self-attention</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/15-58-00-d16f9cdb7c1980680779f7f420de3475-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704155601-824a7c.png"></p>
<p>为了减少注意力的范围，将注意力集中在当前输入样本附近的几个样本上。</p>
<h3 id="3、Attend"><a href="#3、Attend" class="headerlink" title="3、Attend"></a>3、Attend</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/16-05-05-1b9123a86fa2c4c3fa4f73729ddb89d0-20210704160027-f08145.png"></p>
<p>关键字 $z^i$ 对编码器中的每一个输出 $h^i$ 做 Attention，获得 $\alpha_i^j$,其中 $i$ 表示层数，$\alpha_i^j$ 表示 $h^j$ 与 $z^i$,经过 match 之后计算得到的 Attention 得分。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/16-18-10-7ced7509f58534acbbd8c1f03651ddbe-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704161801-0a65e2.png"></p>
<p>所有的 $\alpha_i^j$ 需要经过 softmax 得到最终的概率值 $\hat{\alpha_i^j}$，用 $\hat{\alpha_i^j}$ 和 $h^j$ 相乘做加和，得到最后的 Attention 输出 $c^i$</p>
<p><strong>match 的结构</strong></p>
<p>上面通过将 $h^j$ 和 $z^i$放到 match 中，来获得 Attention 得分 $\alpha_i^j$，match 结构有 Dot-product Attention 和 Additive Attention 两种结构</p>
<p><em>Dot-product Attention</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/16-24-00-ea3f7eec847aa5c393b1eb0952a47651-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704162316-cd5f84.png"></p>
<p><em>Additive Attention</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/16-25-32-832058a4d25c298b5f6257c4ba1b87aa-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704162339-9a774a.png"></p>
<h3 id="4、Spell"><a href="#4、Spell" class="headerlink" title="4、Spell"></a>4、Spell</h3><p>Spell 部分对应的是 Encode-Decoder 模型中得 Decoder 部分，通过这一步可以将得到向量解码成我们想要的单词。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/16-34-32-86da15953fd0654a0005b266a581d6b3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704163009-a208df.png"></p>
<p>根据 $z^0$ 与 $c^0$ 形成 $z^1$,通过 $z^1$ 来预测第一个输出，根据 $z^1$ 与 $c^1$ 形成 $z^2$,通过 $z^2$ 和上一次的输出来预测第二个输出，……。其中 $c^i$ 是 Attend 这一步通过 Attention 拿到的输出。对于每一个输出都是一个大小为 V 的向量，该向量中的每一个值，表示每个 token 的输出概率。由于词表较大，没有办法去搜索穷尽所有的可能性，为了非常快的找到使最后的输出概率最大的路径，可以采用下面两种搜索路径的方法。</p>
<p><em>Greedy Search</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/16-56-32-46576cf5f7bf9b8a74fd1841501a4666-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704165417-d41e4b.png"></p>
<p>每次都选择当前情况下的最优解，这种策略不能保证生成整个序列的概率是最优的。</p>
<p>红色的路径是根据 Greedy Search 得到的结果，显然绿色的路径才是最好的路径。</p>
<p>1处和2处求 A 的概率如下</p>
<pre><code>P(1) = p(A|A) = 0.6
p(2) = P(A|B) = 0.1
所以两处的概率是不同的</code></pre>
<p><em>Beam Search</em></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-06-27-3460259cd4d2c861dacf9597d6d13dab-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704170439-1f4cdd.png"></p>
<p>为了解决 Greedy Search 的问题，可以使用 Beam Search 的搜索方法。</p>
<p>Beam size = 2 使每次都保留两个概率最大的路径</p>
<p>计算过程如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-11-25-eb0528819a554eadf11b141a4daf35ae-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704171120-cc0835.png"></p>
<p>最终得到了两个备选选项。</p>
<h3 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-16-28-5b332a61673940203d8ab1aa810133a2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704171613-3a65b5.png"></p>
<p>将样本输入到编码器中，得到 $h^1,h^2,h^3,h^4$,通过 $z^0$ 使用 Attention，得到 Attention 的输出 $c^0$,通过 $z_1$ 得到此表中 token 的概率分布，取出概率最大的对应的 token 作为输出。在这里我门期望 c 的概率是最大的。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-26-28-9d9d3e29af18622e888e0c7f068c122b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704172402-e3c805.png"></p>
<p>在起始状态，所有输出的概率是混乱的，并不知道会输出什么词,这就有可能使输出 a 的时候发生混乱，本应该是在看到 c 的时候输出 a，但是最初的时候 ？ 有可能是 b，也有可能是 d，那就意味着在看到 b 的时候输出 a，或者是在看到 d 的时候输出 a，这与在看到 c 的时候输出 a 是矛盾的。可以通过每次都把真实的标签作为下一个 $z^i$ 的输入来纠正这个问题。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-32-26-78c4f4323473407df723e7426d66bcda-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704173057-53117b.png"></p>
<p>这样就可以确定在输入 c 的时候输出 a 了。</p>
<h3 id="Attention-相关"><a href="#Attention-相关" class="headerlink" title="Attention 相关"></a>Attention 相关</h3><p>关于 Attention 作用的位置，有两种模型。一种是通过 $z^i$ 生成 $c^i$ 作用于下一个隐层状态,一种是通过 $z^i$ 生成 $c^{i+ 1}$ 作用于当前的隐层状态。</p>
<p><strong>第一种 Attention</strong></p>
<p>把 Attention 的输出作用于下一时刻的隐层状态</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-40-23-145e2817beaeb96b1630857268366d2e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704174014-1a294f.png"></p>
<p>把 Attention 的输出作用于当前时刻的隐层状态</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-40-51-47cde875101127fe77971fb90ac4c430-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704174044-2cd04d.png"></p>
<p>但是在 LAS 中是全都要</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/17-41-48-5c38be82f340ccf3745c9319682e1b0c-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704174123-ef6263.png"></p>
<p>语义识别和机器翻译的的一个非常重要的不同是，在语音识别中不会像机器翻译那样出现交叉对齐的情况（即在翻译的时候原文的第一个词可能对应着翻译的最后一个词）</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-19-13-abaea97a0aba4482277f5047e4b4dd5a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704191859-a53afe.png"></p>
<p>理想情况下，做完 Attention 会出现上面的情况，即只与当前输入的附近几个样本有关。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-24-59-90d647b5521c7a9a7b2b0a93ad72eb1f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704192451-98ae4a.png"></p>
<p>但是做完 Attention 后，可能会出现上面的情况</p>
<p>有可能会出现与当前的输入无关，也可能出现与当前输入的附近几个样本无关</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-25-50-d81cd3601807befb77e0bb7897e4b72f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704192544-d4114e.png"></p>
<p>针对上面的情况，LAS做了上图所示的修改</p>
<h3 id="LAS是否有效"><a href="#LAS是否有效" class="headerlink" title="LAS是否有效"></a>LAS是否有效</h3><p><strong>在小的数据集上</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/06/09-50-49-ace792f583fab927fb59a5825c3754ad-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210706095044-8b048f.png"></p>
<p>似乎不是非常的有效</p>
<p><strong>在大的数据集上</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/20-15-39-40c433c9ac66740a874a6e5e83a865d2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705201531-a8cdb0.png"></p>
<p>无论是在错误率还是内存占用上，都取得非常好的效果。</p>
<h3 id="LAS-的限制"><a href="#LAS-的限制" class="headerlink" title="LAS 的限制"></a>LAS 的限制</h3><pre><code>LAS 的输出依赖于完整的 Encoder输出
LAS 无法实现在线语音识别功能</code></pre>
<h2 id="2、CTC"><a href="#2、CTC" class="headerlink" title="2、CTC"></a>2、CTC</h2><h3 id="CTC-模型"><a href="#CTC-模型" class="headerlink" title="CTC 模型"></a>CTC 模型</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-36-00-5c9c340037599702e5174c3c5b1439bf-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704193554-19c641.png"></p>
<p>CTC 能够进行在线语音识别，编码器采用了单向 RNN（双向的RNN无法进行在线语音识别），需要注意的是 CTC 只有编码器，没有解码器。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-41-45-4590c3a1b9c57855b9dca77ff20016b0-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704194140-d24dea.png"></p>
<p>如果每 10ms 取一个样本，时间非常短，所以在这一个样本中可能不会存在任何信息，所以在词组中加入了 $\phi$ 这个字符，用来表示输入对应的输出没有什么数据。</p>
<h3 id="CTC模型的输入与输出"><a href="#CTC模型的输入与输出" class="headerlink" title="CTC模型的输入与输出"></a>CTC模型的输入与输出</h3><p>CTC 模型的输入是 T 个声学特征，输出是 T 个Token。输出包括 $\phi$,需要先合并重复的字符，然后移除掉 $\phi$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-42-44-129c638dca86bcf11b3140c0fdfe2b8a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704194234-a80803.png"></p>
<p>对于 𝜙 𝜙 d d 𝜙 e 𝜙 e 𝜙 p p 这个输出来说，可采取下面两步得到 deep 这个单词</p>
<pre><code>1、合并重复字符
    𝜙 d 𝜙 e 𝜙 e 𝜙 p
2、移除 𝜙
    d e e p</code></pre>
<h3 id="CTC-的损失函数"><a href="#CTC-的损失函数" class="headerlink" title="CTC 的损失函数"></a>CTC 的损失函数</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-47-57-d429eb12a73a7faffc6d353ef7722b26-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704194748-3a56a5.png"></p>
<p>CTC 的损失函数是交叉熵，通过最小化交叉熵来获取最优解。</p>
<h3 id="预测训练数据"><a href="#预测训练数据" class="headerlink" title="预测训练数据"></a>预测训练数据</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-49-55-c5f879875fe6fbd7060fd75d527340ba-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704194949-699e95.png"></p>
<p>对于 <em>好棒</em> 这样的一个输出来说，会有非常多的路径可以得到这样的一个结果，上图列出的几条路径通过化简都可以得到 <em>好棒</em> 这样的一个输出。</p>
<h3 id="枚举所有可能的对齐"><a href="#枚举所有可能的对齐" class="headerlink" title="枚举所有可能的对齐"></a>枚举所有可能的对齐</h3><p>有非常多的对齐方式可以得到最后的输出，对于 CTC 来说，是全都要，将所有路径的概率全部加起来，就是最后输出的概率。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/19-58-34-e763591002c09b481de5747e76e92b77-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704195826-900dd4.png"></p>
<p>Y 是所有的可能的路径</p>
<p><strong>生成输出的规则</strong></p>
<p>首先，我们构造一个 table，希望通过这个 table，直观的看出可以映射到真实标签序列 ‘cat’ 的可能路径。table的横坐标为输入的时间序列，纵坐标为将真实标签序列两两字母以’-‘分隔，并且在首尾各加一个’-‘。</p>
<p>通过下面的五条规则可以得到所有正确的路径</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/20-23-13-2b2f51431055302084fa7f399aaeec29-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704202305-2a8c62.png"></p>
<p>起点必须是纵坐标的前两个 token 中的任意一个。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/20-04-07-d32fac1d9c43b1ff64457312fe916604-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704200357-049467.png"></p>
<p>当前时刻的 Token 不是 $\phi$,那么会有三种选择，如上图所示</p>
<p>三种情况举例</p>
<pre><code>1、cc𝜙at𝜙
2、c𝜙at𝜙𝜙
3、cat𝜙𝜙𝜙</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/20-08-00-24286fef4e1fc2d52754b2d59fcd3f02-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704200752-2700cb.png"></p>
<p>当前时刻的 Token 是 $\phi$,那么会有两种选择，如上图所示</p>
<p>两种情况举例</p>
<pre><code>1、𝜙𝜙cat𝜙
2、𝜙cat𝜙𝜙
3、这种情况是不合法的，因为在产生的输出中会缺少 c 这个 token</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/20-11-03-d1d564fefdd07ce952e4ee3494de093e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704201056-cb030a.png"></p>
<p>对于 see 这样的输出，存在着两个重复的 token，则采用上面的两条规则</p>
<p>两种情况举例</p>
<pre><code>1、s𝜙ee𝜙e
2、s𝜙e𝜙𝜙e
3、第三种情况是不合法的，这种情况会合并相邻的重复的 token</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/20-24-09-11cc413f1eaad5c1db2b4483fb0eb567-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704202404-61e964.png"></p>
<p>结尾必须是纵坐标的后两个 token 中的任意一个。</p>
<p><strong>生成路径总结</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/04/20-17-06-450d363437efd233ff14043e54583375-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210704201656-2ce7cd.png"></p>
<p>CTC 生成路径的方法，大致可以通过上图进行总结，起始点可以是 token 也可以是 $\phi$,结尾可以是 token 也可以是 $\phi$，中间可以有多个相同 token 也可以有多个 $\phi$，但是一条完整的路径必须包含正确输出的所有 token</p>
<h3 id="计算所有对齐的和"><a href="#计算所有对齐的和" class="headerlink" title="计算所有对齐的和"></a>计算所有对齐的和</h3><p>这一部分使用到了隐马尔可夫模型中的前向-后向算法，并且使用了 <a href="https://www.cnblogs.com/shiyublog/p/10493348.html" title="CTC (Connectionist Temporal Classification) 算法原理">https://www.cnblogs.com/shiyublog/p/10493348.html</a> 博客中数据，讲解 CTC 中的前向和后向算法</p>
<p><strong>前向算法</strong></p>
<p>定义 $\alpha_t(s)$ 为前向变量，表示在 $t$ 时刻到达序列的第 s 个位置的所有可能子路径的概率和。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/14-45-40-3067e5b13e68c3fb64d536df84139c19-1453927-20190308083653176-751249777-763c22.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/14-48-41-fa53e7153acd6c623061983745cf3e93-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705144742-0ef675.png"></p>
<p>可以通过 <em>生成输出的规则</em> 一节，生成下面所有列出的路径</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/14-50-54-1770e09a494ded0407d202280c63e993-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705145032-f2933d.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/14-51-06-1f9ecd79d7b6ef7a9cf5a3fdbb404c94-1453927-20190308085701438-1886927983-8a5de0.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/14-51-48-022f45a22edbb4cb6128dce2a7aa442f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705145140-9dcaef.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/14-52-02-04dfe267a5ea11dde432ab058e14bcaa-1453927-20190308091045087-4112545-17e115.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/14-52-39-410ee06e9947cbc797ebad716f1a7c4c-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705145232-7b4aaa.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/14-52-54-e07a8dca885289901243ebbbe3f56e96-1453927-20190308091534188-1624942636-ab0690.png"></p>
<p><strong>后向算法</strong></p>
<p>定义 $\beta_t(s)$ 为后缀起始于序列末端， t 时刻到达第 s 个符号的所有可能子路径的概率和。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/14-59-11-7854ee90d6b66ed023701763b2732616-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705145903-8b1ffa.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/14-59-39-074e3110ca9b329709808d483e357acc-1453927-20190308101855990-529603551-caa21e.png"></p>
<p>那么，将前向过程中所有箭头反向，使用同样的计算方式，即可计算出反向变量。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/15-01-13-1f27cf020f00532ad0e3ffcc6483b079-1453927-20190308102237401-914286549-dcf475.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-04-26-e19d9d176e564820b602155f6782287f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705150418-4f3cb8.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/15-04-41-4e37464778369d36f66015cac5701837-1453927-20190308105558817-1310458424-90a9d4.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-06-28-3397bc107af6f7b538474c673c0b230d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705150621-4efc41.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/15-06-43-46a4356152937e0e36ce2f5bb6deb183-1453927-20190308110012287-638554946-1ba38f.png"></p>
<p>最后，应用于1 - T的所有时刻，可以得到在任意时刻内预测出正确标签序列的概率。</p>
<p>$p(‘apple’) = \sum_{s = 1}^{|seq|} \frac{\alpha_t(s) * \beta_t(s)}{y_{seq(s)} ^ t}$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-13-48-da22ee1fb7ea1b5a4276a6b3e2e68199-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705151342-879bd5.png"></p>
<p>举个例子：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1453927/201903/2021/07/05/15-14-12-9f1c9ed575798370f55c6701e365eefd-1453927-20190308133939869-524172403-506758.png"></p>
<p><strong>最后的推到步骤如下：</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/i0.wp.com/xiaodu.io/wp-content/uploads/2018/07/2021/07/05/20-55-35-14d1c3ce2bd93e9755b278c832e82472-rnn_of3-3fca25.png"></p>
<h2 id="3、RNN-T"><a href="#3、RNN-T" class="headerlink" title="3、RNN-T"></a>3、RNN-T</h2><h3 id="RNA模型"><a href="#RNA模型" class="headerlink" title="RNA模型"></a>RNA模型</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-19-04-85c9d1d4c61f67a117152ef70d029d2d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705151857-cca00a.png"></p>
<p>由于 CTC 模型当前 token 的生成不依赖前面的 token，所以 RNA 解决了 CTC 的不足。</p>
<h3 id="RNN-T模型"><a href="#RNN-T模型" class="headerlink" title="RNN-T模型"></a>RNN-T模型</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-21-17-49e6159fb9eab717de0fa85dad90ee66-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705152105-b2dcac.png"></p>
<p>无论是 RNA 模型，还是 CTC 模型，他们都是吃一个输入，就输出一个 token，RNN-T 实现了吃一个输入，就输出多个 token。一个输入对应着多个 token，当没有 token 需要输出的时候，需要输出 𝜙 当作结束符。</p>
<h3 id="RNN-T-的运行流程"><a href="#RNN-T-的运行流程" class="headerlink" title="RNN-T 的运行流程"></a>RNN-T 的运行流程</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-24-55-23a4fe4e45247b40d3656a63f1cb3233-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705152442-3a52a8.png"></p>
<p>每个输入都至少对输出一个 𝜙 。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-31-19-d50d08f13a8e5b6e7e4fdc92924333f2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705153110-f392ef.png"></p>
<p>注意画黄线方框的部分，𝜙 是不会影响下一个字符的生成的。这样的好处是在生成 e 这个 token 的时候，只受到 t、h 两个 token 的影响，而不会受到 𝜙 这个token的影响（因为 𝜙 生成的位置和数量都是不固定的）</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-37-52-b226d530ce08c531fd6a749c494a5e9d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705153729-bbc857.png"></p>
<p>这样可以保证，无论从哪个方向到达的 $p_{4,2}$,生成 $p_{4,2}$ 的概率都是一样的（下面会详细说明）。</p>
<p><strong>如何生成输出</strong></p>
<p>RNN-T 和 CTC 生成输出的方式是不一样的，有以下两条规则。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/21-00-25-473c8bbdeb3314d2fdba3e8eb35b7300-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705154614-b684f2.png"></p>
<p>红色方框和黄色方框分别画出了 RNN-T 的起始点与结束点。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-47-11-0fb142e2088135a9570bd143592401dc-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705154704-c9b6b9.png"></p>
<p>RNN-T只能往右走或者是竖着走，不能斜着走。</p>
<p><strong>生成路径总结</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-50-36-a93e50273395655e7e3c5fa4559dc3d2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705154955-97bc85.png"></p>
<p>将 CTC 与 RNN-T 进行对比可以发现，RNN 在每个 token 上面是没有循环的，而且 RNN-T 最后一个字母必须为 𝜙，因为 RNN-T 把 𝜙 作为每一个输入的结束。</p>
<h3 id="计算所有对齐的和-1"><a href="#计算所有对齐的和-1" class="headerlink" title="计算所有对齐的和"></a>计算所有对齐的和</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/15-54-38-7f5762eadd203de497f21e0d18c8ddd9-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705155430-f13aed.png"></p>
<p>在 RNN-T 中计算一条完整路径的概率</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-02-17-e4c3f3525cd86761a73afc73ff593836-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705160208-8b4136.png"></p>
<p>这里的 $h^i$是 Encoder 的输出，$l^i$是下图中黄线所标记的 RNN，可以看出这个 RNN 与前面 𝜙 出现的位置和数目没有任何关系。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-05-12-e81e82426edd8aed7c54554a77c76ca3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705160503-1b1354.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-07-39-e1780c7dd3f60e21b552b5c6a7085fd5-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705160730-bc6158.png"></p>
<p>对于 $p_{4,2}$ 这一个点的概率，无论选择走那条路径，对于下图中的 $l^2$ 来说是不变的，$h^4$ 也不会发生变化，那么 $p_{4,2}$ 也不会发生变化</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-09-32-a3f15f51d9db83d5513f66ade50f259f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705160918-10b4a6.png"></p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-15-28-bd97616c32e75e1c2702c12c0ac0b11a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705161517-63b8a2.png"></p>
<p>需要优化的函数，其中$P(\hat{Y}|X)$是一条正确的输出路径的概率。同样是要所有对齐路径的概率和。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-35-17-748899d9d325ceb2dd7f25dfa9c4e187-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705163345-1b5a0e.png"></p>
<p>正如黄色方框中所显示的那样，上图表格中的每一个箭头都代表一个几率，每个几率都能够影响 $P(\hat{Y}|X)$,而每一个几率都受到 $\theta$ 的影响，首先对 $\frac{\partial{P_{4,1}(a)}}{\partial{\theta}}$ 求偏导</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-45-29-0ce2fdc736a942bea0ce3a5072497b86-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705164518-9ae396.png"></p>
<p>$P_{4,1}(a)$ 的值受到上方 RNN 生成的 $l^1$ 的影响，还受到下方 Encoder 生成的 $h^4$的影响，所以根据反向传播，更新变量参数。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/16-49-52-ed0d1f558b365643bc8f972e5bfac8dc-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705164944-849dd8.png"></p>
<p>接下来求 $\frac{\partial{P(\hat{Y}|X)}}{\partial{P_{4,1}(a)}}$,由于有些路径是经过$P_{4,1}(a)$这个箭头的，有的路径是不经过的，将两条路径分离出来，那么前一部分就与 $P_{4,1}(a)$，有关，另一部分与 $P_{4,1}(a)$ 无关，再把与 $P_{4,1}(a)$ 有关的部分把 $P_{4,1}(a)$ 提取出来，最后对 $P_{4,1}(a)$ 求偏导</p>
<p>计算所有经过 $P_{4,1}(a)$ 路径的概率和，还是使用 HMM 的前向算法和后向算法</p>
<p><strong>前向算法</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/17-15-18-70a7a5f4dfe1825618295f30ceb75ad2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705171511-222a28.png"></p>
<p>$\alpha_{4,2}$ 是所有能产生 4 个声学特征，产生 2 个 token 的概率路径的总和。</p>
<p><strong>后向算法</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/17-03-31-0464f14342730738858cb9911ff3717f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705170321-98cc6e.png"></p>
<p>$\beta_{4,2}$ 是已经产生 4 个声学特征，已经产生 2 个 token 的概率，最后经过所有的可能路径能够到达结尾的所有路径的概率和。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/17-11-36-cc5b26aea373cb0f7bbdc745dcf2c2ca-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705171129-1bef42.png"></p>
<p>根据前向算法产生的 $\alpha_{4,1}$ 和后向算法产生的 $\beta_{4,2}$ 来求$\frac{\partial{P(\hat{Y}|X)}}{\partial{P_{4,1}(a)}}$。</p>
<h2 id="4、Neural-Transducer"><a href="#4、Neural-Transducer" class="headerlink" title="4、Neural Transducer"></a>4、Neural Transducer</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/17-18-34-26f9ff6b341992ad04410273a8ced425-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705171825-28f053.png"></p>
<p>RNN-T 是根据一个输入生成多个输出，而 Neural Transducer 则是根据多个输入生成多个输出。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/07/05/17-20-48-1ef461134c8dea78f441f484a0fd8ef4-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210705172040-b18bc4.png"></p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>端到端</tag>
        <tag>end-to-end</tag>
        <tag>Speech Recognition</tag>
        <tag>语音识别</tag>
      </tags>
  </entry>
  <entry>
    <title>GRU AND NMT Advance</title>
    <url>/2021/06/29/GRU-AND-NMT-Advance/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/30/14-38-52-bd315ad1a71aff2eae980dd68acb6a81-QQ%E6%88%AA%E5%9B%BE20210630143830-6233fc.png"></p>
<a id="more"></a>

<h1 id="深入GRU"><a href="#深入GRU" class="headerlink" title="深入GRU"></a>深入GRU</h1><p>把GRU再详细讲一讲。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/29/21-32-05-f009f3034e3fd2283498fedc4d4b68c9-006Fmjmcly1fh26tkuf3mj31ay0r8qat-c17a45.jpeg"></p>
<p>RNN的梯度消失就不赘述了，红线连乘多次下溢出。</p>
<p>而GRU额外添加了一些“捷径”红线，允许梯度直接流过去，而不是连乘的方式递减过去。</p>
<h2 id="Update-Gate"><a href="#Update-Gate" class="headerlink" title="Update Gate"></a>Update Gate</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/29/21-32-53-e99311e33af9dd235915798e8051a907-006Fmjmcly1fh279bre16j318m0ju0zq-c8f916.jpeg"></p>
<p>用来自适应学习应该把多少注意力放到前一个隐藏层状态上。</p>
<h2 id="Reset-Gate"><a href="#Reset-Gate" class="headerlink" title="Reset Gate"></a>Reset Gate</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/29/21-33-55-49de4b596be349726d0b091e32a4b3c0-006Fmjmcly1fh2ylvadzrj319u0h6agh-07cae5.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/29/21-34-57-d2420576a632bbc23fb4b4f5d1d4f90e-006Fmjmcly1fh301jbqhij31cq0jg0wn-d1b2cd.jpeg"></p>
<p>门多了之后，就可以灵活地选择读取部分寄存器，执行运算，写入部分寄存器。</p>
<p>Reset Gate起到决定要读哪些寄存器的目的，而Update Gate决定要写的寄存器。这里的“决定”其实是“强度”的意思，不是绝对的。</p>
<h1 id="深入LSTM"><a href="#深入LSTM" class="headerlink" title="深入LSTM"></a>深入LSTM</h1><p>宏观上的LSTM Cell：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/29/21-36-47-9f57fbb9c5a0c41eadb2cc4e7e5a546a-006Fmjmcly1fh379ttwk0j30i40iv42d-a289a2.jpeg"></p>
<p>将所有操作都gate起来，方便遗忘甚至忽略一些信息，而不是把所有东西都塞到一起。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/29/21-37-12-5c4ea91b8df24dc02ea6171fee5f53d1-006Fmjmcly1fh37ds6tjlj30hp0ipaeg-0f5862.jpeg"></p>
<p>New Memory Cell的计算是一个非线性的过程：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/29/21-37-41-acbd78154c0cb380c7c77459e5ff8d17-006Fmjmcly1fh37mpm9ylj30ho0isdju-00ba1d.jpeg"></p>
<p>最关键之处在于，Memory Cell的更新中有一个加法项直接来自上一刻的Cell，也就是说建立了ct和ct−1的直接线性连接（与ResNet类似）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/29/21-37-59-6afe95883c3c720f8a377690ea0aa6f2-006Fmjmcly1fh37r3ydazj30hv0in78c-5219c2.jpeg"></p>
<p>类似于GRU中的加法，在反向传播的时候允许原封不动地传递残差，也允许不传递残差，总之是自适应的。</p>
<p>有了这些改进，LSTM的记忆可以比RNN持续更长的step（大约100）：</p>
<h1 id="MT评测"><a href="#MT评测" class="headerlink" title="MT评测"></a>MT评测</h1><p>以前人们认为交给人类译员来打分是最好的，但这太主观了，10个译员给出的翻译可能都不相同。</p>
<h2 id="BLEU"><a href="#BLEU" class="headerlink" title="BLEU"></a>BLEU</h2><p>后来IBM发明了一种简单有效的评价策略，叫BLEU。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/29/20-39-03-e959e2f47e19de2cc496082c14328136-006Fmjmcly1fh3doomj4kj30im0ta79e-a74a75.jpeg"></p>
<p>通过比较标准译文与机翻译文中NGram的重叠比率（0到1之间）来衡量机翻质量。</p>
<h2 id="Brevity-Penalty（BP）"><a href="#Brevity-Penalty（BP）" class="headerlink" title="Brevity Penalty（BP）"></a>Brevity Penalty（BP）</h2><p>是否可以通过输出大量无意义的the之类来作弊？不，通过Brevity Penalty来防止机翻比译员译文短。</p>
<p>一般取4-gram之内参与评测，最终的分值是所有ngram分值的几何平均乘上一个Brevity Penalty：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/29/20-40-06-8832425989bf275cfc29b8d9db846599-006Fmjmcly1fh3dppisxdj30t80oo79p-b07826.jpeg"></p>
<h2 id="BLEU-和-BP-的计算"><a href="#BLEU-和-BP-的计算" class="headerlink" title="BLEU 和 BP 的计算"></a>BLEU 和 BP 的计算</h2><p>计算公式</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/20-42-24-5f5ec3f303a7102b91b27ae7af09f913-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629204218-a4dd66.png"></p>
<p>其中 BP 的计算公式是当$c \leq r 时，为 e^{1-\frac{r}{c}}$,后边的 r，c是一个整体</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/20-48-01-4e1ff3c259a9e12d3bd417a7df83ec78-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629204755-3491e5.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/20-59-15-e358833e774f3ac6f05a92f5332f615c-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629204839-6ae3a2.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/20-53-00-bc8f2c161d9eda0c658d8318d8dc959d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629205130-4fab46.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/21-01-02-18afd9801cdca88151435cdea3f4a0d4-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629205958-b2d8cd.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/20-57-24-eefb09c8031f37d24d3db63a35e514ff-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629205612-d251bb.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/21-01-53-ed262d9c21b1e7f2f22fc7fe02e06c93-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629210146-b27a1c.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/29/21-02-21-1347d3bfab8f29fb7f685028d551a2cb-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210629210215-545c17.png"></p>
<h2 id="BP代码实现"><a href="#BP代码实现" class="headerlink" title="BP代码实现"></a>BP代码实现</h2><p>求 BP 的代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">def _modified_precision(candidate, references, n):</span><br><span class="line">    counts &#x3D; Counter(ngrams(candidate, n))</span><br><span class="line"></span><br><span class="line">    if not counts:</span><br><span class="line">        return 0</span><br><span class="line"></span><br><span class="line">    max_counts &#x3D; &#123;&#125;</span><br><span class="line">    for reference in references:</span><br><span class="line">        reference_counts &#x3D; Counter(ngrams(reference, n))</span><br><span class="line">        for ngram in counts:</span><br><span class="line">            max_counts[ngram] &#x3D; max(max_counts.get(ngram, 0), reference_counts[ngram])</span><br><span class="line"></span><br><span class="line">    clipped_counts &#x3D; dict((ngram, min(count, max_counts[ngram])) for ngram, count in counts.items())</span><br><span class="line"></span><br><span class="line">    return sum(clipped_counts.values()) &#x2F; sum(counts.values())</span><br></pre></td></tr></table></figure>
<h3 id="BLEU的计算"><a href="#BLEU的计算" class="headerlink" title="BLEU的计算"></a>BLEU的计算</h3><h2 id="Multiple-Reference-Translations"><a href="#Multiple-Reference-Translations" class="headerlink" title="Multiple Reference Translations"></a>Multiple Reference Translations</h2><p>为了防止某篇机翻实际上很好，可就是跟人类译文用词行文不相似的情况，IBM的论文建议多准备几篇标准答案，这样总会撞上一个：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/29/20-41-15-fd67588f9d53939ca0e80ec068df9ab8-006Fmjmcly1fh3dvvdsv0j31a20u0dtk-f8b371.jpeg"></p>
<h1 id="解决大词表问题"><a href="#解决大词表问题" class="headerlink" title="解决大词表问题"></a>解决大词表问题</h1><p>大词表问题指的是softmax的计算难度：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/29/21-07-48-750c28ec597a57f56f64511e7f04bc19-006Fmjmcly1fh3hecak26j30od0etwi2-78fd99.jpeg"></p>
<p>早期的MT系统会使用较小的词表，但这并不是解决问题，而是逃避问题。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/29/21-08-07-9b4cc78572cd696123a963ce0f9404d1-006Fmjmcly1fh3hfv049aj30o60cqwh8-390bbe.jpeg"></p>
<p>另一种思路是，hierarchical softmax，建立树形词表，但这类方法过于复杂，让模型对树形结构敏感而不是对词语本身敏感。</p>
<h2 id="Large-vocab-NMT"><a href="#Large-vocab-NMT" class="headerlink" title="Large-vocab NMT"></a>Large-vocab NMT</h2><p>最新的方法是训练时每次只在词表的一个小子集上训练，因为40%的词语只出现一次，如果把训练数据均分为许多份，则每一份中的稀有词可能都不会在其他语料中出现。然后测试时加一些技巧。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/29/21-11-01-7143c0386ddfd057b1ea7093a079aaf1-006Fmjmcly1fh3hspe86yj30sq0csq5v-0a9e45.jpeg"></p>
<p>如何选择小词表呢？在刚才的方法上更进一步，让用词相似的文章进入同一个子集，这样每个子集的词表就更小了。</p>
<p>测试</p>
<p>测试的时候先雷打不动将前K个最常使用的单词加入备选词表，然后将原文中每个单词可能的前K′个翻译加进去。最后在备选词表上softmax。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/29/21-16-33-68175d48113b4971eb189f102b156aa2-006Fmjmcly1fh3i0897abj31b80sc0wo-324545.jpeg"></p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>LSTM</tag>
        <tag>GRU</tag>
        <tag>NMT</tag>
        <tag>MT评测</tag>
      </tags>
  </entry>
  <entry>
    <title>NMT And Attention</title>
    <url>/2021/06/28/NMT-And-Attention/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/17-22-59-213aec43c17a297eeb2e153f45e99dc0-QQ%E6%88%AA%E5%9B%BE20210628172251-6e8dab.png"></p>
<a id="more"></a>


<h1 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h1><pre><code>传统衡量机器对语言理解的测试之一
同时涉及到语言分析与理解
一个每年400亿美金的产业
主要在欧洲，亚洲也在兴起</code></pre>
<h2 id="机器翻译的需求"><a href="#机器翻译的需求" class="headerlink" title="机器翻译的需求"></a>机器翻译的需求</h2><pre><code>Google每天翻译1000亿单词
Facebook研发了自己的翻译系统，因为通用的机器翻译系统无法适应社交领域
eBay用机器翻译来促进跨境交易</code></pre>
<h1 id="什么是NMT"><a href="#什么是NMT" class="headerlink" title="什么是NMT"></a>什么是NMT</h1><p>抽象的架构就是一个encoder一个decoder：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/28/17-26-18-ecc7dc425f0b9e35df71cf6f4213b118-006Fmjmcly1fgzs95nj6uj31dw0h4q4q-fc3f66.jpeg"></p>
<h2 id="NMT-青铜时代"><a href="#NMT-青铜时代" class="headerlink" title="NMT:青铜时代"></a>NMT:青铜时代</h2><p>80年代神经网络是个很边缘的领域，另外计算力也很有限。当时的NMT系统只是个玩具：词表四五十，固定的50个输入（二进制编码），固定的66个输出，一到三层隐藏层，150个单元……</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/17-28-15-3b519db477d0d255529da78f386eb4b0-006Fmjmcly1fgzsp3ng38j30h40rqqa4-ce7511.jpg"></p>
<p>90年代出现了一种类似RNN的更复杂的框架：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/28/17-29-02-68701e7f1051beee1637c9672d12e360-006Fmjmcly1fgzsryhke6j31ce0qman9-571767.jpeg"></p>
<h2 id="现代NMT模型"><a href="#现代NMT模型" class="headerlink" title="现代NMT模型"></a>现代NMT模型</h2><p>之前课上也提到过，一个RNN做encoder，另一个RNN做decoder：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/28/17-30-29-cc0174ea27f10d2097340750ccf21a08-006Fmjmcly1fgzszvodr6j30p80bp75k-832d93.jpeg"></p>
<p>实际使用的系统更加复杂：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/28/17-30-49-30f3a54825a31b43859de2a682ba6533-006Fmjmcly1fgzt1in82bj31ee0mathi-0008b1.jpeg"></p>
<p>这里的RNN可视作以原文为条件的conditional语言模型</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/28/17-35-03-1d82967585b4365e4e2a029ba26ced54-006Fmjmcly1fgzt4fg1zrj30o60dn79s-0e64c9.jpeg"></p>
<h2 id="RNN-Encoder"><a href="#RNN-Encoder" class="headerlink" title="RNN Encoder"></a>RNN Encoder</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/28/17-34-44-faa5192c934f75d1c6253791cfdf03a8-006Fmjmcly1fgzt7kl6x4j318y0cqgnp-8e84fc.jpeg"></p>
<p>最后一个隐藏层的状态Y是整个原文的总结。</p>
<h2 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h2><p>常见的做法是把encoder的最后一层（最后一个时刻）作为decoder的第一层，这样就必须用LSTM保持中期记忆。</p>
<p>另一种做法是将encoder最后一层喂给decoder的每一层，这样就不会有记忆丢失的后顾之忧了。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/28/17-37-50-c328e1bbf299e726567f97c0fc57a5e9-006Fmjmcly1fgztg6wu3dj310u0cgjtk-e9d2dc.jpeg"></p>
<h2 id="MT的发展"><a href="#MT的发展" class="headerlink" title="MT的发展"></a>MT的发展</h2><p>基于短语的MT就是2016-11之前的Google翻译所采用的系统，其发展是缓慢的。神经网络兴起之后，出现了一种基于Syntax-based SMT（估计是换换词向量），发展也不快。但NMT的发展是最迅猛的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/28/17-39-04-304a301d7b9e3c85493abdc90529e68a-006Fmjmcly1fgztoujd7sj30ow0g0diz-5f87cf.jpeg"></p>
<p>NMT的四大优势</p>
<pre><code>End-to-end training
    为优化同一个损失函数调整所有参数
Distributed representation
    更好地利用词语、短语之间的相似性
Better exploitation of context
    利用更多上下文——原文和部分译文的上下文
生成的文本更流畅
    可能跟上述优势有关</code></pre>
<p>NMT也存在弱点</p>
<pre><code>无法显式利用语义或语法结构（依存句法分析完全用不上了，有些工作正在展开）
无法显式利用指代相消之类的结果</code></pre>
<h1 id="统计-神经网络机器翻译"><a href="#统计-神经网络机器翻译" class="headerlink" title="统计/神经网络机器翻译"></a>统计/神经网络机器翻译</h1><p>Manning说除了英语之外，学生中第二大语种是中文，而且他亮出了简体中文的例子，真是让人激动啊。他还特意在不同年份测试了google翻译的效果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/28/17-45-05-627c74d9183e5a152035b57ac7897afe-006Fmjmcly1fgzudxxklwj31bk0q6tjt-e9ddc2.jpeg"></p>
<p>其中，13年有所进步，14-16年又退步了并且停滞了3年。直到2017年才有质的飞跃。</p>
<h2 id="NMT主要由工业界促进"><a href="#NMT主要由工业界促进" class="headerlink" title="NMT主要由工业界促进"></a>NMT主要由工业界促进</h2><pre><code>2016-02 微软在Android和iOS上发布了离线NMT系统，这对境外旅游人士特别有帮助。
2016-08 Systran发布了NMT模型
2016-09 Google发布了NMT系统，大肆宣传了一番，并且overclaim比得上人工翻译质量。Manning真是直言不讳啊。</code></pre>
<h1 id="介绍Attention"><a href="#介绍Attention" class="headerlink" title="介绍Attention"></a>介绍Attention</h1><p>朴素encoder-decoder的问题是，只能用固定维度的最后一刻的encoder隐藏层来表示源语言Y，必须将此状态一直传递下去，这是个很麻烦的事情。事实上，早期的NMT在稍长一点的句子上效果就骤降。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/28/19-45-57-54a88fd9cfc13007c9d816b21d5bd6da-006Fmjmcly1fgzwgwjmunj30te0iwjtb-50a328.jpeg"></p>
<p>解决方法是将encoder的历史状态视作随机读取内存，这样不仅增加了源语言的维度，而且增加了记忆的持续时间（LSTM只是长时记忆）。</p>
<p>这种机制也与人类译员的工作流程类似：不是先把长长的一个句子暗记于心再开始闭着眼睛翻译，而是草草扫一眼全文，然后一边查看原文一边翻译。这种“一边……一边……”其实类似于语料对齐的过程，即找出哪部分原文对应哪部分译文。而NMT中的attention是隐式地做对齐的。</p>
<h2 id="词语对齐"><a href="#词语对齐" class="headerlink" title="词语对齐"></a>词语对齐</h2><p>传统的SMT中需要显式地做双语对齐：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/28/19-48-43-545fe7a1135ae6635e5c97932dc318ed-006Fmjmcly1fgzwnas6czj31aq0v20xx-0bb186.jpeg"></p>
<h2 id="同时学习翻译和对齐"><a href="#同时学习翻译和对齐" class="headerlink" title="同时学习翻译和对齐"></a>同时学习翻译和对齐</h2><p>一个非常棒的可视化，显示attention model成功地对齐了法语和英语，其中一小段语序的调整也反应出来了：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/28/19-49-30-fd6937e2a41aa9779aaa07f6e71181a6-006Fmjmcly1fgzws04dx8j318y0u4tdr-26eabe.jpeg"></p>
<h2 id="打分"><a href="#打分" class="headerlink" title="打分"></a>打分</h2><p>在图示问号时刻，究竟应该关注哪些时刻的encoder状态呢？关注的强度是多少呢？</p>
<p>有一种打分机制，以前一刻的decoder状态和某个encoder状态为参数，输出得分：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/28/20-29-28-c03de7873eae432b89512839aa4cac6e-006Fmjmcly1fgzww54hclj30pm0mcmzi-50a36d.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/28/20-29-55-bdaa1e44bdacb14bde23244d8179744b-006Fmjmcly1fgzwwjf5qoj30oa0li40t-78cd2f.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/28/20-30-15-89330c765efc778bb9322d6a46fa7ee0-006Fmjmcly1fgzwwv8cxnj30oa0lw0v2-6a3638.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/28/20-30-31-0ebf4df1ed9fa23b3af91f0734650866-006Fmjmcly1fgzwx6217oj30o60mkjtq-f117ea.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/28/20-30-52-b8a23db75a144806b4ffb60f1fbe8710-006Fmjmcly1fh0n92pf50j315c0o8whi-6dbc50.jpeg"></p>
<p>这个概率也代表模型应该将多少比例的注意力放在一个历史状态上：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/28/20-31-28-a62bb822ef3468c0f8dd28981faee353-006Fmjmcly1fh0nbnn8a6j310y0n041b-b177e8.jpeg"></p>
<p>加权和得到一个context vector，作为条件之一生成decoder的当前状态：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/28/20-32-16-70c448a00fd99c3a280be0308475b843-006Fmjmcly1fh0ndqpe6sj30oi0mkmz9-7f6389.jpeg"></p>
<p>而分数的获得，是通过attention function进行的。attention function有多种选择，其中流行的是中间这种。$W_a$给了两个向量更复杂的interaction，而最后一种根本没有interaction。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/28/20-33-48-3015ec15934be4d31aaf0a43f3dfe41b-006Fmjmcly1fh0nj86k1fj30nx07m0uh-241042.jpeg"></p>
<p>有一些观点认为模型不应该注意所有的事情，可能对长句子来讲比较有潜力：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/28/20-34-22-e2d219493c73238b096f4d64d2bdf920-006Fmjmcly1fh0nn6bwbgj31c60mcn1d-f2ac66.jpeg"></p>
<p>但这些观点并没有取得更好的成绩：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/28/20-35-33-cdc5d7d1fe80ab39534d0c27a34e02e3-006Fmjmcly1fh0nr2tqe4j30of0dcdk5-4d6f0f.jpeg"></p>
<h2 id="更多attention！覆盖范围"><a href="#更多attention！覆盖范围" class="headerlink" title="更多attention！覆盖范围"></a>更多attention！覆盖范围</h2><p>在图片标题生成研究中，模型通过对图片不同区域的attention生成了不同的词语：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/28/20-38-39-71b5d4f09d0439a1f651f1a50b2adbbd-006Fmjmcly1fh0o257v1jj319q0a27fj-e2e400.jpeg"></p>
<h2 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/20-42-06-29d784290bf821a37eaec371a803e2ec-QQ%E6%88%AA%E5%9B%BE20210628204159-e5e071.png"></p>
<h2 id="Ancestral-sampling"><a href="#Ancestral-sampling" class="headerlink" title="Ancestral sampling"></a>Ancestral sampling</h2><p>在时刻$t$,根据之前的词语生成当前词语$x_t$:</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/20-44-46-d2c6fa9c0bda2c5da595a4d6c02f1db7-QQ%E6%88%AA%E5%9B%BE20210628204441-b6b803.png"></p>
<p>可以多次sample取最好的。</p>
<p>理论上完美无缺，但实践中只会产生高方差的差效果。你也不想同一个句子每次翻译结果都不一样。</p>
<h2 id="Greedy-Search"><a href="#Greedy-Search" class="headerlink" title="Greedy Search"></a>Greedy Search</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/28/20-45-23-9a81be0c5df7bb4ab8816bdb5b0b8a41-006Fmjmcly1fh19bsnpcuj309y0g2dh0-005a82.jpeg"></p>
<h2 id="Beam-search"><a href="#Beam-search" class="headerlink" title="Beam search"></a>Beam search</h2><p>每个时刻记录$k$个最可能的选项（剪枝），在其中进行搜索。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/20-47-01-7c531bb811b3ea5e6fa42531a57da495-QQ%E6%88%AA%E5%9B%BE20210628204656-802286.png"></p>
<p>然后递推 $H_{t+1}$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/20-50-02-e71e27b0412dc0b398478ef6e033b53e-QQ%E6%88%AA%E5%9B%BE20210628204957-318fe3.png"></p>
<p>其中</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/20-50-41-f2113f8926dcb9d72a5b890ffca765fa-QQ%E6%88%AA%E5%9B%BE20210628205020-0fe0b0.png"></p>
<p>也就是说把词表中的词丢进入计算概率取前$K$个。</p>
<h2 id="效果对比"><a href="#效果对比" class="headerlink" title="效果对比"></a>效果对比</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/28/20-52-02-df022b1a7b2183ba61bda3c4053fa2cf-006Fmjmcly1fh19j0rpu6j31960fognz-28c085.jpeg"></p>
<p>采样要采50轮才得到比贪心搜索稍好的结果，但很小的柱搜索轻松超越了它们。另外，基于短语的MT常用的柱搜索大小是100到150，可见NMT的优势。</p>
<h1 id="Encoder-Decoder框架"><a href="#Encoder-Decoder框架" class="headerlink" title="Encoder-Decoder框架"></a>Encoder-Decoder框架</h1><p>注意力机制是一种通用的思想，本身不依赖于特定框架，但是目前主要和Encoder-Decoder框架（编码器-解码器）结合使用。下图是二者相结合的结构：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1630478/201904/2021/06/28/21-03-23-5c13469575a712116661e953346a7add-1630478-20190415162604257-952024342-ba64cd.png"></p>
<p>类似的，Encoder-Decoder框架作为一种深度学习领域的常用框架模式，在文本处理、语言识别和图像处理等领域被广泛使用。其编码器和解码器并非是特定的某种神经网络模型，在不同的任务中会套用不同的模型，比如文本处理和语言识别中常用RNN模型，图形处理中一般采用CNN模型。</p>
<p>以下是没有引入注意力机制的RNN Encoder-Decoder框架：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1630478/201904/2021/06/28/21-04-11-e7e66753a093ec9af54b4ac80ed337f1-1630478-20190415162651216-1322445418-eb842e.png"></p>
<p>下面就以Seq2Seq(异步的序列到序列模型)模型为例，来对比未加入注意力机制的模型和加入了注意力机制后的模型。</p>
<h2 id="未加入注意力机制的RNN-Encoder-Decoder"><a href="#未加入注意力机制的RNN-Encoder-Decoder" class="headerlink" title="未加入注意力机制的RNN Encoder-Decoder"></a>未加入注意力机制的RNN Encoder-Decoder</h2><p>未加入注意力机制的RNN Encoder-Decoder框架在处理序列数据时，可以做到先用编码器把长度不固定的序列X编码成长度固定的向量表示C，再用解码器把这个向量表示解码为另一个长度不固定的序列y，输入序列X和输出序列y的长度可能是不同的。</p>
<p>《Learning phrase representations using RNN encoder-decoder for statistical machine translation》这篇论文提出了一种RNN Encoder-Decoder的结构，如下图。除外之外，这篇文章的牛逼之处在于首次提出了GRU(Gated Recurrent Unit)这个常用的LSTM变体结构。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1630478/201904/2021/06/28/21-05-34-18542d076db77434a6e9f5a1a6272698-1630478-20190415173815973-490353215-f3cce0.png"></p>
<p>把这种结构用在文本处理中，给定输入序列$X=[x_1,x_2,…,x_T]$,也就是由单词序列构成的句子，这样的一个解码-编码过程相当于是求另一个长度可变的序列$y=[y_1, y_2, …, y_{T′}]$的条件概率分布：$p(y)=p(y_1, y_2, …, y_{T′} | x_1,x_2,…,x_T)$。经过解码后，这个条件概率分布可以转化为下面的连乘形式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-07-53-802a4f442a3ef7ba4a6daa0ab118f2e1-QQ%E6%88%AA%E5%9B%BE20210628210744-0a3960.png"></p>
<p>所以在得到了表示向量c和之前预测的所有词 ${y_1,y_2,…, y_{t-1}}$后，这个模型是可以用来预测第$t$个词$yt$的，也就是求条件概率$p(y_t | {y_1,y_2,…, y_{t-1}}, c)$。</p>
<p>对照上面这个图，我们分三步来计算这个条件概率：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-09-14-002cc7d641c2e2c22a3d196c6cdb5808-QQ%E6%88%AA%E5%9B%BE20210628210908-b367ca.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-09-41-8901066726df25ad916273f8be41da1d-QQ%E6%88%AA%E5%9B%BE20210628210936-71a6be.png"></p>
<h2 id="加入注意力机制的RNN-Encoder-Decoder"><a href="#加入注意力机制的RNN-Encoder-Decoder" class="headerlink" title="加入注意力机制的RNN Encoder-Decoder"></a>加入注意力机制的RNN Encoder-Decoder</h2><p>《Neural Machine Translation by Jointly Learning to Align and Translate 》这篇论文在上面那篇论文的基础上，提出了一种新的神经网络翻译模型（NMT）结构，也就是在RNN Encoder-Decoder框架中加入了注意力机制。这篇论文中的编码器是一个双向GRU，解码器也是用RNN网络来生成句子。</p>
<p>用这个模型来做机器翻译，那么给定一个句子$X=[x_1,x_2,…,x_T]$，通过编码-解码操作后，生成另一种语言的目标句子$y=[y_1, y_2, …, y_{T′}]$，也就是要计算每个可能单词的条件概率，用于搜索最可能的单词，公式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-10-21-5521e737850276c820b3184501e62eb8-QQ%E6%88%AA%E5%9B%BE20210628211017-93c2f6.png"></p>
<p>生成第$t$个单词的过程图示如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img2018.cnblogs.com/blog/1630478/201904/2021/06/28/21-10-49-34ba0b6979c893277ad75d0729fdf0cf-1630478-20190415214603559-1644831845-c6a2f1.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-11-16-e336d408944f0f84b53e44ec6fc0fb05-QQ%E6%88%AA%E5%9B%BE20210628211109-a1d779.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-11-43-e386ad2bea3b265995737907d46d05e8-QQ%E6%88%AA%E5%9B%BE20210628211137-c8a815.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-12-01-4b2f293aa6e283da576931890fa7974a-QQ%E6%88%AA%E5%9B%BE20210628211156-0d2c85.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/28/21-12-20-8fc0ff077ac2fb4724d20cfad8927799-QQ%E6%88%AA%E5%9B%BE20210628211215-4aa25d.png"></p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>NMT</tag>
        <tag>MT</tag>
        <tag>机器翻译</tag>
        <tag>Attention</tag>
        <tag>注意力机制</tag>
      </tags>
  </entry>
  <entry>
    <title>LSTMs and GRUs</title>
    <url>/2021/06/23/LSTMs-and-GRUs/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/15-07-44-5473f62de3badd7522c101358cf53525-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625150625-4b0b5c.png"></p>
<a id="more"></a>

<h1 id="机器翻译"><a href="#机器翻译" class="headerlink" title="机器翻译"></a>机器翻译</h1><p>对于情感分析这类还算简单的任务，你可以整理一个情感极性词典、编写一堆规则做出一个勉强能用的系统。但到了机器翻译这个高级应用，就无法完全依靠规则了。现代机器翻译手段都是基于统计的，在平行语料上学习语言知识。</p>
<p>传统机器翻译系统非常复杂，因为不同阶段用到了不同的机器学习方法。</p>
<h1 id="传统统计机器翻译系统"><a href="#传统统计机器翻译系统" class="headerlink" title="传统统计机器翻译系统"></a>传统统计机器翻译系统</h1><p>定义一些符号：</p>
<p>原文$f$</p>
<p>译文$e$</p>
<p>机器翻译定义为找到使如下条件概率最大：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/15-11-58-62ae6b36dec102ec0ee54fb260fe1bb5-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625151149-a173a6.png"></p>
<p>这里利用了贝叶斯公式。翻译模型$p(f|e)$,在平行语料上训练得到，语言模型$p(e)$在未对齐的原文语料上训练（是非常廉价的）。</p>
<p>公式描述的翻译过程如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/25/15-13-31-c0db088629033dc4009a009af6a4f335-006Fmjmcly1fgux4v9ttnj31a40ceac0-1e3d10.jpeg"></p>
<h2 id="第一步：对齐"><a href="#第一步：对齐" class="headerlink" title="第一步：对齐"></a>第一步：对齐</h2><p>找到原文中的哪个句子或短语翻译到译文中的哪个句子或短语。</p>
<p>对齐时，原文中可能有部分词语没有对应的译文：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/25/15-20-08-947f4035e97cf913a03649e15e5a283b-006Fmjmcly1fgux9eqlddj30w60h0tbe-3d8c91.jpeg"></p>
<p>也可能在译文中有部分词语没有对应的原文，根据模型不同，可能有一对多的对齐方式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/25/15-20-39-83cf8117f0f534bb20b8c4efccbfb973-006Fmjmcly1fguxap5mpvj31cg0t879t-8cd0ba.jpeg"></p>
<p>也可能有多对一的对齐方式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/25/15-21-33-7bb23176ba87ed5d38604fb81ee4c104-006Fmjmcly1fguxe8dxcaj30lw0dx0v7-f9df3e.jpeg"></p>
<p>还可能有多对多的对齐方式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/25/15-21-56-24e84349021d66e1ff47b5266306f09e-006Fmjmcly1fguxezha4ej313u0tygpw-6eee5d.jpeg"></p>
<p>有时候还要通过句法分析，来进行不同颗粒度的对齐：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/25/15-22-21-8320f231e2eb1a6a8e0cbf6d13ec6851-006Fmjmcly1fguxhht1r8j319o0aign2-95f1fb.jpeg"></p>
<h2 id="对齐之后"><a href="#对齐之后" class="headerlink" title="对齐之后"></a>对齐之后</h2><p>原文中每个单词都有多个备选单词，导致了许多短语的组合方式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/25/15-23-03-200aad31e0f521c47be807379e4a06d9-006Fmjmcly1fguxkn5uz5j319s0nqq7l-5b082a.jpeg"></p>
<h2 id="解码：在海量假设中搜索最佳选择"><a href="#解码：在海量假设中搜索最佳选择" class="headerlink" title="解码：在海量假设中搜索最佳选择"></a>解码：在海量假设中搜索最佳选择</h2><p>这是一个特别复杂的搜索问题，涉及到许多语言模型。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/25/15-23-41-e1c0e155d65080df5c0f80a901318a33-006Fmjmcly1fguxn8iuxfj314q0noq5g-be8a32.jpeg"></p>
<h2 id="传统机器翻译"><a href="#传统机器翻译" class="headerlink" title="传统机器翻译"></a>传统机器翻译</h2><p>这还只是传统机器翻译系统的冰山一角，有许多细节没有涉及到，还需要大量的人肉特征工程，总之是非常复杂的系统。其中每个环节都是独立不同的机器学习问题。这些独立的模型各自为政，并不以一个统一的优化目标为最终目标。</p>
<p>而深度学习则提供了一个统一的模型，一个统一的最终目标函数。在优化目标函数的过程中，得到一个end to end的完整的joint模型。传统机器翻译系统与深度学习是截然相反的，对齐模型、词序模型、语言模型……一堆独立的模型无法联合训练。</p>
<h1 id="深度学习来救场"><a href="#深度学习来救场" class="headerlink" title="深度学习来救场"></a>深度学习来救场</h1><p>也许可以直接用RNN来接受原文，预测译文“下一个单词”：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/25/15-25-55-235058e0c64ebfc20b9315806ea15715-006Fmjmcly1fguy1l3e1wj31e20oy0xx-0712ed.jpeg"></p>
<p>红圈所示特征表示必须能捕捉整个原文短语的语义，但是RNN无法记住太久之前的事情，大概五六个单词就到极限了。所以这不是个实用的模型。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/15-31-56-ba74addf1b83c9456daae98478b387b6-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625153148-39e822.png"></p>
<p>softmax分类器中必须有个代表句子终止的“单词”，不然模型会无休止地输出下去。</p>
<p>但神经网络机器翻译模型没有这么简单，必须加一些拓展。</p>
<p>1、编码器和解码器训练不同的权值矩阵</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/25/15-33-31-49826bf18f3cb3e7485e1ee62b361e71-006Fmjmcly1fguyfoyrvfj31a00owdjs-1cf28e.jpeg"></p>
<p>红蓝代表不同的权值。</p>
<p>2、decoder中的隐藏层的输入来自3个方面：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/15-34-36-ebe88dcf45df691de8fa8d164ea64064-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625153428-16ff57.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/25/15-34-58-60b946d4572edd207dcfe15a9cf81953-006Fmjmcly1fguakyetk3j30gf0g6q48-33dbab.jpeg"></p>
<p>这可以辅助训练softmax的权值矩阵，防止模型重复生成同一个单词。</p>
<p>上图还有一个复杂版本，表达的是同一个意思：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/25/15-35-53-11d37ea9182de7ff3c22eb3e85a7e3db-006Fmjmcly1fguysycf7kj31c60wuth0-db6410.jpeg"></p>
<p>3、使用深度RNN</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/25/15-37-18-09b5fb3c415d3fc571e93510315fcb06-006Fmjmcly1fgu9l36s34j30kg0jjju1-7d17b6.jpeg"></p>
<p>4、使用 bi-directional encoder</p>
<p>5、不再用 A B C→X Y作为训练实例，而是逆转原文词序：C B A→X Y。因为A更可能翻译为X，而梯度消失导致A无法影响输出，倒过来A离输出近一些。逆转词序不会带来“语法语义上的改变”，因为模型学习的就是如何从逆序的原文翻译顺序的译文。但相应的，C就离Y更远了</p>
<h1 id="LSTM"><a href="#LSTM" class="headerlink" title="LSTM"></a>LSTM</h1><h2 id="RNN-存在的问题"><a href="#RNN-存在的问题" class="headerlink" title="RNN 存在的问题"></a>RNN 存在的问题</h2><p>误差项沿时间反向传播的公式：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/15-46-46-69774b6629219e29c615c8feb8afaf08-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625154641-cf2d75.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/15-47-10-a190dd9ea3666be9e4c566d768a05d0b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625154704-4a15b9.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/15-54-04-e9cf9447069d00565f5aedb5c7b09037-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625155359-74472d.png"></p>
<h2 id="LSTM结构示意图"><a href="#LSTM结构示意图" class="headerlink" title="LSTM结构示意图"></a>LSTM结构示意图</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/upload-images.jianshu.io/upload_images/2021/06/25/15-57-30-bbc55e04db51e1dcabc6855982c05212-2256672-7ea82e4f1ac6cd75-1ef7fc.png"></p>
<h2 id="LSTM前向计算公式"><a href="#LSTM前向计算公式" class="headerlink" title="LSTM前向计算公式"></a>LSTM前向计算公式</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-00-50-063956c2d489e0d2ae0cdef3489696cb-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625155808-817b14.png"></p>
<h2 id="误差项沿时间的反向传递（误差的横向计算）"><a href="#误差项沿时间的反向传递（误差的横向计算）" class="headerlink" title="误差项沿时间的反向传递（误差的横向计算）"></a>误差项沿时间的反向传递（误差的横向计算）</h2><p>计算 t-1 时刻的误差项$\delta^{t-1}$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-05-31-f5a6262b10ffde64de874da14fecf9dc-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625160526-71940b.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-07-31-38d2a2419647b0f810ebaebef2716996-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625160722-cb6582.png"></p>
<p>其中</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-08-34-88808808004e812073a015be226d6df3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625160829-9789da.png"></p>
<p>经过推导可以得到</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-10-00-a7308cc08d0c84b1d81123009269134b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625160955-3d820c.png"></p>
<h2 id="将误差项传递到上一层（误差的竖向计算）"><a href="#将误差项传递到上一层（误差的竖向计算）" class="headerlink" title="将误差项传递到上一层（误差的竖向计算）"></a>将误差项传递到上一层（误差的竖向计算）</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-11-45-7e8bf0b72ba3a7d4b864bcde71ed6cc9-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625161133-7d7b2a.png"></p>
<h2 id="权重梯度的计算"><a href="#权重梯度的计算" class="headerlink" title="权重梯度的计算"></a>权重梯度的计算</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-12-54-1a58f8033f824db2e2621beba78fcbbe-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625161250-319222.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-12-54-1a58f8033f824db2e2621beba78fcbbe-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625161250-319222.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-13-26-5652d7e94ac087102cca28cb9be8ba66-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625161321-6ef92d.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-14-02-ef3ff32d36ed8071f0580d1e12a29d7a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625161357-a96091.png"></p>
<h1 id="主要改进：更好的单元-GRU"><a href="#主要改进：更好的单元-GRU" class="headerlink" title="主要改进：更好的单元 GRU"></a>主要改进：更好的单元 GRU</h1><p>前面我们讲了一种普通的LSTM，事实上LSTM存在很多变体，许多论文中的LSTM都或多或少的不太一样。在众多的LSTM变体中，GRU (Gated Recurrent Unit)也许是最成功的一种。它对LSTM做了很多简化，同时却保持着和LSTM相同的效果。因此，GRU最近变得越来越流行。</p>
<h2 id="GRU的示意图"><a href="#GRU的示意图" class="headerlink" title="GRU的示意图"></a>GRU的示意图</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/upload-images.jianshu.io/upload_images/2021/06/25/16-15-21-620fd71cd786900fd8c474cc70d8a242-2256672-b784d887bf693253-5d131e.png"></p>
<h2 id="GRU的前向计算公式"><a href="#GRU的前向计算公式" class="headerlink" title="GRU的前向计算公式"></a>GRU的前向计算公式</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/25/16-15-53-32d97b26b151cc0dc1cff679db4ff4ea-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210625161547-c2ded2.png"></p>
<p>GRU的训练算法比LSTM简单一些，，自行推导</p>
<h1 id="RNN最新改进"><a href="#RNN最新改进" class="headerlink" title="RNN最新改进"></a>RNN最新改进</h1><h2 id="softmax的问题：无法出新词"><a href="#softmax的问题：无法出新词" class="headerlink" title="softmax的问题：无法出新词"></a>softmax的问题：无法出新词</h2><p>对分类问题来讲，你无法指望分类模型给你分出一个训练集中不存在的类。即便是训练集中存在的类，如果样本数很少，模型也很难预测出该类。</p>
<p>对于预测下一个单词的语言模型来讲，也是如此。比如某某女士巴拉巴拉，然后自我介绍说我叫某某。如果某某不存在于训练集中，则模型无法预测出某某。</p>
<p>虽然可以用字符级的模型，但代价实在太大。</p>
<h2 id="用指针来解决问题"><a href="#用指针来解决问题" class="headerlink" title="用指针来解决问题"></a>用指针来解决问题</h2><p>如果把某某替换为“向前数第10个单词”这样的指针，问题就迎刃而解了。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/25/16-23-40-0b6fa2b62acba61a0153e1ebcac063c1-006Fmjmcly1fgva657a97j30z40h4ju0-d5d426.jpeg"></p>
<p>具体做法是，以前100个时刻的隐藏层作为输入，用一个softmax去计算前100个单词是pointer的概率，与原来的词表上的分布混合。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/25/16-25-51-af53b852b6f56cb25fd02ea5aed6e040-006Fmjmcly1fgvd7psrsoj31bk0xegyr-4d4a80.jpeg"></p>
<p>使用了pointer之后，困惑度下降了零点几个百分点：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/25/16-29-23-918da019d6f2bdc100121632a4856ec4-006Fmjmcly1fgvd9ulzzlj31aa0qithe-9bdb56.jpeg"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>RNN很强大</p>
<p>有很多进行中的工作</p>
<p>GRU更强大</p>
<p>LSTM又更强大</p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>LSTM</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title>Recurrent neural network</title>
    <url>/2021/06/22/Recurrent-neural-network/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-31-57-7db7a024b0f1e7d31b44b25c5b17ddff-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622103138-81442d.png"></p>
<a id="more"></a>

<h1 id="语言模型"><a href="#语言模型" class="headerlink" title="语言模型"></a>语言模型</h1><p>语言模型就是计算一个单词序列（句子）的概率（P(w1,…,wm)）的模型。听上去很简单，做起来很难；听上去没什么用处，但用处非常多。比如在机器翻译中，判断译文序列中一种词序的自然程度高于另一种，判断一种用词选择优于另一种。</p>
<h2 id="传统语言模型"><a href="#传统语言模型" class="headerlink" title="传统语言模型"></a>传统语言模型</h2><p>为了简化问题，必须引入马尔科夫假设，句子的概率通常是通过待预测单词之前长度为n的窗口建立条件概率来预测：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-37-53-0edc788abde54521f50d89e0edcd26bb-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622103747-80603c.png"></p>
<p>为了估计此条件概率，常用极大似然估计，比如对于BiGram和TriGram模型，有：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-38-35-e20b6b86310a9d6444608aa872e9771d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622103821-f2c2ba.png"></p>
<p>在数据量足够的情况下，n-gram中的n越大，模型效果越好。但实际上，数据量总是不如人意，这时候一些平滑方法就不可或缺。另外，这些ngram可能会占用上G的内存，在最新的研究中，一个1260亿的语料在140G内存的单机上花了2.8天才得到结果。</p>
<p>Bengio et al提出了第一个大规模深度学习自然语言处理模型，只不过是用前n个单词的词向量来做同样的事情（上文建模）而已，其网络结构如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/22/10-39-27-7d47b1fa585c7b6e8b8761c4b1f6eb3d-006Fmjmcly1fgttmr9ufjj30kb0i5wgm-be948a.jpeg"></p>
<p>公式如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-40-24-e761a48e611c6d198d056fdeb58c7e09-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104016-dde895.png"></p>
<h1 id="RNN"><a href="#RNN" class="headerlink" title="RNN"></a>RNN</h1><h2 id="RNN的说明及推导"><a href="#RNN的说明及推导" class="headerlink" title="RNN的说明及推导"></a>RNN的说明及推导</h2><p>完全抄录的该篇博客，并对感觉不对的地方进行修正，对难以理解的地方加以补充：<a href="https://zybuluo.com/hanbingtao/note/541458" title="零基础入门深度学习(5) - 循环神经网络">https://zybuluo.com/hanbingtao/note/541458</a></p>
<p>在全连接神经网络和卷积神经网络中，只能单独的去处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理序列的信息，即前面的输入和后面的输入是有关系的。比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。这时，就需要用到深度学习领域中另一类非常重要神经网络：循环神经网络(Recurrent Neural Network)。RNN种类很多，也比较绕脑子。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-43-40-c3185fd2f8458a3653dc035a68575b84-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104334-8ce28b.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-44-19-49c94200244be9c56e4a544771ae742a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104409-90c38e.png"></p>
<p>上图修正：<br>N-Gram的含义是，假设一个词出现的概率只与前面N - 1个词相关。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-44-57-12567096c3fbc92a7a2199ef22743850-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104451-cdb930.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-45-21-f74b4c5c32bec7318e45e31c99f4dc06-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104516-aadcec.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-45-47-6ee325d3906f10f2793fbb01e489930a-2256672-cf18bb1f06e750a4-b9834d.jpg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-46-34-f4855fe9a5ef4908bc4c345bb57778af-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104625-87f87e.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-47-44-89563cd6e42094746868f1972f5e932f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104738-1439bc.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-48-27-9b9d18524446bfc97892d5a6196c216a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104816-cf69ae.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-48-52-0c8d438c12bce0260b16cf270db56f0b-2256672-039a45251aa5d220-3619b7.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-49-22-e0e9240b802a20ad50579a31d6bc0c45-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104916-2e7e6f.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-49-55-28f7677c012b3092338993ab032e94e6-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622104950-0c42ab.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-50-22-672001428ca5343238451a8c8aa0ab09-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105012-c4db5d.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-50-33-09607ca6bd45d27c704efcfe84724d9f-2256672-df137de8007c3d26-7b3f7c.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-50-56-e835a43ba65aeb5506c11d83769ed08b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105050-8a7cf7.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-51-21-ea1f8b4f2eb4c42a270e4f0703cbb4f0-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105115-fdc5d3.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-51-51-fd64624cac95d8a43720e5868e29e27c-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105146-299f71.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-54-12-144567938bb01cd7dc6b812c3df30a1a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105408-7f447b.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-54-56-eeae052d477aa5489373c2f703459569-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105449-a86df1.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-55-29-4644407605b512eafa82c4f8353f05dd-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105524-07af36.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-56-04-e867611d117d1ee0767696d53709bc79-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105559-6d2b03.png"></p>
<p>对上述的推导过程进行解释</p>
<p>$$\begin{bmatrix}<br>{s_1^{t-1}}\\<br>{s_2^{t-1}}\\<br>{\vdots}\\<br>{s_n^{t-1}}\\<br>\end{bmatrix} = f(<br>\begin{bmatrix}<br>{u_{11}}&amp;{u_{12}}&amp;{\cdots}&amp;{u_{1m}}\\<br>{u_{21}}&amp;{u_{22}}&amp;{\cdots}&amp;{u_{2m}}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{u_{n1}}&amp;{u_{n2}}&amp;{\cdots}&amp;{u_{nm}}\\<br>\end{bmatrix}\begin{bmatrix}<br>{x_1}\\<br>{x_2}\\<br>{\vdots}\\<br>{x_m}\\<br>\end{bmatrix} + \begin{bmatrix}<br>{w_{11}}&amp;{w_{12}}&amp;{\cdots}&amp;{w_{1n}}\\<br>{w_{21}}&amp;{w_{22}}&amp;{\cdots}&amp;{w_{2n}}\\<br>{\vdots}&amp;{\vdots}&amp;{\ddots}&amp;{\vdots}\\<br>{w_{n1}}&amp;{w_{n2}}&amp;{\cdots}&amp;{w_{nn}}\\<br>\end{bmatrix}\begin{bmatrix}<br>{s_1^{t-2}}\\<br>{s_2^{t-3}}\\<br>{\vdots}\\<br>{s_n^{t-2}}\\<br>\end{bmatrix})$$</p>
<p>$$=f(\begin{bmatrix}<br>{u_1}\\<br>{u_2}\\<br>{\vdots}\\<br>{u_n}\end{bmatrix}x+ \begin{bmatrix}<br>{w_1}\\<br>{w_2}\\<br>{\vdots}\\<br>{w_n}\end{bmatrix}s^{t-2})$$</p>
<p>$$=f(\begin{bmatrix}<br>{u_1x + w_1s^{t-2}}\\<br>{u_2x + w_2s^{t-2}}\\<br>{\vdots}\\<br>{u_nx + w_ns^{t-2}}\end{bmatrix})$$<br>在这里面 $u_i 和 w_i 是待优化的参数，x 和 s^{t-2} 都是实数组成的矩阵，$</p>
<p>$所以参数是 u_i 和 w_i,u_i组成U是输入层到隐藏层的权重矩阵，w_i组成W是上一次的s^{t-2}值作为这一次的输入的权重矩阵$</p>
<p>$\frac{\partial{s_2^{t-1}}}{\partial{net_1^{t-1}}} = \frac{\partial{f(u_2x + w_2s^{t-2})}}{\partial{u_1x + w_1s^{t-2}}}$</p>
<p>由于上式中的变量为 $u_2 和 w_2$,而下式的变量为 $u1 和 w_1$,所以求导为 0</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-56-39-8ad0521194e00210fd1d738f106a223a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105630-b64241.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-57-06-3e295e0b908a1b968684789e56e39c29-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105659-052f58.png"></p>
<p>其中 $(δ_t^l)^T$ 表示 $\frac{\partial{E}}{\partial{net_t}}$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-57-36-068747cfec9e3e68ed0a346cd989ce65-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105731-d9b553.png"></p>
<p>这里的 $a_t^{l-1}$ 表示得是第 l - 1 层神经元的输出，式（22） ，式（39） 求得偏导实际上是对深度神经网络的图进行求的偏导（即下图），式 （22） 是横向求得，式（39） 是竖向求得，本片博客的作者在标记神经元的输入时，除了最底层使用 $x$ 进行标记，其他的输入层都是用的 $a$ 进行标记的,可以查看后向传播算法</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-50-33-09607ca6bd45d27c704efcfe84724d9f-2256672-df137de8007c3d26-7b3f7c.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-58-13-07b04ecd42d322c53ceb7f90d4039fa3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105806-6dce0a.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-58-52-cf48cf2343929521b6ab4259f73596a2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105848-86d2f6.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-59-24-ba6e7714af2626728f6bb0e1a9d602f9-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105919-62f085.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/10-59-49-cfd002b93e38c0cd21626ab366239e8e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622105945-03383d.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-14-47-34dc2a40c03263b1387470f8622e4707-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622211438-3d088b.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-15-46-18b25fb5a1048e88ac3483ea18bc1f93-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622211540-e077d4.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-16-32-e5505655f820450ea448aa8d0bad44cf-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622211627-1bc4c7.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-17-13-5bb5309c0f617433a4a7a6fdf2f5f9f1-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622211707-bf53b9.png"></p>
<p>感觉这里的推导过程有错误，具体的提到过程应该如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-37-07-104e2eb5d191ed1c6799cdd0a1816c5a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622213630-8aed28.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-37-28-8aadfc80eac8528bc9f4f60cd553a6c2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622213642-651341.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-37-43-20cef9f6d5b914833185a6e35c311452-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622213658-156359.png"></p>
<p>按照乘法法则乘进去和最后的结果一样</p>
<p><strong>注意</strong></p>
<p><strong>第二张图、第三张图都是实数与矩阵相乘再相加</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/22/21-17-51-ee1d88b423d0902a1548d3b20d6a6bca-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210622211746-51a132.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-44-42-11b860d97481f63f36ad84c7b52f6745-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074429-222cc8.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-45-27-34c217fa27e33fbbaf9b7710ce0f226e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074520-1d574e.png"></p>
<h2 id="梯度爆炸和梯度消失"><a href="#梯度爆炸和梯度消失" class="headerlink" title="梯度爆炸和梯度消失"></a>梯度爆炸和梯度消失</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-53-46-73ebbc68083804710a4978792edcfb89-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623075338-234acb.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-54-13-daba273fac5ff8024a81281c38f62194-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623075407-0e839d.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-54-36-c1ed66d98b47a27aa95820de559d8c6b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623075431-7a8e01.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-55-03-b49896e590e6d68a087c727aa458f5b9-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623075458-155405.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-45-52-0a01da7aa6aa405eedb908ce5972b791-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074547-168d81.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-46-14-cb9007be3116e2642b89d49e2a870d7d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074609-62d733.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-46-45-f9b736fe1b4d8bf3b9932faabf88fac3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074639-106617.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-47-16-b83c92d22d15270bf04b78e6d6025452-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074711-8bfc3b.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-47-52-1e9979a82884058966600c231fda74f4-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074748-601ee9.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-48-32-2492883dec122ee45445c9ce8ef5718f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074825-d8a5c5.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-49-18-b39a088514966fe9a3f42ddb0217a6da-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074914-644069.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/07-49-53-9b597589ff41f408bc46aeb905576d3a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623074947-47fdf3.png"></p>
<h2 id="防止梯度爆炸"><a href="#防止梯度爆炸" class="headerlink" title="防止梯度爆炸"></a>防止梯度爆炸</h2><p>一种暴力的方法是，当梯度的长度大于某个阈值的时候，将其缩放到某个阈值。虽然在数学上非常丑陋，但实践效果挺好。</p>
<p>其直观解释是，在一个只有一个隐藏节点的网络中，损失函数和权值w偏置b构成error surface，其中有一堵墙：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/23/08-18-01-ed3e949b7f1c9b18588d5960bd957ab1-006Fmjmcly1fgu80lzfw9j30pr0g70yp-89c5fa.jpeg"></p>
<p>每次迭代梯度本来是正常的，一次一小步，但遇到这堵墙之后突然梯度爆炸到非常大，可能指向一个莫名其妙的地方（实线长箭头）。但缩放之后，能够把这种误导控制在可接受的范围内（虚线短箭头）。</p>
<p>但这种trick无法推广到梯度消失，因为你不想设置一个最低值硬性规定之前的单词都相同重要地影响当前单词。</p>
<h2 id="减缓梯度消失"><a href="#减缓梯度消失" class="headerlink" title="减缓梯度消失"></a>减缓梯度消失</h2><p>与其随机初始化参数矩阵，不如初始化为单位矩阵。这样初始效果就是上下文向量和词向量的平均。然后用ReLU激活函数。这样可以在step多了之后，依然使得模型可训练。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/23/08-18-38-9e3d812ea27fa3d0abbb709bc5ac0f99-006Fmjmcly1fgu8faifyuj30my0jctei-bddcfd.jpeg"></p>
<h2 id="困惑度结果"><a href="#困惑度结果" class="headerlink" title="困惑度结果"></a>困惑度结果</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/23/08-19-40-ec416c5bd9ae7f36a2fc0445d0181d5a-006Fmjmcly1fgu8jbejq7j30y00dialo-4bfb39.jpeg"></p>
<p>相较于NGram，RNN的困惑度要小一些。</p>
<h2 id="最后的实现技巧"><a href="#最后的实现技巧" class="headerlink" title="最后的实现技巧"></a>最后的实现技巧</h2><p>记录每个t的误差不要丢，反向传播的时候将其累加起来。</p>
<h1 id="序列模型的应用"><a href="#序列模型的应用" class="headerlink" title="序列模型的应用"></a>序列模型的应用</h1><p>可以把每个词分类到NER、实体级别的情感分析（饭菜味道不错，但环境不太卫生）、意见表达。</p>
<p>其中，意见挖掘任务就是将每个词语归类为：</p>
<p>DSE：直接主观描述（明确表达观点等）</p>
<p>ESE：间接主观描述（间接地表达情感等）</p>
<p>语料标注采用经典的BIO标注：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/23/08-24-39-1fdb6a3e71e51bd6f3049b60c0a712d0-006Fmjmcly1fgu8zv8hgfj31a20i8q8o-2b18f0.jpeg"></p>
<p>实现这个任务的朴素网络结构就是一个裸的RNN：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/23/08-24-56-a8e42bf80b38660d1c49400cada32069-006Fmjmcly1fgu95qcl7tj30ww0d0t9r-179be2.jpeg"></p>
<p>但是这个网络无法利用当前词语的下文辅助分类决策，解决方法是使用一些更复杂的RNN变种。</p>
<h2 id="Bidirectional-RNNs"><a href="#Bidirectional-RNNs" class="headerlink" title="Bidirectional RNNs"></a>Bidirectional RNNs</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/23/08-26-19-36b8ac5021b6cdf31c8a21d6a82831cd-006Fmjmcly1fgu9838i5wj30nc0ef75m-c7ce78.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/08-26-01-7d008a1a5a2641cd0af3f1cf1f2d54bb-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623082556-530ee4.png"></p>
<h2 id="Deep-Bidirectional-RNNs"><a href="#Deep-Bidirectional-RNNs" class="headerlink" title="Deep Bidirectional RNNs"></a>Deep Bidirectional RNNs</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/23/08-27-21-09b5fb3c415d3fc571e93510315fcb06-006Fmjmcly1fgu9l36s34j30kg0jjju1-84935c.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/08-28-15-7d008a1a5a2641cd0af3f1cf1f2d54bb-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623082556-31f180.png"></p>
<h2 id="评测"><a href="#评测" class="headerlink" title="评测"></a>评测</h2><p>评测方法是标准的F1（因为标签样本不均衡），在不同规模的语料上试验不同层数的影响：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/23/08-28-47-21b05ec8e9b8b9f4274aa1227c4ce5e7-006Fmjmcly1fgu9skeeejj31a80nqtba-886ae4.jpeg"></p>
<p>可见层数不是越多越好。</p>
<h2 id="应用：RNN机器翻译模型"><a href="#应用：RNN机器翻译模型" class="headerlink" title="应用：RNN机器翻译模型"></a>应用：RNN机器翻译模型</h2><p>传统机器翻译模型在不同的阶段用到大量不同的机器学习算法，这里讨论用RNN统一整个流水线。</p>
<p>比如将3个单词的德语翻译为2个单词的英语，用到如下RNN：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/23/08-29-36-ad5a8fe0f8ca4789ba2c57560aa12998-006Fmjmcly1fguacg65ahj30z20guacc-a36331.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/08-31-22-63fb9a5ab207f587eaddae3e1fa6186a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623083117-1188cd.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/08-32-28-532b231d71754ca88392189389f21bbe-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623083224-2c7031.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/23/08-32-48-60b946d4572edd207dcfe15a9cf81953-006Fmjmcly1fguakyetk3j30gf0g6q48-537786.jpeg"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/23/08-33-17-a39bde91efe1c49e3d1e10b88230fc6d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210623083309-e3f6c3.png"></p>
<h1 id="回顾"><a href="#回顾" class="headerlink" title="回顾"></a>回顾</h1><pre><code>RNN是最好的DeepNLP模型之一
因为梯度消失和梯度爆炸，训练很难
可以用很多技巧来训练
下次课将介绍更强大的RNN拓展：LSTM和GRU</code></pre>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>循环神经网络</tag>
        <tag>CNN</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title>Linguistic Structure: Dependency Parsing</title>
    <url>/2021/06/20/Linguistic-Structure-Dependency-Parsing/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/08-07-44-6e3918ac3ccd05224052041a4d14b010-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620080712-11c8b4.png"></p>
<a id="more"></a>

<h1 id="语言学的两种观点"><a href="#语言学的两种观点" class="headerlink" title="语言学的两种观点"></a>语言学的两种观点</h1><p>依存句法树和短语结构树详细解释：<a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/12.%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90.md" title="依存句法树和短语结构树详细解释">https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/12.%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90.md</a></p>
<p>如何描述语法，有两种主流观点，其中一种是上下文无关文法，英文术语是：Constituency = phrase structure grammar = context-free grammars (CFGs)。</p>
<h2 id="短语结构树"><a href="#短语结构树" class="headerlink" title="短语结构树"></a>短语结构树</h2><p>这种短语语法用固定数量的rule分解句子为短语和单词、分解短语为更短的短语或单词，一个取自WSJ语料库的短语结构树示例：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/08-10-15-de55b626a442e75026f86a38f641d775-6cbb8645gw1fb9et7tnw3j211i0k2n0e-0564cf.jpg"></p>
<h2 id="上下文无关文法"><a href="#上下文无关文法" class="headerlink" title="上下文无关文法"></a>上下文无关文法</h2><p>语言学中，上下文无关文法由如下组件构成:</p>
<pre><code>终结符结合 Σ，比如汉语的一个词表
非终结符集合 V，比如“名词短语”“动词短语”等短语结构组成的集合。
V 中至少包含一个特殊的非终结符，即句子符或初始符，计作 S
推到规则 R，即推到非终结符的一系列规则: V -&gt; V U Σ</code></pre>
<p>基于上下文无关文法理论，我们可以从 S 出发，逐步推导非终结符。一个非终结符至少产生一个下级符号，如此一层一层地递推下去，我们就得到了一棵语法树。但在NLP中，我们称其为短语结构树。也就是说，计算机科学中的术语“上下文无关文法”在语言学中被称作“短语结构语法”。</p>
<h2 id="依存结构"><a href="#依存结构" class="headerlink" title="依存结构"></a>依存结构</h2><p>另一种是依存结构，用单词之间的依存关系来表达语法。如果一个单词修饰另一个单词，则称该单词依赖于另一个单词。一个由HanLP输出的依存句法树如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/08-22-13-c4b7882ad0f2dfe85d4fb1d1345058bb-6cbb8645gw1exy8efomtkj20t505f3zj-d93fb8.jpg"></p>
<h2 id="依存句法树"><a href="#依存句法树" class="headerlink" title="依存句法树"></a>依存句法树</h2><p>将一个句子中所有词语的依存关系以有向边的形式表示出来，就会得到一棵树，称为依存句法树( dependency parse tree)。比如句子“弱小的我也有大梦想”的依存句法树如图所示。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/09-39-16-31b4b99010881eed95de380289d8e1c1-2020-2-14_17-58-48-81ccea.png"></p>
<p>现代依存语法中，语言学家 Robinson 对依存句法树提了 4 个约束性的公理。</p>
<pre><code>有且只有一个词语(ROOT，虚拟根节点，简称虚根)不依存于其他词语。
除此之外所有单词必须依存于其他单词。
每个单词不能依存于多个单词。
如果单词 A 依存于 B，那么位置处于 A 和 B 之间的单词 C 只能依存于 A、B 或 AB 之间的单词。</code></pre>
<p>这 4 条公理分别约束了依存句法树(图的特例)的根节点唯一性、 连通、无环和投射性( projective )。这些约束对语料库的标注以及依存句法分析器的设计奠定了基础。</p>
<h1 id="歧义"><a href="#歧义" class="headerlink" title="歧义"></a>歧义</h1><p>通过句法树可以表达歧义，一个确定的句法树对应句子的一个确定解读，比如对介词短语依附（attachment of prepositional phrases (PPs)）：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/20/08-23-06-586c391ea64d9481a8491a50e420288f-006Fmjmcly1fgij616l29j311i0ps0vn-db7a81.jpeg"></p>
<p>from space这个介词短语到底依附谁？不同的答案导致对句子不同的理解。</p>
<h2 id="依附歧义"><a href="#依附歧义" class="headerlink" title="依附歧义"></a>依附歧义</h2><p>很难确定如何把一个短语（介词短语、状语短语、分词短语、不定式）依附到其他成分上去，比如下列句子：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/20/08-28-35-79c7003af3f1c7d1cf24fcaf67eadf73-006Fmjmcly1fgijfj3n2vj31ds0fwter-af195c.jpeg" alt="董事会在月度会议上批准了多伦多的皇家信托公司以每股27美元的价格收购该公司。"></p>
<p>每个括号中都是一个短语，它们依附的对象各不相同。对于n个短语来讲，组成的树形结构有$C_n=\frac{(2n)!}{(n+1)!n!}$。这是Catalan数，指数级增长，常用于树形结构的计数问题。</p>
<h2 id="卡特兰数"><a href="#卡特兰数" class="headerlink" title="卡特兰数"></a>卡特兰数</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/08-39-58-a5badd4b4dc08755ffe6a94e7df564ac-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620083951-c8e370.png"></p>
<h1 id="标注数据集的崛起：Universal-Dependencies-treebanks"><a href="#标注数据集的崛起：Universal-Dependencies-treebanks" class="headerlink" title="标注数据集的崛起：Universal Dependencies treebanks"></a>标注数据集的崛起：Universal Dependencies treebanks</h1><p>虽然上下文无关文法中的语法集很容易写，无非是有限数量的规则而已，但人工费时费力标注的树库却茁壮成长了起来。在 1993 年首次面世的 Universal Dependencies treebanks 如今在 Google 的赞助下发布了 2.0，其授权大多是署名-相同方式共享，覆盖了全世界绝大多数语言（不包括简体中文）。</p>
<p>其官网是：<a href="http://universaldependencies.org/" title="Universal Dependencies 官网">http://universaldependencies.org/</a></p>
<p>GitHub主页是：<a href="https://github.com/UniversalDependencies" title="Universal Dependencies github 地址">https://github.com/UniversalDependencies</a></p>
<p>树库示例：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/20/08-50-15-5a4473df5335b63bba17a07f65708d65-006Fmjmcly1fgikc4szn1j30tm0howhf-764e95.jpeg"></p>
<p>人们偏好树库多于规则的原因是显而易见的，树库虽然标注难度高，但每一份劳动都可被复用（可以用于词性标注命名实体识别等等任务）；而每个人编写的规则都不同，并且死板又丑陋。树库的多用性还是得其作为评测的标杆数据，得到了越来越多的引用。</p>
<h2 id="依存文法与依存结构"><a href="#依存文法与依存结构" class="headerlink" title="依存文法与依存结构"></a>依存文法与依存结构</h2><p>这节课以及练习用的都是依存句法树，而不是短语结构树。这并不是随机选择，而是由于前者的优势。90年代的句法分析论文99%都是短语结构树，但后来人们发现依存句法树标注简单，parser准确率高，所以后来（特别是最近十年）基本上就是依存句法树的天下了（至少80%）。</p>
<p>不标注依存弧label的依存句法树就是短语结构树的一种：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/20/08-55-06-bc8e46a51f9778586f8ed87e5f6d1408-006Fmjmcly1fgikxq72uqj30xy0kmacl-8229fa.jpeg"></p>
<p>一旦标上了，两者就彻底不同了：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/20/08-55-26-1d8c657640d69fe3d3d815a083cab981-006Fmjmcly1fgikyihe3lj30yi0kuwhw-601696.jpeg"></p>
<p>这里箭头的尾部是head（被修饰的主题），箭头指向的是dependent（修饰语）。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>每个句子都有一个虚根，代表句子之外的开始，这样句子中的每个单词都有自己的依存对象了。</p>
<h1 id="句法分析可用的特征"><a href="#句法分析可用的特征" class="headerlink" title="句法分析可用的特征"></a>句法分析可用的特征</h1><pre><code>双词汇亲和（Bilexical affinities），比如discussion与issues。

词语间距，因为一般相邻的词语才具有依存关系

中间词语，如果中间词语是动词或标点，则两边的词语不太可能有依存

词语配价，一个词语最多有几个依赖者。</code></pre>
<h1 id="依存句法分析"><a href="#依存句法分析" class="headerlink" title="依存句法分析"></a>依存句法分析</h1><p>有几个约束条件：</p>
<pre><code>ROOT只能被一个词依赖
无环</code></pre>
<p>英语中大部分句子是 projective 的，少数是 non-projective 的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/20/10-06-07-80d7ec96ee803de0501963531ede4052-006Fmjmcly1fgm6p51a0oj31eg070abc-9724e5.jpeg"></p>
<h2 id="依存句法分析方法"><a href="#依存句法分析方法" class="headerlink" title="依存句法分析方法"></a>依存句法分析方法</h2><pre><code>Dynamic programming
    估计是找出以某head结尾的字串对应的最可能的句法树。
Graph algorithms
    最小生成树。
Constraint Satisfaction
    估计是在某个图上逐步删除不符合要求的边，直到成为一棵树。
“Transition-based parsing” or “deterministic dependency parsing”
    主流方法，基于贪心决策动作拼装句法树。</code></pre>
<h2 id="Arc-standard-transition"><a href="#Arc-standard-transition" class="headerlink" title="Arc-standard transition"></a>Arc-standard transition</h2><p>arc-standard方法，通过SHIFT、LEFT-ARC和RIGHT-ARC三种动作来决定单词之间是否依赖以及依赖关系。每个单词只能被修饰（指向）一次，但是可以多次修饰（指向）其他单词。</p>
<pre><code>SHIFT 指两个词 A 与 B 之间没有依存关系；
LEFT-ARC 指两个词 A 与 B 之间是 A &lt;- B 的左链接关系；
RIGHT-ARC 指两个词 A 与 B 之间是 A -&gt; B 的右链接关系。</code></pre>
<p>动作体系的 formal 描述如下：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/10-22-52-6cbe50db9fa5c7cf910c0e9aeb1c318f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620102245-099759.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/20-08-51-336b71240ee6b7c69720a5f9b5e94651-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620200839-ea3493.png"></p>
<p>图片原地址：<a href="https://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html" title="基于神经网络的高性能依存句法分析器">https://www.hankcs.com/nlp/parsing/neural-network-based-dependency-parser.html</a></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/10-24-25-2678aa38f038981db0bf56a15c67f431-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620102357-d2c157.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/10-26-14-88f9311ad23beab59f60dc82e7855fba-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620102605-e8d752.png"></p>
<p>截图来自：<a href="https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/12.%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90.md" title="依存句法树和短语结构树详细解释">https://github.com/NLP-LOVE/Introduction-NLP/blob/master/chapter/12.%E4%BE%9D%E5%AD%98%E5%8F%A5%E6%B3%95%E5%88%86%E6%9E%90.md</a></p>
<h2 id="MaltParser模型-没看懂"><a href="#MaltParser模型-没看懂" class="headerlink" title="MaltParser模型(没看懂)"></a>MaltParser模型(没看懂)</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/20-16-34-83d5e77c541057c8cda80956036adf9a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620201527-8f5eef.png"></p>
<h2 id="传统特征表示-没看懂"><a href="#传统特征表示-没看懂" class="headerlink" title="传统特征表示(没看懂)"></a>传统特征表示(没看懂)</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/https/img-blog.csdnimg.cn/2021/06/20/20-19-35-414accd8219e4dc41e19aa981eccf94f-20200706205434481-403b2e.png"></p>
<p>传统的特征表示使用二元的稀疏向量，一个超长的稀疏01向量。</p>
<pre><code>特征模板：通常由配置中的1 ~ 3个元素组成
Indicator features</code></pre>
<h2 id="依赖解析的评估：（标记）依赖精度-准确率评价"><a href="#依赖解析的评估：（标记）依赖精度-准确率评价" class="headerlink" title="依赖解析的评估：（标记）依赖精度/准确率评价"></a>依赖解析的评估：（标记）依赖精度/准确率评价</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/20-32-36-577f70203e36e31cb06613c20eae9648-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620203226-c8a4f8.png"></p>
<p>继续化简得 $LSA = \frac{2 * |A∩B|}{|A| + |B|}$</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/20-48-56-bb0e952947b07ef6413e1cd66ee54126-2020070620561090-073591.png"></p>
<p>求 LAS 时，对于该图 |A∩B| = 2，比对的时两幅图得 1，2 列相同的行</p>
<p>求 UAS 时，对于该图 |A∩B| = 4，比对的时两幅图得 1 列相同的行</p>
<p>|A| = |B| = 5</p>
<h2 id="其他评价指标"><a href="#其他评价指标" class="headerlink" title="其他评价指标"></a>其他评价指标</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/20-51-19-23d7e95d1f92ce79d38d9d62a418a928-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620205110-3b2511.png"></p>
<h2 id="投影"><a href="#投影" class="headerlink" title="投影"></a>投影</h2><p>所谓投射性是指：如果词p依存于词q，那么p和q之间的任意词r就不能依存到p和q所构成的跨度之外（用白话说，就是任意构成依存的两个单词构成一个笼子，把它们之间的所有单词囚禁在这个笼子里，只可内部交配，不可与外族通婚）。比如：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/ww2.sinaimg.cn/large/2021/06/20/20-06-01-5caaa193cae9b2ab12da13ac20f80a37-6cbb8645gw1exvojk1givj20h804sdg9-c04dda.jpeg"></p>
<p>再比如：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/ww3.sinaimg.cn/large/2021/06/20/20-06-20-e58b4d476bfc2923afdf4b81c87b170d-6cbb8645jw1emokwlqh42j20aw04vglr-13e739.jpeg"></p>
<h2 id="非投射"><a href="#非投射" class="headerlink" title="非投射"></a>非投射</h2><p>非投射就没有上述限制了，这会导致依存边有交叉，怎么都理不顺：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/ww4.sinaimg.cn/large/2021/06/20/20-07-00-7f99265c1ddaa1857a486ca1c2e7a4eb-6cbb8645gw1exvop84e10j20iw04ywf0-beaf48.jpeg"></p>
<h2 id="投射性"><a href="#投射性" class="headerlink" title="投射性"></a>投射性</h2><p>CFG转换得到的依存树一定是投射性的，但依存理论允许非投射性的依存句法树（一些语义需要通过非投射性表达）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/20/20-22-34-d53750cd99e1023ef1f849edd57bd461-006Fmjmcly1fgncuqs168j31co0cojud-8ea68f.jpeg"></p>
<p>arc-standard算法只能拼装投射性的句法树，但换个体系、加上后处理、采用graph-based方法就能得到非投射的句法树。</p>
<h1 id="为什么需要神经网络句法分析器"><a href="#为什么需要神经网络句法分析器" class="headerlink" title="为什么需要神经网络句法分析器"></a>为什么需要神经网络句法分析器</h1><p><a href="https://blog.csdn.net/yu5064/article/details/82186738" title="基于神经网络的依存句法分析总结及代码详解">https://blog.csdn.net/yu5064/article/details/82186738</a></p>
<p><a href="http://fancyerii.github.io/books/nndepparser/#dp-nn-2" title="基于神经网络的依存句法分析">http://fancyerii.github.io/books/nndepparser/#dp-nn-2</a></p>
<p>因为：传统特征表示稀疏、不完全、计算代价大，SVM之类的线性分类器本身是很快的，但传统 parser 的95%时间都花在拼装查询特征上了。</p>
<p>无非是传统方法拼接单词、词性、依存标签，新方法拼接它们的向量表示：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/20/21-09-43-b12466b0814951453b3d41caa1f15f7b-006Fmjmcly1fgnda5d97hj310c0nk785-cca20e.jpeg"></p>
<p>经过一个神经网络</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/21/14-50-21-f058a82f34e626efc9d98a52bdda1811-2018082819195541-7df6b7.png"></p>
<p>模型的作用</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/21/14-32-06-cc8d1bc45c631f2a64fcea495a840caa-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210621143133-5bd1c1.png"></p>
<p>如何找输入的词向量</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/21/14-51-38-2910f2e666cd2a576705b13122208268-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210621144208-e9c675.png"></p>
<p>事实上，在“深度学习”“神经网络”与传统的graph-based方法相比较，花了这么多功夫得到的只是0.1%的LAS提升：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/21/14-34-25-1b2c659ee458ce5ffc777c25aa0e32b6-006Fmjmcly1fgnd5teln2j31aw0k0gon-432664.jpeg"></p>
<h2 id="非线性函数"><a href="#非线性函数" class="headerlink" title="非线性函数"></a>非线性函数</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/21-07-49-1d505aa57921e51396ae4a5f10c67c21-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620210742-87a06a.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/20/21-08-33-86377f2b1aafcf6ddea00bc290fa1dbd-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210620210825-34d08d.png"></p>
<h2 id="未来的工作"><a href="#未来的工作" class="headerlink" title="未来的工作"></a>未来的工作</h2><p>Chen&amp;Manning的工作被许多人继续往前推进，走在最前沿的是Google。趋势是：</p>
<pre><code>更大更深调参调得更好（更昂贵）的神经网络
Beam Search
在决策序列全局进行类似CRF推断的方法（CRF宝刀未老，老当益壮啊）</code></pre>
<p>Google的SyntaxNet 中的 Parsey McParseFace的效果：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx4.sinaimg.cn/large/2021/06/20/21-04-58-895952771c1a40eb7782f00995c002d9-006Fmjmcly1fgndqwd3yaj315e0asmz0-16e0c9.jpeg"></p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>短语结构树</tag>
        <tag>依存句法树</tag>
        <tag>句法分析</tag>
      </tags>
  </entry>
  <entry>
    <title>Word-Window-Classification-Neural-Networks</title>
    <url>/2021/06/15/Word-Window-Classification-Neural-Networks/</url>
    <content><![CDATA[<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-15-14-14af5e584ec2022c16b52cdbd5762f4c-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615201501-16d775.png"></p>
<a id="more"></a>

<h1 id="交叉熵问题"><a href="#交叉熵问题" class="headerlink" title="交叉熵问题"></a>交叉熵问题</h1><h2 id="交叉熵："><a href="#交叉熵：" class="headerlink" title="交叉熵："></a>交叉熵：</h2><p><a href="https://blog.csdn.net/b1055077005/article/details/100152102" title="交叉熵问题">https://blog.csdn.net/b1055077005/article/details/100152102</a></p>
<h2 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL散度）</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-18-37-764c9101051994bc1c96d0732f8dfd0d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615201830-ca0a06.png"><br><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-10-40-e8f9d3130b950f1f3f13209ce6858bc8-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615201027-fb9e7c.png"></p>
<p>展开得</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-11-38-5219a1bdfc0571f049fab71dc707d1e0-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615201118-2a8722.png"></p>
<p>前部分为信息熵，后部分为交叉熵</p>
<p>交叉熵公式表示为：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-12-33-8e233d9fdc55ef25792d4ad09b95cc49-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615201227-bef448.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-13-04-d5e69de51a69b47682e62e73697ed588-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615201258-8d3293.png"></p>
<h2 id="相对熵习题"><a href="#相对熵习题" class="headerlink" title="相对熵习题"></a>相对熵习题</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-14-04-48545639f8e21b05b07cbbe7453f7e89-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615201336-a8d147.png"></p>
<h2 id="交叉熵的性质"><a href="#交叉熵的性质" class="headerlink" title="交叉熵的性质"></a>交叉熵的性质</h2><p>交叉熵能够衡量同一个随机变量中的两个不同概率分布的差异程度，在机器学习中就表示为真实概率分布与预测概率分布之间的差异。交叉熵的值越小，模型预测效果就越好。</p>
<p>交叉熵在分类问题中常常与softmax是标配，softmax将输出的结果进行处理，使其多个分类的预测值和为1，再通过交叉熵来计算损失。</p>
<h2 id="交叉熵与softmax的关系"><a href="#交叉熵与softmax的关系" class="headerlink" title="交叉熵与softmax的关系"></a>交叉熵与softmax的关系</h2><p><a href="https://zhuanlan.zhihu.com/p/27223959" title="Softmax函数与交叉熵">https://zhuanlan.zhihu.com/p/27223959</a></p>
<p>对于多分类问题，第 i 类样本出现的概率为</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-51-46-49136dc5e488583513b34b4a00f55c2b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615205125-d94ea4.png"></p>
<p>$t$是样本对应的类标签,$y_i$是第 i 类出现的概率</p>
<p>取对数的（连乘可能导致最终结果接近0的问题）</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-52-29-7c200889c5f47f2e457789dbc669a0c4-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615205220-b6a1b8.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/20-56-06-17334a2f35d1efcbcee891eca0763eef-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210615205556-905402.png"></p>
<p>可以看出，该等式于上面对数似然函数的形式一样！</p>
<h1 id="分类问题"><a href="#分类问题" class="headerlink" title="分类问题"></a>分类问题</h1><h2 id="优化："><a href="#优化：" class="headerlink" title="优化："></a>优化：</h2><p>一般的ML问题中，参数由权值矩阵的列组成维度不会太大。而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/16/09-33-28-adbf6c2ff08cc32d210ada78e1227cce-006Fmjmcly1fgf164jxu1j31b00kcwgx-f2decb.jpg"></p>
<p>下面两张图非常好的说明了，在更新参数的时候，及要跟新超平面相关参数，也会更新词向量相关参数（参数的大小为）。</p>
<h2 id="re-training词向量"><a href="#re-training词向量" class="headerlink" title="re-training词向量"></a>re-training词向量</h2><p>一般的ML问题中，参数由权值矩阵的列组成维度不会太大。而在词向量或其他深度学习中，需要同时学习权值矩阵和词向量。参数一多，就容易过拟合：</p>
<p>比如有一个给单词做情感分析的小任务，在预训练的词向量中，这三个表示电视的单词都是在一起的：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/21-02-16-7b27e7109dddc45fe43a210a7e4ce2bd-006Fmjmcly1fgf1biu3l2j30en0en3z7-2bf5e9.jpg"></p>
<p>但由于情感分析语料中，训练集只含有TV和telly，导致re-training之后两者跑到别处去了：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/21-03-04-a4d1b5d4ab0d27eab210cce8126e22c0-006Fmjmcly1fgf1biu3l2j30en0en3z7-0e709b.jpg"></p>
<p>于是在测试集上导致television被误分类。</p>
<p>这个例子说明，如果任务的语料非常小，则不必在任务语料上重新训练词向量，否则会导致词向量过拟合。</p>
<h1 id="Window-classification"><a href="#Window-classification" class="headerlink" title="Window classification"></a>Window classification</h1><p>这是一种根据上下文给单个单词分类的任务，可以用于消歧或命名实体分类。上下文Window的向量可以通过拼接所有窗口中的词向量得到：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/15/21-12-41-b47071cf5ef7b679398896d90031d170-v2-5f2945e81573b7cbf811a57321cf8cbe_720w-8a666e.jpg"></p>
<p>最简单的分类器：softmax</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/16/10-25-02-3840f2e053892f56f7c916d4beb390e8-006Fmjmcly1fgf1xr22uxj31e80oowjf-191b26.jpg"></p>
<p>J 对x求导，注意这里的x指的是窗口所有单词的词向量拼接向量。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/16/10-26-24-369bb65feea4693058807a1164195a60-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210616102534-68f846.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/16/10-27-48-8bc44062edf1f3a027412c0dc7849048-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210616102733-de32fb.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/16/10-27-53-578203c7d7c9275f4fe934edf9eb16a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210616102738-97c4c1.png"></p>
<p>其中 t 是第 y 个词的热编码</p>
<p><a href="https://zhuanlan.zhihu.com/p/27223959" title="softmax函数与交叉熵的关系">https://zhuanlan.zhihu.com/p/27223959</a> 中最后得出了 $σ = [ \hat{y}- t]$ 的结论</p>
<h1 id="神经网络"><a href="#神经网络" class="headerlink" title="神经网络"></a>神经网络</h1><h2 id="softmax（等价于逻辑斯谛回归）效果有限"><a href="#softmax（等价于逻辑斯谛回归）效果有限" class="headerlink" title="softmax（等价于逻辑斯谛回归）效果有限"></a>softmax（等价于逻辑斯谛回归）效果有限</h2><p>仅限于较小的数据集，能够提供一个勉强的线性分类决策边界。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/16/10-51-50-b45f360720ab393476d944dd91ac46f4-006Fmjmcly1fgf9smlwe0j30o80cdtag-a29e88.jpeg"></p>
<p>神经网络可以提供非线性的决策边界：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/16/10-52-54-a468fd421b32f7ee12bb1d78a9e32ecb-006Fmjmcly1fgf9u1m2dzj309e09d3zf-713800.jpeg"></p>
<p>神经网络的术语</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/16/10-55-50-09d790e1567d331a9533da2810b806d3-006Fmjmcly1fgf9w5ckvnj30dl0e1jtm-e6c03b.jpeg"></p>
<p>每个神经元是一个二分类逻辑斯谛回归单元：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/16/10-57-37-31b42b4c00cb8580b873dd98f56c0595-006Fmjmcly1fgf9xs1tc2j30n80er76y-21db2e.jpeg"></p>
<p>神经网络同时运行多个逻辑斯谛回归，但不需要提前指定它们具体预测什么：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx3.sinaimg.cn/large/2021/06/16/10-58-00-1a0656e28e3385bbf5f6bf4814ca35d8-006Fmjmcly1fgf9ynezpjj308c0b7dhr-9f588e.jpeg"></p>
<p>我们把预测结果喂给下一级逻辑斯谛回归单元，由损失函数自动决定它们预测什么：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx1.sinaimg.cn/large/2021/06/16/10-59-38-84f005003e1dab179233ce181b2d57ef-006Fmjmcly1fgf9zpfaa2j30eo0botbq-edb273.jpeg"></p>
<p>于是就得到了一个多层网络</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/http/wx2.sinaimg.cn/large/2021/06/16/10-59-58-416226bf8865fe79d28789bf15f55a18-006Fmjmcly1fgfa0qmyeoj30jg0bkwim-9c5982.jpeg"></p>
<p>为什么需要非线性</p>
<p>因为线性系统所有层等效于一层：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/16/11-01-03-6711e3c71c03d87eb6f21b9cfc2c353e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210616110057-b89af5.png"></p>
<p>而非线性模型可以捕捉很复杂的数据：</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/16/11-01-45-2d93666e80685b17b46eeccec780673a-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210616110127-e78c28.png"></p>
<h2 id="前向传播"><a href="#前向传播" class="headerlink" title="前向传播"></a>前向传播</h2><p>所谓的前向传播算法就是：将上一层的输出作为下一层的输入，并计算下一层的输出，一直到运算到输出层为止。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/17/09-21-46-ae480c045c628ea1dcad176da56daecb-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210617091621-699c67.png"></p>
<p>最后可以通过与期望值做对比，求出损失，为了使损失最小化，使用后向传播算法。</p>
<h2 id="后向传播"><a href="#后向传播" class="headerlink" title="后向传播"></a>后向传播</h2><p><a href="https://blog.csdn.net/bitcarmanlee/article/details/78819025" title="后向传播过程推到">https://blog.csdn.net/bitcarmanlee/article/details/78819025</a></p>
<p><a href="https://www.cnblogs.com/charlotte77/p/5629865.html" title="后向传播过程演算">https://www.cnblogs.com/charlotte77/p/5629865.html</a></p>
<p>第一个链接证明了后向传播的推导过程，推导过程非常详细，推到结果也是对的，但在最后求 $w_{31},w_{32},w_{41},w_{42}$结果是错误的<br>第二个链接推导过程不是跟详细，但是演算的结果是对的</p>
<p>反向传播（英语：Backpropagation，缩写为BP）是“误差反向传播”的简称，是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法计算对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。 在神经网络上执行梯度下降法的主要算法。该算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式计算损失函数值相对于每个参数的偏导数。</p>
<p><strong>bp算法的学习过程</strong></p>
<p>BP算法的学习过程由正向传播过程和反向传播过程组成。在正向传播过程中，输入信息通过输入层经隐含层，逐层处理并传向输出层。如果在输出层得不到期望的输出值，则取输出与期望的误差的平方和作为目标函数，转入反向传播，逐层求出目标函数对各神经元权值的偏导数，构成目标函数对权值向量的梯量，作为修改权值的依据，网络的学习在权值修改过程中完成。误差达到所期望值时，网络学习结束。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/17/09-26-15-ad4f18eadd27e8e8b87dd6cc42971b5e-f408ba03f7685c23f9daf05fccdfef07-9fdd83.png"></p>
<p>$w_{54}$ 求梯度的过程</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/17/09-27-19-74bb570260d428fc7759837f765f697a-70cab29fb1769089ea951550412b84b5-5a51cc.png"></p>
<p>$w_{31}$ 求梯度的过程</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/17/09-28-47-118722f5e16238a2590f9961990ac456-78406d4d609f2383b8253d16d35d48e9-8eb888.png"></p>
<p>可以通过梯度更新权值，使得损失损失最小化</p>
]]></content>
      <tags>
        <tag>NLP</tag>
        <tag>正向传播</tag>
        <tag>反向传播</tag>
        <tag>BP</tag>
        <tag>交叉熵</tag>
      </tags>
  </entry>
  <entry>
    <title>高级词向量表示</title>
    <url>/2021/06/14/gao-ji-ci-xiang-liang-biao-shi/</url>
    <content><![CDATA[<p>词向量的 Negative Sampling 与 Hierachical Softmax</p>
<a id="more"></a>

<h1 id="为什么-word2vec-会消耗大量的时间"><a href="#为什么-word2vec-会消耗大量的时间" class="headerlink" title="为什么 word2vec 会消耗大量的时间"></a>为什么 word2vec 会消耗大量的时间</h1><pre><code>是因为 softmax 会消耗大量的时间，softmax 的分母与整个语料库有关（呈线性关系），
而且指数运算也比较耗时，所以计算量超级大</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/14-35-33-6f9226f894a22cf472522cb2727b4d82-20200701205113938-9bacf0.png"></p>
<h1 id="解决方法有以下几点"><a href="#解决方法有以下几点" class="headerlink" title="解决方法有以下几点"></a>解决方法有以下几点</h1><h2 id="把常见的词组作为一个单词"><a href="#把常见的词组作为一个单词" class="headerlink" title="把常见的词组作为一个单词"></a>把常见的词组作为一个单词</h2><pre><code>作者指出像“Boston Globe“（一家报社名字）这种词对，和两个单词 Boston／Globe有着完全不同语义。
所以更合理的是把“Boston Globe“看成一个单词，有他自己的word vector。</code></pre>
<h2 id="少采样常见的词-（比如：A-the）"><a href="#少采样常见的词-（比如：A-the）" class="headerlink" title="少采样常见的词 （比如：A the）"></a>少采样常见的词 （比如：A the）</h2><pre><code>像“the“这种常见的词，我们会遇到两个问题：
比如（fox，the）其实没有传递我们关于 fox的信息。‘the‘出现得太多了；
我们有太多 （‘the‘，…）的样本，多于我们真的需要的；
所以word2vec采用了降采样的策略。对于每个我们在训练样本中遇到的词，
我们有一个概率去删除它，称之为“采样率”这个概率与单词出现的频率相关。</code></pre>
<h3 id="采样率"><a href="#采样率" class="headerlink" title="采样率"></a>采样率</h3><p>我们使用$w_i$来表示单词，$z(w_i)$表示它出现在词库中的概率（频率）。比如花生在1bilion的词库中出现了1,000次，那么$z(花生)=1E^{-6}$。在代码中还有一个参数叫“sample”，这个参数代表一个阈值，默认值为0.001.这个值越小意味着这个单词被保留下来的概率越小（即有越大的概率被我们删除）。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/16-09-57-f621c0a36618280a1e24bb9ca0d4bf71-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210614160948-7c3bc2.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/16-10-20-b5040c150edb31a6ee445d4ad771a1ab-v2-b5040c150edb31a6ee445d4ad771a1ab_r-acb4f9.jpg"></p>
<p>图中x轴代表着$Z(w_i)$，即单词$w_i$在语料中出现频率，y轴代表某个单词被保留的概率。对于一个庞大的语料来说，单个单词的出现频率不会很大，即使是常用词，也不可能特别大。从这个图中，我们可以看到，随着单词出现频率的增高，它被采样保留的概率越来越小。</p>
<h2 id="负采样"><a href="#负采样" class="headerlink" title="负采样"></a>负采样</h2><pre><code>使得每个训练样本只去更新模型中一小部分的weights。</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/15-24-23-07ae3164fc39fff912ff3e44be81a3b0-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210614152412-935b9e.png"></p>
<p>当我们用训练样本 ( input word: “fox”，output word: “quick”) 来训练我们的神经网络时，“ fox”和“quick”都是经过one-hot编码的。如果我们的vocabulary大小为10000时，在输出层，我们期望对应“quick”单词的那个神经元结点输出1，其余9999个都应该输出0。在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们称为“negative” word。</p>
<p>当使用负采样时，我们将随机选择一小部分的negative words（比如选5个negative words）来更新对应的权重。我们也会对我们的“positive” word进行权重更新（在我们上面的例子中，这个单词指的是”quick“）。</p>
<p>假如我们的隐层-输出层拥有300 x 10000的权重矩阵。如果使用了负采样的方法我们仅仅去更新我们的positive word-“quick”的和我们选择的其他5个negative words的结点对应的权重，共计6个输出神经元，相当于每次只更新 300 * 6 = 1800个权重。对于3百万的权重来说，相当于只计算了0.06%的权重，这样计算效率就大幅度提高。</p>
<h1 id="skip-gram-负采样公式推导："><a href="#skip-gram-负采样公式推导：" class="headerlink" title="skip-gram 负采样公式推导："></a>skip-gram 负采样公式推导：</h1><p>1、条件概率分布使用 sigmod 函数表示</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/20-58-15-d6de486aa70b29dbfc07b2997421d2e3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210614202954-3fdba7.png"></p>
<p>2、按照一定的概率分布$P(\widetilde{w})$从样本中抽取K个负样本，其一部分表示从中心词和上下文词共同出现的概率，第二部分表示中心词和负样本词共同出现的概率。然该公式最大化目的是让中心词和上下文词和中心词和负样本词以尽可能大的概率共同出现</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/21-05-06-98abe8c03f0ba32be639f7ed419e8346-%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20210614210455-88a16e.png"></p>
<p>3、将 sigmod 函数带入得到下列公式，求最大化</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/20-58-19-c8f1262e4eaa24ad57aefa0d7194a946-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210614205558-1e824d.png"></p>
<p>4、添加符号求最小化问题</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/20-58-27-6dc4139cc1555cdb28e6d63af9fd09eb-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210614205611-fbfacc.png"></p>
<h1 id="CBOW-负采样公式推导："><a href="#CBOW-负采样公式推导：" class="headerlink" title="CBOW 负采样公式推导："></a>CBOW 负采样公式推导：</h1><p>推导过程基本与 skip-gram 类似，如下</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/20-58-34-08bfdd0c91f36d2ddf906e520d89a377-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210614205748-55bc12.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/14/20-58-39-2d323463050b7dd14a9c70a59209ae80-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210614205802-7c665c.png"></p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>将负采样近似训练方法和标准word2vec进行对比，可以发现负采样在标准word2vec的基础上做了两点改进：</p>
<p>1.针对softmax运算导致的每次梯度计算开销过⼤，将softmax函数调整为sigmoid函数，当然对应的含义也由给定中心词，每个词作为背景词的概率，变成了给定中心词，每个词出现在背景窗口中的概率</p>
<p>2.进行负采样，引入负样本，负采样的名字就是取了第二个改进点。</p>
]]></content>
      <tags>
        <tag>skip-gram</tag>
        <tag>word2vec</tag>
        <tag>NLP</tag>
        <tag>CBOW</tag>
        <tag>负采样</tag>
        <tag>negative sampling</tag>
      </tags>
  </entry>
  <entry>
    <title>skip-gram</title>
    <url>/2021/06/13/skip-gram/</url>
    <content><![CDATA[<p>skip-gram 流程图解释如下：</p>
<a id="more"></a>

<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/Users/93497/Desktop/2021/06/13/10-49-14-4326e64fe0f495f6632be2093c165e8c-006Fmjmcly1fgco3v2ca7j30pq0j7drt%20-2--0670c6.jpg"></p>
<ol>
<li>$w_t$ 表示单词的 one-hot 编码  维度为 $V * 1$，$V$ 为文本中单词的个数</li>
<li>$W$ 表示中心词向量矩阵，是需要优化的变量，开始时随机初始化，维度为 $d * V$，$V$ 同上，$d$ 是超参数需要手动设置，一般在 50 - 300</li>
<li>$V_c$ 表示中心词向量，是由 $W$ 与 $w_t$ 做矩阵乘法的结果</li>
<li>$W’$ 表示上下文词向量矩阵，也是需要优化的变量，开始时随机初始化，维度为$V * d$。$V，d$ 同上</li>
<li>是 $W’$ 与 $V_c$ 做矩阵乘法得到的结果，维度为 $V * 1$，5 部分列举的 3 个向量是一样的</li>
<li>$P(x|c)$ 表示在 c 出现的前提下，x 出现的概率，条件概率函数定义为 softmax 函数，维度为 $V * 1$。在 6 中出现的 3 个向量也是一样的，向量的第 1 个数表示，在中心词 c 出现的条件下，第一个词出现的概率。</li>
<li>Truth 表示真实出现的词，7 中第 1 个向量表示，在中心词前面的第 3 个词，是 one-hot 编码的第 6 个词</li>
<li>根据 6，7 求损失</li>
<li>由梯度的的推导公式更新 $W$ 和 $W’$</li>
</ol>
]]></content>
      <tags>
        <tag>skip-gram</tag>
        <tag>word2vec</tag>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title>方向选择</title>
    <url>/2021/03/08/fang-xiang-xuan-ze/</url>
    <content><![CDATA[<p>从几篇文章中选一下研究方向，简单看下三篇文章的摘要</p>
<a id="more"></a>

<h1 id="1、A-Survey-on-Multi-Label-Data-Stream-Classification"><a href="#1、A-Survey-on-Multi-Label-Data-Stream-Classification" class="headerlink" title="1、A Survey on Multi-Label Data Stream Classification"></a>1、A Survey on Multi-Label Data Stream Classification</h1><p>题目：多标签数据流分类研究</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/fang-xiang-xuan-ze/2021/03/08/19-24-58-5e43a376d1da6b50629fdbc293f410c4-A%20Survey%20on%20Multi-Label%20Data%20Stream%20Classification_abstract-7cd420.png"></p>
<p>摘要：如今，我们日常生活中的许多现实应用都产生了大量的流数据，产生数据的速度比以往任何时候都高，仅举几个例子，即 Web 点击数据流，感知网络数据和信用交易流。与使用静态数据集的传统数据挖掘相反，数据流挖掘有几个挑战，例如有限的内存，一站式及时响应。在这项调查中，我们对现有的多标签流挖掘算法进行了全面的回顾，并根据不同的角度对这些方法进行分类，主要集中在多标签数据流分类。我们首先简要概述现有的多标签和数据流分类算法，以及讨论他们的优点和缺点。其次，我们确定了多标签流数据分类的挖掘约束，并提出了对多标签数据流分类算法的全面研究。最后，讨论了多标签数据流分类中的一些挑战和未解决的问题，这些都是值得今后研究人员继续探索的。</p>
<p>关键字：数据流挖掘;多标签数据;多标签分类</p>
<p>流数据：流数据是由不同来源连续生成的数据</p>
<h1 id="2、Information-Security-in-Big-Data-Privacy-and-Data-Mining"><a href="#2、Information-Security-in-Big-Data-Privacy-and-Data-Mining" class="headerlink" title="2、Information Security in Big Data: Privacy and Data Mining"></a>2、Information Security in Big Data: Privacy and Data Mining</h1><p>题目：大数据中的信息安全：隐私和数据挖掘</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/fang-xiang-xuan-ze/2021/03/08/19-27-26-921f9c899880f484ed0ca731f07be320-Information%20Security%20in%20Big%20Data%20Privacy%20and%20Data%20Mining_abstract-3715a4.png"></p>
<p>摘要：数据挖掘技术的日益普及和发展对个人敏感信息的安全性构成了严重威胁。数据挖掘中的一个新兴研究主题，即隐私保护数据挖掘（PPDM），近年来已经得到了广泛的研究。PPDM的基本思想是修改数据，以便有效地执行数据挖掘算法，而不会损害数据中包含的敏感信息的安全性。PPDM的当前研究主要集中在如何减少数据挖掘操作带来的隐私风险上，而实际上，在数据收集，数据发布和信息（即：数据挖掘的结果）传递过程中也可能发生不希望的敏感信息泄露。在本文中，我们从更广阔的角度看待与数据挖掘相关的隐私问题，并研究可以帮助保护敏感信息的各种方法。特别是，我们确定了四种不同类型的用户，它们是数据挖掘应用程序，即数据提供者，数据收集者，数据挖掘者和决策者。对于每种类型的用户，我们讨论他的隐私问题以及可以用来保护敏感信息的方法。我们简要介绍了相关研究主题的基础知识，回顾了最先进的方法，并对未来的研究方向提出了一些初步的想法。<br>除了探讨每种类型用户的隐私保护方法外，我们还回顾了博弈论方法，该方法旨在分析数据挖掘场景中不同用户之间的交互，每个人对敏感信息都有自己的评估。通过区分不同用户关于敏感信息安全方面的职责，我们希望提供一些对于PPDM研究有用见解。</p>
<p>关键字：数据挖掘;敏感信息;隐私保护数据挖掘;匿名化;追踪;博弈论;隐私拍卖;反追踪</p>
<h1 id="3、Mining-Conditional-Functional-Dependency-Rules-on-Big-Data"><a href="#3、Mining-Conditional-Functional-Dependency-Rules-on-Big-Data" class="headerlink" title="3、Mining Conditional Functional Dependency Rules on Big Data"></a>3、Mining Conditional Functional Dependency Rules on Big Data</h1><p>题目：在大数据上挖掘条件功能依赖规则</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/fang-xiang-xuan-ze/2021/03/08/19-10-58-9f4b91438594f4fde6de1df53a3a1dac-Mining%20Conditional%20Functional%20Dependency%20Rules%20on%20Big%20Data_abstract-bff334.png"></p>
<p>摘要：当前的条件功能依赖（CFD）发现算法始终需要准备充分的训练数据集。这种情况使它们难以应用于大型和低质量的数据集。为了处理大数据的体积问题，我们开发了采样算法来获得一个小的有代表性的训练集。我们设计了容错规则发现和冲突解决算法，以解决大数据的低质量问题。我们还提出了参数选择策略，以确保CFD发现算法的有效性。实验结果表明，我们的方法可以在合理的时间内发现数十亿元数据的有效CFD规则。</p>
<p>关键字：数据挖掘;条件功能依赖;大数据;数据质量</p>
]]></content>
      <tags>
        <tag>数据流挖掘</tag>
        <tag>多标签数据</tag>
        <tag>多标签分类</tag>
        <tag>数据挖掘</tag>
        <tag>敏感信息</tag>
        <tag>条件功能依赖</tag>
        <tag>大数据</tag>
        <tag>数据质量</tag>
      </tags>
  </entry>
  <entry>
    <title>0-1 背包问题</title>
    <url>/2021/01/22/0-1-bei-bao/</url>
    <content><![CDATA[<p>使用动态规划解决 0-1 背包问题</p>
<a id="more"></a>

<h1 id="一、问题描述"><a href="#一、问题描述" class="headerlink" title="一、问题描述"></a>一、问题描述</h1><p>有 n 个物品，它们有各自的重量和价值，现有给定容量的背包，</p>
<p>如何让背包里装入的物品具有最大的价值总和？（注意：每种物品只能放入到背包中一次）</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/12/11-23-18-d873196dcb9b54a1001a4559f2d350a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111327-2d3f85.png"></p>
<p>上图给出了 a、b、c、d 4 种物品以及每种物品对应的体积和价值。</p>
<p>我们将要求体积为 8 的背包最多可以存放多大的价值。</p>
<h1 id="二、预备知识"><a href="#二、预备知识" class="headerlink" title="二、预备知识"></a>二、预备知识</h1><p><strong>1、将使用 w(i) 表示物品 i 的重量，如</strong></p>
<pre><code>w(a) 表示物品 a 的重量
w(b) 表示物品 b 的重量
w(c) 表示物品 c 的重量
w(d) 表示物品 d 的重量</code></pre>
<p><strong>2、将使用 v(i) 表示物品 i 的价值，如</strong></p>
<pre><code>v(a) 表示物品 a 的价值
v(b) 表示物品 b 的价值
v(c) 表示物品 c 的价值
v(d) 表示物品 d 的价值</code></pre>
<p><strong>3、dp[i][j] 的含义</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/13/16-22-44-933b8ac26bb816025a4e8ec066c5dfca-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210313161113-0977f1.png"></p>
<p>上图为初始化时，动态规划所用到的 dp 数组</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/14/21-55-23-e50b7b34bc1fb1d530cf9d57686242b6-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111818-eb374c.png"></p>
<p>上图为动态规划执行完毕之后得到的二维数组 dp，怎么构建该数组会在下面介绍，</p>
<p>这里先不用先关注计算的过程</p>
<p>我们使用二维数组 dp 表示动态规划需要构造的数组，其中 dp[i][j] 表示，</p>
<p>当背包的<strong>容量为 j</strong> 时，且只有<strong>前 i 件</strong>物品时，</p>
<p>背包里装入物品的<strong>最大</strong>的价值总和为 <strong>dp[i][j]</strong></p>
<p><strong>4、对于 dp[i][j] 的进一步解释</strong></p>
<p>对于本题来说，我们假设</p>
<pre><code>a 为第一件商品        b 为第二件商品        c 为第三件商品        d 为第四件商品</code></pre>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/14/20-06-20-6ac242d1cdca69d1d5d7005911f49f6e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210314200427-a67598.png"></p>
<p>图中标黄的值可以用 dp[3][7] = 9 表示</p>
<p>她所代表的含义是当背包的容量为 7 时，且只有前 3 件物品时，</p>
<p>背包里装入物品的最大价值总和为 9，</p>
<p>dp[3][7] = 9 需要注意到的重点</p>
<pre><code>1、只有前 3 件商品，即 a b c 这三件商品，即使物品的总数量为 4，但是就 dp[3][7]

来说只能取前三件，要是需要取全部的 4 件商品，需要求 dp[4][7]

2、背包的容量是 7，即使背包总容量是 8，但是就 dp[3][7] 来说，目前背包的容量为 7

3、p[3][7] 表示 当只有前 3 件商品时，并且背包的容量为 7 时，

背包里面能装的最大价值为 9，这里的 9 表示的是最大价值，是一个最优的结果，

无论前三种商品怎么组合，在现有的容量下，能够得到的最大价值是 9</code></pre>
<p><strong>5、动态规划矩阵（即 dp 矩阵）计算的方向</strong></p>
<p>我们在计算 dp 矩阵的时候，计算的方向是从上到下，从左到右依次计算的</p>
<p>这又有什么意义呢？</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/13/16-51-17-c164af316dee7f8f802a27336cecf586-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210313164831-7bafeb.png"></p>
<p>假设我们需要计算图中标黄的值（该值可以用 dp[2][4] 表示），</p>
<p>按照从上到下，从左到右的计算方向，计算的顺序是</p>
<pre><code>dp[0][0] -&gt; dp[1][0] -&gt; dp[2][0] -&gt; dp[3][0] -&gt; dp[4][0] -&gt; 
dp[0][1] -&gt; dp[1][1] -&gt; dp[2][1] -&gt; dp[3][1] -&gt; dp[4][1] -&gt;
dp[0][2] -&gt; dp[1][2] -&gt; dp[2][2] -&gt; dp[3][2] -&gt; dp[4][2] -&gt; 
dp[0][3] -&gt; dp[1][3] -&gt; dp[2][3] -&gt; dp[3][3] -&gt; dp[4][3] -&gt;
dp[0][4] -&gt; dp[1][4] -&gt; dp[2][4]</code></pre>
<p>也就是说在计算 标黄值位置的时候，标绿值的位置已经全部都计算完毕了</p>
<p><strong>6、数据动态变化的过程</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/13/16-51-17-c164af316dee7f8f802a27336cecf586-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210313164831-7bafeb.png"></p>
<p>还是拿着上图标黄的值来搞事情</p>
<p>计算 dp[2][4] = 4 的原因</p>
<p>根据计算方向，求完了 dp[1][4] = 3 才能求 dp[2][4] = 4,</p>
<p>dp[1][4] = 3 说明了当背包的容量为 4 时，</p>
<p>如果我有前 1 件商品（即 只有 a 商品），背包中能存放的最大价值为 3，</p>
<p>但是在计算 dp[2][4] 时，由计算 a 这一件商品变成了计算 a b 两件商品</p>
<p>这是我们需要在 a b 中选择物品添加到背包中，</p>
<p>背包还是那个背包（容量没有发生变化），这时背包能存放的总价值变成了 4。</p>
<p>这就是由 dp[1][4] -&gt; dp[2][4] 的过程</p>
<h1 id="三、对于公式的理解"><a href="#三、对于公式的理解" class="headerlink" title="三、对于公式的理解"></a>三、对于公式的理解</h1><p>几乎在所有的博客之中都会出现下列公式，那么就让我来给你解释一下它的意义；</p>
<pre><code>dp[i][j] = max &#123; dp[i - 1][j], dp[i - 1][j - w[i]] + v[i] &#125;</code></pre>
<p>在 <strong>二、符号化</strong>中 我们已经介绍了 w[i] 和 v[i] 的含义，</p>
<p>其中 w[i] 表示物品 i 的重量，v[i] 表示物品 i 的价值</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/13/16-51-17-c164af316dee7f8f802a27336cecf586-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210313164831-7bafeb.png"></p>
<p>物品的体积和价值图</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/12/11-23-18-d873196dcb9b54a1001a4559f2d350a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111327-2d3f85.png"></p>
<p>上图中 </p>
<pre><code>dp[2][4] = max &#123; dp[2 - 1][4],dp[2 - 1][4 - w(b)] + v(b) &#125;

= max&#123;dp[1][4],dp[1][1] + 4&#125; = max&#123;3，4&#125; = 4</code></pre>
<p>影响 dp[2][4] 值大小的影响因素有两个，一个是 dp[1][4]，另一个是 dp[1][1] + 4，</p>
<p>下面分别介绍两部分的值是如何得到的。</p>
<h2 id="1、当物品-b-出现时，背包装不下-（此处是用来求-dp-1-4-大小）"><a href="#1、当物品-b-出现时，背包装不下-（此处是用来求-dp-1-4-大小）" class="headerlink" title="1、当物品 b 出现时，背包装不下 （此处是用来求 dp[1][4] 大小）"></a>1、当物品 b 出现时，背包装不下 （此处是用来求 dp[1][4] 大小）</h2><p><strong>1、dp[1][4] 是如何来的</strong></p>
<p>当物品 b 出现时，背包的容量已经放不下物品 b 了，</p>
<p>这时背包能够存放的最大价值就是只有前 1 件货物（即只有货物 a）时所能存放最大价值，</p>
<p>因为没法把 b 放入到背包，所以背包存放的最大价值不会发生改变</p>
<p>背包的总价值就是前 1 件物品（只有货物 a）的总价值，即 dp[1][4]</p>
<p><strong>2、知识延申</strong></p>
<p>dp[i][j] 在这种情况下可以表示为，</p>
<p>当物品 i 出现时，背包的容量已经放不下物品 i 了，</p>
<p>这时背包能够存放的最大价值就是只有前 i - 1 件货物时所能存放最大价值，</p>
<p>因为没法把 i 放入到背包，所以背包存放的最大价值不会发生改变，</p>
<p>背包的容量为 j 时，背包的总价值就是只有前 i - 1 时的最大价值</p>
<p>我们知道 dp[i-1][j] 就代表了背包的容量为 j 时，背包的总价值就是只有前 i - 1 时的最大价值</p>
<p>所以公式中前一部分搞定了，即图中划红线的部分</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/13/17-52-16-63dc62414102a1b76abc36f254cabe4e-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210313171644-5652a0.png"></p>
<h2 id="2、当物品-b-出现时，背包能装下-（此处是用来求-dp-1-1-4-的大小）"><a href="#2、当物品-b-出现时，背包能装下-（此处是用来求-dp-1-1-4-的大小）" class="headerlink" title="2、当物品 b 出现时，背包能装下 （此处是用来求 dp[1][1] + 4 的大小）"></a>2、当物品 b 出现时，背包能装下 （此处是用来求 dp[1][1] + 4 的大小）</h2><p><strong>1、dp[1][1] + 4 是如何来的</strong></p>
<p>当物品 b 出现时，物品 b 能够放入到背包中，那么背包就要留出一定的空间，</p>
<p>那么需要留出多大的空间呢？</p>
<p>留出的空间的大小是刚好能够容纳 b，这样可以充分利用背包中的空间。</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/12/11-23-18-d873196dcb9b54a1001a4559f2d350a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111327-2d3f85.png"></p>
<p>就本题而言，w(b) = 3,也就是说我们再没有把 b 放入背包之前需要留出 3 的空间大小</p>
<p><strong>重点强调</strong></p>
<pre><code>1、因为要留出空间来存放 b，所以要在 b 未放入之前为 b 留出空间
2、留出空间的大小为 3，因为这样刚好能够放下 b
3、若要能满足以上的条件，我们从满足添加的数据里面挑选一个价值最大的数据就行了</code></pre>
<p>对于本题来说，我们要保证在背包容量小于等于 1（即 4 - w(b) = 4 - 3 = 1） 的时候，</p>
<p>才能安全的把 b 物品放入到背板</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/13/17-30-07-cce8c6e9fdf6024371afeaa913deeab8-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210313172958-9bf56e.png"></p>
<p>我们用上图来分析一下那些区域可以放 c 物品，图中蓝色区域是已经有了 b 货物，所以不能选，</p>
<p>橙色区域表示<strong>若</strong>我们把 b 物品放入到背包中，背包的容量是放不下的，</p>
<p>绿色区域是安全区域，我们可以安全的把 b 货物放入到背包,</p>
<p>对于 dp[0][0],dp[1][0] 和 dp[0][1],dp[1][1],我们要选dp[0][1],dp[1][1],因为 背包的容量大啊，能装的东西就多啊，能得到的价值就多啊，你 dp[0][0],dp[1][0],能装的 我 dp[0][1],dp[1][1] 都能装啊，而且我还能比你更能装。</p>
<p>但是对于 dp[0][1],dp[1][1] 这两个值，我们要选择 dp[1][1],因为 dp[0][0] 是在前 0 种物品中选取物品，而我们要把第 2 件商品放入背包之前，是要在前 1 件物品中选择物品的</p>
<p>我们选择了 dp[1][1],这是背包有多余的空间放入 b 物品，这时将 b 物品放入背包，</p>
<p>这时背包的最大价值为 dp[1][1] + v(b) = dp[1][1] + 4 = 4</p>
<p><strong>2、知识延申</strong></p>
<p>dp[i][j] 在这种情况下可以表示为，</p>
<p>当物品 i 出现时，物品 i 能够放入到背包中，那么背包就要留出一定的空间,</p>
<p>留出的空间大小为 w(i)，保证背包的容量为 j - w（i） 这样刚好放下物品 i</p>
<p>而且还要在前 i - 1 件货物中挑选物品，</p>
<p>而且还要保证得到最大的价值，只能选择 dp[i - 1][j - w(i)]</p>
<p>选择了 dp[i - 1][j - w(i)] 之后，物品 i 就可以放入到背包中了，</p>
<p>这时背包的最大价值就是 dp[i - 1][j - w(i)] + v[i]</p>
<p>这里解释了公式的后半部分</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/14/21-57-56-36765a6a260f1f821c9a354d159dad03-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210314215731-a22cca.png"></p>
<h2 id="3、补充"><a href="#3、补充" class="headerlink" title="3、补充"></a>3、补充</h2><p><strong>1、当背包中容量留不出第 i 件物品需要的空间</strong></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/15/19-47-47-e5ad3d771ad98712fe209446bf94040f-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210315193253-f27007.png"></p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/12/11-23-18-d873196dcb9b54a1001a4559f2d350a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111327-2d3f85.png"></p>
<p>图中标绿的位置可以使用 dp[2][2] = 3 表示，按照上述公式，该位置为</p>
<pre><code>dp[2][2] = max&#123; dp[1][2], dp[1][2 - w(b)] + v(a) &#125;</code></pre>
<p>由于在后半部分 2 - w(b) 2 - 3 &lt; -1 &lt; 0,这说明无论背包怎么留空间都不足以给物品 b 留出足够的空间，</p>
<p>所以这时候 dp[2][2] 只来自于第一部分，即 dp[2][2] = dp[1][2]</p>
<p>在完善以下上述公式可以得到如下公式：</p>
<pre><code>dp[i][j] = max &#123; dp[i - 1][j], dp[i - 1][j - w(i)] + v(i) &#125;  j - w(i) &gt;=0
dp[i][j] = dp[i - 1][j]                                         j - w(i) &lt; 0</code></pre>
<h1 id="四、对于本题来说完整的执行过程"><a href="#四、对于本题来说完整的执行过程" class="headerlink" title="四、对于本题来说完整的执行过程"></a>四、对于本题来说完整的执行过程</h1><p>下述的所有过程会在代码中体现出来</p>
<h2 id="1、初始化"><a href="#1、初始化" class="headerlink" title="1、初始化"></a>1、初始化</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/15/20-00-43-80fd021643237a39faff9da800387918-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210315195715-b52a66.png"></p>
<p>将上图中标记为绿色和黄色的部分初始化为 0</p>
<p><strong>这样做的好处</strong></p>
<p>1、便于计算，不如说我们在计算橙色位置 dp[1][1] 的时候，</p>
<p>这时候背包的总容量时小于 a 的体积的，所以无法留出阻攻的空间来存放物品 a，</p>
<p>所以根据公式该处的位置直接是 dp[1][1] = dp[0][1] = 0</p>
<p>这样可以省略一些判断</p>
<p>2、也符合逻辑，绿色部分表示，一件物品都没有，所以背包能存放的最大价值为 0，</p>
<p>因为没有东西可以往背包中放；黄色部分表示，背包的容量为 0，</p>
<p>这时候无论什么物品都没法放入到背包中，所以背包能存放的最大价值为 0</p>
<h2 id="2、第一部分计算"><a href="#2、第一部分计算" class="headerlink" title="2、第一部分计算"></a>2、第一部分计算</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/15/20-29-45-49d1af40a82559ce820b70c206a3ea88-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210315202934-73abd8.png"></p>
<p>对于上图来说，灰色部分表示已经计算完毕的部分，绿色部分表示即将要计算的部分</p>
<p>他们分别用 dp[1][1],dp[2][1],dp[3][1],dp[4][1] 表示</p>
<p>物品的体积和价值表</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/12/11-23-18-d873196dcb9b54a1001a4559f2d350a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111327-2d3f85.png"></p>
<p><strong>对于 dp[1][1]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(a) = 1 - 2 = -1 &lt; 0（背包留不出足够的空间)</p>
<p>所以有</p>
<pre><code>dp[1][1] = dp[1 - 1][1] = dp[0][1] = 0</code></pre>
<p><strong>对于 dp[2][1]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(b) = 1 - 3 = -2 &lt; 0（背包留不出阻攻的空间）</p>
<pre><code>dp[2][1] = dp[2 - 1][1] = dp[1][1] = 0</code></pre>
<p><strong>对于 dp[3][1]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(c) = 1 - 4 = -3 &lt; 0（背包留不出阻攻的空间）</p>
<pre><code>dp[3][1] = dp[3 - 1][1] = dp[2][1] = 0</code></pre>
<p><strong>对于 dp[4][1]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(d) = 1 - 5 = -4 &lt; 0（背包留不出阻攻的空间）</p>
<pre><code>dp[4][1] = dp[4 - 1][1] = dp[3][1] = 0</code></pre>
<p>这一列计算完毕的结果如下图，</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/15/20-46-44-c57957c93415fc3f346dc6bbfd60488d-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210315204634-84beff.png"></p>
<p>其中灰色的部分表示已经计算完毕</p>
<h2 id="3、第二部分计算"><a href="#3、第二部分计算" class="headerlink" title="3、第二部分计算"></a>3、第二部分计算</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/15/20-41-19-50918d671635b22991b8943ed889d49b-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210315204107-bd96af.png"></p>
<p>对于上图来说，灰色部分表示已经计算完毕的部分，绿色部分表示即将要计算的部分</p>
<p>他们分别用 dp[1][2],dp[2][2],dp[3][2],dp[4][2] 表示</p>
<p>物品的体积和价值表</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/12/11-23-18-d873196dcb9b54a1001a4559f2d350a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111327-2d3f85.png"></p>
<p><strong>对于 dp[1][2]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(a) = 2 - 2 = 0 &gt;= 0（背包能留出足够的空间)</p>
<p>所以有</p>
<pre><code>dp[1][2] = max&#123; dp[0][2],dp[0][0] + v(a) &#125; = max&#123; dp[0][2],dp[0][0] + 3 &#125;
= max&#123; 0,3 &#125; = 3</code></pre>
<p><strong>对于 dp[2][2]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(b) = 2 - 3 = -1 &lt; 0（背包不能留出足够的空间)</p>
<p>所以有</p>
<pre><code>dp[2][2] = dp[1][2] = 3</code></pre>
<p><strong>对于 dp[3][2]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(b) = 2 - 4 = -2 &lt; 0（背包不能留出足够的空间)</p>
<p>所以有<br>    dp[3][2] = dp[2][2] = 3</p>
<p><strong>对于 dp[4][2]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(b) = 2 - 5 = -3 &lt; 0（背包不能留出足够的空间)</p>
<p>所以有<br>    dp[4][2] = dp[3][2] = 3</p>
<p>这一列计算完毕的结果如下图，</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/15/20-52-42-756817a6e9d93cd806c4c141121bf82c-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210315205229-04c016.png"></p>
<h2 id="4、第三部分计算"><a href="#4、第三部分计算" class="headerlink" title="4、第三部分计算"></a>4、第三部分计算</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/16/20-27-07-05840f576e8afc56aacc3b91dfff63e0-20210316202555-9c8ff2.png"></p>
<p>对于上图来说，灰色部分表示已经计算完毕的部分，绿色部分表示即将要计算的部分</p>
<p>他们分别用 dp[1][3],dp[2][3],dp[3][3],dp[4][3] 表示</p>
<p>物品的体积和价值表</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/12/11-23-18-d873196dcb9b54a1001a4559f2d350a3-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210312111327-2d3f85.png"></p>
<p><strong>对于 dp[1][3]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(a) = 3 - 2 = 1 &gt;= 0（背包能留出足够的空间)</p>
<p>所以有<br>    dp[1][3] = max{ dp[0][3],dp[0][1] + v(a) } = max{ dp[0][3],dp[0][1] + 3 }<br>    = max{ 0,3 } = 3</p>
<p><strong>对于 dp[2][3]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(a) = 3 - 3 = 0 &gt;= 0（背包能留出足够的空间)</p>
<p>所以有<br>    dp[2][3] = max{ dp[1][3],dp[1][0] + v(a) } = max{ dp[1][3],dp[1][0] + 4 }<br>    = max{ 0,4 } = 4</p>
<p><strong>对于 dp[3][3]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(a) = 3 - 4 = -1 &lt; 0（背包能留出足够的空间)</p>
<p>所以有<br>    dp[3][3] = dp[2][3] = 4</p>
<p><strong>对于 dp[4][3]</strong></p>
<p>判断背包能不能留出足够的空间</p>
<p>利用公式 j - w(a) = 3 - 5 = -2 &lt; 0（背包能留出足够的空间)</p>
<p>所以有<br>    dp[4][3] = dp[3][3] = 4</p>
<p>这一列计算完毕的结果如下图，</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/0-1-bei-bao/2021/03/16/20-34-55-e994224c25d25b206b1abaf263b958fd-20210316203441-afeb6f.png"></p>
]]></content>
      <tags>
        <tag>dp</tag>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>背包问题</tag>
        <tag>0-1 背包</tag>
      </tags>
  </entry>
  <entry>
    <title>使用并查集查找无向图回路</title>
    <url>/2021/01/14/bing-cha-ji/</url>
    <content><![CDATA[<p> 一步步的教你使用并查集查找无向图回路</p>
<a id="more"></a>

<h1 id="一、并查集的操作"><a href="#一、并查集的操作" class="headerlink" title="一、并查集的操作"></a>一、并查集的操作</h1><p>1、查找（find）：确定元素属于哪个集合</p>
<p>2、合并（union）：将两个集合归并成一个集合</p>
<h1 id="二、查找、合并操作代码"><a href="#二、查找、合并操作代码" class="headerlink" title="二、查找、合并操作代码"></a>二、查找、合并操作代码</h1><p><strong>1、查找操作 代码</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public int find(int x, int[] find) &#123;</span><br><span class="line">    if (find[x] &lt; 0)</span><br><span class="line">        return x;</span><br><span class="line">    else</span><br><span class="line">        return find(find[x], find);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>2、合并操作 代码</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public void union(int root1, int root2, int[] find) &#123;</span><br><span class="line">    find[root1] &#x3D; root2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h1 id="三、-实例讲解"><a href="#三、-实例讲解" class="headerlink" title="三、 实例讲解#"></a>三、 实例讲解#</h1><h2 id="1、准备工作"><a href="#1、准备工作" class="headerlink" title="1、准备工作"></a>1、准备工作</h2><p><strong>1、初始化</strong></p>
<pre><code>1、在无向图中存在几个顶点，就开辟一个长度为顶点数量大小的数组， 并初始化为 -1，数组记为 trance</code></pre>
<p><strong>2、find 函数的作用</strong></p>
<p><em>1、</em> 若此时 trance 数组为图示状态，先不用计较该数组是如何得到的，只需要知道 find 函数的执行流程，下边会介绍如何构建 trance 数组</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/21/16-50-45-14cbd3089cc9c8fedbb709741155f999-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210121165039-e52219.png"></p>
<p><em>2、</em>find 函数的执行过程</p>
<p>对 3 调用 find 函数</p>
<pre><code>搜索 trance[3],
由于 trance[3] = 2,
搜索 trance[2],
由于 trance[2] = 1,
搜索 trance[1],
由于 trance[1] = 0,
搜索 trance[0],
由于 trance[0] = -1,
此时返回 0</code></pre>
<p>对 5 调用 find 函数</p>
<pre><code>搜索 trance[5],
由于 trance[5] = 4,
搜索 trance[4],
由于 trance[4] = 1,
此时返回 4</code></pre>
<p>这就是整个 find 的函数的执行流程</p>
<p><strong>2、union 函数的作用</strong></p>
<p>union 函数非常简单，传入两个参数 root1, root2</p>
<pre><code>让第一个参数作为数组 trance 的下标，让第二个参数作为第一个参数对应下标的值
例如 root1 = 3, root2 = 5
则有 trance[3] = 5

例如 例如 root1 = 0, root2 = 2
则 trance[0] = 2</code></pre>
<p><strong>3、如何判断是否存在回路</strong></p>
<p>存在一条边  side，该边有两个顶点 a，b(find 函数的详细过程请看上面 <strong>2、find 函数的作用</strong>)</p>
<p>若</p>
<pre><code>使用 a 调用 find 函数返回 num1 &gt;= 0
使用 b 调用 find 函数返回 num2 &gt;= 0
当 num1 与 num2 相等时，证明若把 side 边纳入到 trance 数组中时，
会出现回路，因此此时不要将 side 边纳入到 trance 数组中</code></pre>
<p><em>示例：</em></p>
<p>此时 trance 数组为图示状态，先不用计较该数组是如何得到的，</p>
<p>只需要知道如何判断回路就可以，下边会介绍如何构建 trance 数组</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/18-59-39-0ac970c02df0b3b774112fa1f9afbfde-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122185914-284af2.png"></p>
<p>存在一条边 side，该边有两个顶点 3，5</p>
<pre><code>使用 2 调用 find 函数 trance[3] = 2
                     trance[2] = 1
                     trance[1] = 0
                     trance[0] = 4
                     trance[4] = -1
                     所以 num1 = 4 &gt;= 0

使用 5 调用 find 函数 trance[5] = 4
                     trance[4] = -1
                     所以 num2 = 4 &gt;= 0
                    由于 num1 = num2,所以在该有无图中存在回路
                    不会将边 side 纳入到 trance 数组中</code></pre>
<h2 id="2、使用并查集判断下图所示的无向图中是否存在回路"><a href="#2、使用并查集判断下图所示的无向图中是否存在回路" class="headerlink" title="2、使用并查集判断下图所示的无向图中是否存在回路"></a>2、使用并查集判断下图所示的无向图中是否存在回路</h2><p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/21/16-14-08-973fd7b073b8a90c47a1b0c185c5addf-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210118200641-15e9b8.png"></p>
<p><em>下面将通过实例讲解 trance 数组是如何构建的</em></p>
<p><strong>1、初始化</strong></p>
<p>由于在图示中存在 4 个顶点，所以初始化一个长度为 4 的数组，数组记为 trance，</p>
<p>并初始化为 -1</p>
<p>数组的初始状态如图所示</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/19-20-23-f763a0d4ed8b88879304c1a2656221b2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122192012_1-0cd5df.png"></p>
<p><strong>2、将边 a 纳入到 trance 数组</strong></p>
<p>此时数组 trance 的状态如图</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/19-20-23-f763a0d4ed8b88879304c1a2656221b2-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122192012_1-0cd5df.png"></p>
<pre><code>边 a 有 0， 1 顶点
对 0 调用 find 函数    tranc[0] = -1
                    所以 num1 = 0 &gt;= 0

对 1 调用 find 函数    tranc[1] = -1
                    所以 num2 = 1 &gt;= 0

此时使用 num1 和 num2 调用 union 函数 (这里可以看 2、union 函数的作用 上方黑体字部分)
                    有 trance[0] = 1</code></pre>
<p><strong>2、将边 b 纳入到 trance 数组</strong></p>
<p>此时数组 trance 的状态如图</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/19-25-48-8bead02351c0bcceb3d024ee7d908a71-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122192540_2-718ed1.png"></p>
<pre><code>边 b 有 0， 2 顶点
对 0 调用 find 函数    tranc[0] = 1
                    tranc[1] = -1
                    所以 num1 = 1 &gt;= 0

对 2 调用 find 函数 tranc[2] = -1
                    所以 num2 = 2 &gt;= 0

此时使用 num1 和 num2 调用 union 函数
                    有 trance[1] = 2</code></pre>
<p><strong>3、将边 c 纳入到 trance 数组</strong></p>
<p>此时数组 trance 的状态如图</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/19-44-24-f50b59405fefcfe8783fc42be5a83c21-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122193619_3-d5651c.png"></p>
<pre><code>边 c 有 0， 3 顶点
对 0 调用 find 函数    tranc[0] = 1
                    tranc[1] = 2
                    trance[2] = -1
                    所以 num1 = 2 &gt;= 0

对 3 调用 find 函数 tranc[3] = -1
                    所以 num2 = 3 &gt;= 0

此时使用 num1 和 num2 调用 union 函数
                    有 trance[2] = 3</code></pre>
<p><strong>4、将边 d 纳入到 trance 数组</strong></p>
<p>此时数组 trance 的状态如图</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/19-49-25-7bfd5acf78cff097307e28014a4e08da-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122194910_4-be3740.png"></p>
<pre><code>边 d 有 1， 2 顶点
对 1 调用 find 函数    tranc[1] = 2
                    tranc[2] = 3
                    trance[3] = -1
                    所以 num1 = 3 &gt;= 0

对 2 调用 find 函数    tranc[2] = 3
                    tranc[3] = -1
                    所以 num2 = 3 &gt;= 0

此时 num1 = num2 存在回路，就不向 trance 中纳入边 d，此时 trance 数组不发生变化</code></pre>
<p><strong>5、将边 e 纳入到 trance 数组</strong></p>
<p>此时数组 trance 的状态如图</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/19-49-25-7bfd5acf78cff097307e28014a4e08da-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122194910_4-be3740.png"></p>
<pre><code>边 e 有 1， 3 顶点
对 1 调用 find 函数    tranc[1] = 2
                    tranc[2] = 3
                    trance[3] = -1
                    所以 num1 = 3 &gt;= 0

对 3 调用 find 函数    tranc[3] = -1
                    所以 num2 = 3 &gt;= 0

此时 num1 = num2 存在回路，就不向 trance 中纳入边 e，此时 trance 数组不发生变化</code></pre>
<p><strong>6、将边 f 纳入到 trance 数组</strong></p>
<p>此时数组 trance 的状态如图</p>
<p><img src="https://cdn.jsdelivr.net/gh/clay-nuyoah/ImageHosting@master/img/clay-nuyoah/source/post-images/bing-cha-ji/2021/01/22/19-49-25-7bfd5acf78cff097307e28014a4e08da-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20210122194910_4-be3740.png"></p>
<pre><code>边 f 有 2， 3 顶点
对 2 调用 find 函数    tranc[2] = 3
                    trance[3] = -1
                    所以 num1 = 3 &gt;= 0

对 3 调用 find 函数    tranc[3] = -1
                    所以 num2 = 3 &gt;= 0

此时 num1 = num2 存在回路，就不向 trance 中纳入边 e，此时 trance 数组不发生变化</code></pre>
<p>四、完整代码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package other;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.Arrays;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * 并查集</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class UnionFindSet &#123;</span><br><span class="line"></span><br><span class="line">    public static void main(String[] args) &#123;</span><br><span class="line">        UnionFindSet unionFindSet &#x3D; new UnionFindSet();</span><br><span class="line">        int[] trance &#x3D; new int[4];</span><br><span class="line">        List&lt;String&gt; list &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">        list.add(&quot;0-1&quot;);</span><br><span class="line">        list.add(&quot;0-2&quot;);</span><br><span class="line">        list.add(&quot;0-3&quot;);</span><br><span class="line">        list.add(&quot;1-2&quot;);</span><br><span class="line">        list.add(&quot;1-3&quot;);</span><br><span class="line">        list.add(&quot;2-3&quot;);</span><br><span class="line"></span><br><span class="line">        for (int i &#x3D; 0; i &lt; trance.length; i++) &#123;</span><br><span class="line">            trance[i] &#x3D; -1;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        for (String s : list) &#123;</span><br><span class="line">            System.out.println(Arrays.toString(trance));</span><br><span class="line">            String[] split &#x3D; s.split(&quot;-&quot;);</span><br><span class="line">            int find1 &#x3D; unionFindSet.find(Integer.parseInt(split[0]), trance);</span><br><span class="line">            int find2 &#x3D; unionFindSet.find(Integer.parseInt(split[1]), trance);</span><br><span class="line">            if (find1 !&#x3D; find2) &#123;</span><br><span class="line">                unionFindSet.union(find1, find2, trance);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public int find(int x, int[] trance) &#123;</span><br><span class="line">        if (trance[x] &lt; 0)</span><br><span class="line">            return x;</span><br><span class="line">        else</span><br><span class="line">            return find(trance[x], trance);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    public void union(int root1, int root2, int[] trance) &#123;</span><br><span class="line">        trance[root1] &#x3D; root2;&#x2F;&#x2F;将root1作为root2的新树根</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>算法</tag>
        <tag>并查集</tag>
        <tag>查找无向图回路</tag>
      </tags>
  </entry>
  <entry>
    <title>最后一块石头的重量 leetcode</title>
    <url>/2021/01/11/zui-hou-yi-kuai-shi-tou-de-zhong-liang/</url>
    <content><![CDATA[<p>leetcode 题库 1046 最后一块石头的重量</p>
<p>使用了二分查找和插入排序的方法</p>
<a id="more"></a>

<h1 id="一、题目描述："><a href="#一、题目描述：" class="headerlink" title="一、题目描述："></a>一、题目描述：</h1><p>有一堆石头，每块石头的重量都是正整数。</p>
<p>每一回合，从中选出两块 最重的 石头，然后将它们一起粉碎。</p>
<p>假设石头的重量分别为 x 和 y，且 x &lt;= y。那么粉碎的可能结果如下：</p>
<p>如果 x == y，那么两块石头都会被完全粉碎；</p>
<p>如果 x != y，那么重量为 x 的石头将会完全粉碎，而重量为 y 的石头新重量为 y-x。</p>
<p>最后，最多只会剩下一块石头。返回此石头的重量。如果没有石头剩下，就返回 0。</p>
<p>来源：力扣（LeetCode）</p>
<p>链接：<a href="https://leetcode-cn.com/problems/last-stone-weight">https://leetcode-cn.com/problems/last-stone-weight</a></p>
<p>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<h1 id="二、解题思路"><a href="#二、解题思路" class="headerlink" title="二、解题思路"></a>二、解题思路</h1><p><strong>一、初始化</strong></p>
<pre><code>对数组中的元素进行排序;

这样做有以下两个好处：

1、能从数组的末尾取出最重的两块石头

2、取出两块最重的石头之后，剩余的石头仍然保持有序，

可以使用 二分查找 找到 新石头 在数组中插入的位置。</code></pre>
<p>关于如何进行二分查找，这里有篇非常好的讲解，对二分查找，寻找左侧边界的二分查找，</p>
<p>寻找右侧边界的二分查找都做了详细的介绍</p>
<p>地址：<a href="https://leetcode-cn.com/problems/binary-search/solution/er-fen-cha-zhao-xiang-jie-by-labuladong/">https://leetcode-cn.com/problems/binary-search/solution/er-fen-cha-zhao-xiang-jie-by-labuladong/</a></p>
<p><strong>二、当两块最重的石头重量相同时</strong></p>
<p>设保存石头重量的数组名称为 stones    </p>
<pre><code>1、将两块石头同时粉碎，但是并不会产生新的石头

2、创建新数组 newStones 用来保存新的数据(新数组的长度为 stones.length - 2)

3、将 stones 前 stones.length - 2 的数据依次复制到新数组 newStones 中</code></pre>
<p><strong>三、当两块石头单位重量不同时</strong></p>
<pre><code>1、在数组 stones 取出两块最重的石头

2、将两块石头粉碎，并形成新的石头 newStone

3、使用新石头 newStone 在数组 Stones 中查找新石头需要插入的的位置 i

（查找插入位置时需要排除掉最后的两块石头，即返回 i 的范围为 0 &lt;= i &lt; stones.length - 2）

4、创建新数组 newStones 用来保存新的数据(新数组的长度为 stones.length - 1)

5、将数组 stones i 之前的数据依次插入到 newStones 数组 i 之前的位置

5、将 newStone 插入到 newStones 的 i 位置

6、将 stones i 位置和 i 之后的数据插入依次插入到 newStones i 之后的位置</code></pre>
<p><strong>四、结果返回</strong></p>
<p><em>当数组的长度为 0 或者是为 1 时返回结果</em></p>
<pre><code>当数组长度为 0 时返回 0

当数组的长度为 1 时，返回数组中的第一个元素</code></pre>
<h1 id="三、代码如下"><a href="#三、代码如下" class="headerlink" title="三、代码如下"></a>三、代码如下</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public int lastStoneWeight(int[] stones) &#123;</span><br><span class="line">       &#x2F;&#x2F;用来保存新的数据</span><br><span class="line">       int[] newStones &#x3D; stones;</span><br><span class="line">       Arrays.sort(newStones);</span><br><span class="line">       for (; newStones.length &gt; 1; ) &#123;</span><br><span class="line">           int max_1 &#x3D; newStones[newStones.length - 1];</span><br><span class="line">           int max_2 &#x3D; newStones[newStones.length - 2];</span><br><span class="line">           int t &#x3D; max_1 - max_2;</span><br><span class="line">           int[] original &#x3D; newStones;</span><br><span class="line">           if (t &#x3D;&#x3D; 0) &#123;</span><br><span class="line">               &#x2F;&#x2F;每次都会创建一个新数组</span><br><span class="line">               newStones &#x3D; new int[newStones.length - 2];</span><br><span class="line">               &#x2F;&#x2F;将 stones 前 stones.length - 2 的数据依次复制到新数组 newStones 中</span><br><span class="line">               copyArray(original, 0, newStones, 0, newStones.length);</span><br><span class="line">           &#125; else &#123;</span><br><span class="line">               &#x2F;&#x2F;每次都会创建一个新数组</span><br><span class="line">               newStones &#x3D; new int[newStones.length - 1];</span><br><span class="line">			&#x2F;&#x2F;排除掉最后的两块石头</span><br><span class="line">               int index &#x3D; leftBoundBinarySearch(original, t, 0, original.length - 2);</span><br><span class="line">               &#x2F;&#x2F;original 0 ~ index - 1   arr 0 ~ index - 1</span><br><span class="line">               &#x2F;&#x2F;将数组 stones i 之前的数据依次插入到 newStones 数组 i 之前的位置</span><br><span class="line">               copyArray(original, 0, newStones, 0, index);</span><br><span class="line">               &#x2F;&#x2F;arr index</span><br><span class="line">               &#x2F;&#x2F;将 newStone 插入到 newStones 的 i 位置</span><br><span class="line">               newStones[index] &#x3D; t;</span><br><span class="line">               &#x2F;&#x2F;将 stones i 之后的数据插入依次插入到 newStones i 之后的位置</span><br><span class="line">               copyArray(original, index, newStones, index + 1, newStones.length - index - 1);</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return newStones.length &#x3D;&#x3D; 0 ? 0 : newStones[0];</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   &#x2F;**</span><br><span class="line">    * @param originalArr   数据来源的数组</span><br><span class="line">    * @param originalIndex 从数据来源的那个下标开始取值</span><br><span class="line">    * @param targetArr     复制的目标数组</span><br><span class="line">    * @param targetIndex   从目标数组的哪个下标开始赋值</span><br><span class="line">    * @param len           需要赋值的长度</span><br><span class="line">    *&#x2F;</span><br><span class="line">   private void copyArray(int[] originalArr, int originalIndex, int[] targetArr, int targetIndex, int len) &#123;</span><br><span class="line">       for (int i &#x3D; 0; i &lt; len; i++) &#123;</span><br><span class="line">           targetArr[targetIndex + i] &#x3D; originalArr[originalIndex + i];</span><br><span class="line">       &#125;</span><br><span class="line">   &#125;</span><br><span class="line"></span><br><span class="line">   &#x2F;**</span><br><span class="line">    * 寻找左侧边界的二分搜索</span><br><span class="line">    *</span><br><span class="line">    * @param nums</span><br><span class="line">    * @param target</span><br><span class="line">    * @return</span><br><span class="line">    *&#x2F;</span><br><span class="line">   int leftBoundBinarySearch(int[] nums, int target, int l, int r) &#123;</span><br><span class="line">       if (nums.length &#x3D;&#x3D; 0) return -1;</span><br><span class="line">       int left &#x3D; l;</span><br><span class="line">       int right &#x3D; r; &#x2F;&#x2F; 注意</span><br><span class="line"></span><br><span class="line">       while (left &lt; right) &#123; &#x2F;&#x2F; 注意</span><br><span class="line">           int mid &#x3D; (left + right) &#x2F; 2;</span><br><span class="line">           if (nums[mid] &#x3D;&#x3D; target) &#123;</span><br><span class="line">               right &#x3D; mid;</span><br><span class="line">           &#125; else if (nums[mid] &lt; target) &#123;</span><br><span class="line">               left &#x3D; mid + 1;</span><br><span class="line">           &#125; else if (nums[mid] &gt; target) &#123;</span><br><span class="line">               right &#x3D; mid; &#x2F;&#x2F; 注意</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return left;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>leetcode</tag>
        <tag>二分查找</tag>
        <tag>插入排序</tag>
      </tags>
  </entry>
  <entry>
    <title>多数元素 leetcode</title>
    <url>/2021/01/10/duo-shu-yuan-su-leetcode/</url>
    <content><![CDATA[<p>leetcode 题库 剑指 Offer 39 数组中出现次数超过一半的数字</p>
<p>使用 摩尔投票法 解决众数问题</p>
<a id="more"></a>

<h1 id="一、题目描述"><a href="#一、题目描述" class="headerlink" title="一、题目描述"></a>一、题目描述</h1><p>数组中有一个数字出现的次数超过数组长度的一半，请找出这个数字。</p>
<p>你可以假设数组是非空的，并且给定的数组总是存在多数元素。</p>
<p>示例 1:</p>
<p>输入: [1, 2, 3, 2, 2, 2, 5, 4, 2]</p>
<p>输出: 2</p>
<p>来源：力扣（LeetCode）</p>
<p>链接：<a href="https://leetcode-cn.com/problems/shu-zu-zhong-chu-xian-ci-shu-chao-guo-yi-ban-de-shu-zi-lcof">https://leetcode-cn.com/problems/shu-zu-zhong-chu-xian-ci-shu-chao-guo-yi-ban-de-shu-zi-lcof</a></p>
<p>著作权归领扣网络所有。商业转载请联系官方授权，非商业转载请注明出处。</p>
<p>可以换做下面的方式来理解本体：</p>
<h1 id="二、诸侯争霸游戏："><a href="#二、诸侯争霸游戏：" class="headerlink" title="二、诸侯争霸游戏："></a>二、诸侯争霸游戏：</h1><p><strong>游戏规则设定：</strong></p>
<pre><code>1、m 个国家进行诸侯争霸；

2、每个国家的士兵人数为 a1,a2……，am,总士兵人数为 a1 + a2 + a3 …… + am 记为 C；

3、其中存在一个国家 i 的士兵人数超过总士兵人数的一半，即 ai &gt; C/2；

4、m 个国家随机的往战场上面投放士兵，并且每个国家每次只能投放一个士兵；

5、只要战场上存在两个不同国家的士兵，那么就会相会搏杀，两个士兵同归于尽；

6、所有的国家都要出动所有的士兵参战，中途不能退出，直至士兵消耗完毕；

7、最后的胜利者属于最后能够生存下来的国家；

则最终的胜利一定属于第 i 个国家，这个国家就是那个士兵数量超过总士兵人数的一半的国家；</code></pre>
<h1 id="三、代码如下："><a href="#三、代码如下：" class="headerlink" title="三、代码如下："></a>三、代码如下：</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public int majorityElement(int[] nums) &#123;</span><br><span class="line">    int mode &#x3D; 0;&#x2F;&#x2F;士兵数量暂时领先的国家</span><br><span class="line">    int count &#x3D; 0;&#x2F;&#x2F;士兵数量暂时领先的数量</span><br><span class="line">    for (int num : nums) &#123;      &#x2F;&#x2F;每个国家都随机的往战场中投放士兵</span><br><span class="line">        if (count &#x3D;&#x3D; 0) &#123;       &#x2F;&#x2F;若战场中不存在士兵</span><br><span class="line">            mode &#x3D; num;         &#x2F;&#x2F;投放士兵的国家就变成了  士兵数量暂时领先的国家</span><br><span class="line">        &#125;</span><br><span class="line">        if (mode &#x3D;&#x3D; num) &#123;      &#x2F;&#x2F;如果投放的士兵和战场中的士兵来自相同的国家</span><br><span class="line">            count++;            &#x2F;&#x2F;士兵数量就会 + 1</span><br><span class="line">        &#125; else &#123;                &#x2F;&#x2F;如果投放的士兵和战场中的士兵来自不同的国家</span><br><span class="line">            count--;            &#x2F;&#x2F;两个士兵同归于尽 这是会消耗掉 1 个 暂时领先的国家的士兵</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    &#x2F;&#x2F;最后可以生存下来的国家</span><br><span class="line">    return mode;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>算法</tag>
        <tag>leetcode</tag>
        <tag>摩尔投票法</tag>
        <tag>众数</tag>
      </tags>
  </entry>
  <entry>
    <title>玩筹码 leetcode</title>
    <url>/2020/11/30/wan-chou-ma-leetcode/</url>
    <content><![CDATA[<p>leetcode 题库 1217 玩筹码</p>
<a id="more"></a>

<h1 id="一、题目描述："><a href="#一、题目描述：" class="headerlink" title="一、题目描述："></a>一、题目描述：</h1><p>数轴上放置了一些筹码，每个筹码的位置存在数组 chips 当中。</p>
<p>你可以对 任何筹码 执行下面两种操作之一（不限操作次数，0 次也可以）：</p>
<p>将第 i 个筹码向左或者右移动 2 个单位，代价为 0。</p>
<p>将第 i 个筹码向左或者右移动 1 个单位，代价为 1。</p>
<p>最开始的时候，同一位置上也可能放着两个或者更多的筹码。</p>
<p>返回将所有筹码移动到同一位置（任意位置）上所需要的最小代价。</p>
<p>示例 1：</p>
<p>输入：chips = [1,2,3]</p>
<p>输出：1</p>
<p>解释：第二个筹码移动到位置三的代价是 1，第一个筹码移动到位置三的代价是 0，总代价为 1。</p>
<p>示例 2：</p>
<p>输入：chips = [2,2,2,3,3]</p>
<p>输出：2</p>
<p>解释：第四和第五个筹码移动到位置二的代价都是 1，所以最小总代价为 2。</p>
<p>来源：力扣（LeetCode）</p>
<p>链接：<a href="https://leetcode-cn.com/problems/minimum-cost-to-move-chips-to-the-same-position">https://leetcode-cn.com/problems/minimum-cost-to-move-chips-to-the-same-position</a></p>
<h1 id="二、解题思路"><a href="#二、解题思路" class="headerlink" title="二、解题思路"></a>二、解题思路</h1><p>我们任意挑选两个相邻的位置，两个位置必然会有一个偶数位置(记为 a)，</p>
<p>一个奇数位置(记为b),由于移动偶数个位置的代价为 0，</p>
<p>所以把所有在偶数位置的筹码移动到 a 点的代价和为 0</p>
<p>(每一个筹码都可以通过移动偶数个单位移动到 a 点),</p>
<p>同样把所有在奇数位置的筹码移动到 b 点的代价和也为 0,</p>
<p>这时所有的筹码都集中到了 a b 两点，</p>
<p>那么将 筹码最少的点的所有筹码 移动到 筹码最多的点 的代价就是所求的最小代价</p>
<p>算法实现是直接求偶数位置的个数和奇数位置的个数，然后返回个数少的数目就行了</p>
<h1 id="三、代码"><a href="#三、代码" class="headerlink" title="三、代码"></a>三、代码</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">	public int minCostToMoveChips(int[] positions) &#123;</span><br><span class="line">	        int eventCount &#x3D; 0;</span><br><span class="line">	        int addCount &#x3D; 0;</span><br><span class="line">	        for (int position : positions) &#123;</span><br><span class="line">	            if (position % 2 &#x3D;&#x3D; 0) &#123;</span><br><span class="line">	                eventCount++;</span><br><span class="line">	            &#125; else &#123;</span><br><span class="line">	                addCount++;</span><br><span class="line">	            &#125;</span><br><span class="line">	        &#125;</span><br><span class="line">	        return Math.min(eventCount, addCount);</span><br><span class="line">	    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
  </entry>
  <entry>
    <title>TSP旅行推销员问题</title>
    <url>/2020/11/25/tsp-lu-xing-tui-xiao-yuan-wen-ti/</url>
    <content><![CDATA[<p>使用动态规划的方式求解TSP旅行推销员问题</p>
<a id="more"></a>
<h1 id="一、问题描述："><a href="#一、问题描述：" class="headerlink" title="一、问题描述："></a>一、问题描述：</h1><p>给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。</p>
<h1 id="二、例题"><a href="#二、例题" class="headerlink" title="二、例题"></a>二、例题</h1><p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/o_image_2.png"></p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;图 1    </p>
<p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20201125211440.png"><br>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;图 2</p>
<p>有向图图1可以用图2的矩阵进行表示。</p>
<p>现在要求从城市 0 出发，访问城市 1，城市 2，城市 3 （需要访遍每一座城市），</p>
<p>最后回到城市 0 的最短路径</p>
<h1 id="三、基础知识讲解"><a href="#三、基础知识讲解" class="headerlink" title="三、基础知识讲解"></a>三、基础知识讲解</h1><h2 id="1、首先定义一项规则"><a href="#1、首先定义一项规则" class="headerlink" title="1、首先定义一项规则"></a>1、首先定义一项规则</h2><p>i -&gt; V’ -&gt; 0 = num    ①</p>
<p>可以记为 d(i,V’) = num ②</p>
<p>其中 V’ = {i1, i2, …, ik} k = 1,2,3,…,n 表示需要经过的点集</p>
<p> ① 表示，从点 i 出发，经果点集 V’，最后到达起始点 0 的最短路径为 num </p>
<p> ②是简洁的表示形式</p>
<p>如 3 -&gt; {2, 3} -&gt; 0    10</p>
<p>所代表的意思是，从点 3 出发，经过点 2，点3 最后到达起始点 0 的最短路径为 10</p>
<p>也可以表示为 d(3,{2,3}) = 10</p>
<h1 id="四、例题分析"><a href="#四、例题分析" class="headerlink" title="四、例题分析"></a>四、例题分析</h1><p><strong>1、该题需要进行逆过程思考, 接下来将会对下列的过程进行讨论：</strong></p>
<pre><code>**1、不经过任何点集**

    1 -&gt; &#123;&#125; -&gt; 0    记为 d(1,&#123;&#125;)    (4.1.1)
    2 -&gt; &#123;&#125; -&gt; 0    记为 d(2,&#123;&#125;)    (4.1.2)
    3 -&gt; &#123;&#125; -&gt; 0    记为 d(3,&#123;&#125;)    (4.1.3)

**2、经过 1 条点集**

    2 -&gt; &#123;1&#125; -&gt; 0    记为 d(2,&#123;1&#125;)    (4.1.4)
    3 -&gt; &#123;1&#125; -&gt; 0    记为 d(3,&#123;1&#125;)    (4.1.5)

    1 -&gt; &#123;2&#125; -&gt; 0    记为 d(1,&#123;2&#125;)    (4.1.6)
    3 -&gt; &#123;2&#125; -&gt; 0    记为 d(3,&#123;2&#125;)    (4.1.7)

    1 -&gt; &#123;3&#125; -&gt; 0    记为 d(1,&#123;3&#125;)    (4.1.8)
    2 -&gt; &#123;3&#125; -&gt; 0    记为 d(2,&#123;3&#125;)    (4.1.9)

**3、经过 2 条点集**

    1 -&gt; &#123;2, 3&#125; -&gt; 0    记为 d(1,&#123;2, 3&#125;)    (4.1.10)
    2 -&gt; &#123;1, 3&#125; -&gt; 0    记为 d(2,&#123;1, 3&#125;)    (4.1.11)
    3 -&gt; &#123;1, 2&#125; -&gt; 0    记为 d(3,&#123;1, 2&#125;)    (4.1.12)

**4、经过 3 条点集**

    0 -&gt; &#123;1, 2, 3&#125; -&gt; 0    (4.1.13) d(0,&#123;1, 2, 3&#125;) (4.1.13)</code></pre>
<p><strong>2、对 1 内容进行解释</strong></p>
<pre><code>由于最终要回到起点 0 点，在我们不经过任何点集时，有 1 点，2 点，3 点，可以回到 0 点，

所以有如下解释

(4.1.1) 表示，从 1 点不经过任何点集，到达起始点 0 点；
(4.1.2) 表示，从 2 点不经过任何点集，到达起始点 0 点；
(4.1.3) 表示，从 3 点不经过任何点集，到达起始点 0 点；

(4.1.4) 表示，从 2 点经过点集 1，到达起始点 0 点；
(4.1.5) 表示，从 3 点经过点集 1，到达起始点 0 点；

(4.1.6) 表示，从 1 点经过点集 2，到达起始点 0 点；
(4.1.7) 表示，从 3 点经过点集 2，到达起始点 0 点；

(4.1.8) 表示，从 1 点经过点集 3，到达起始点 0 点；
(4.1.9) 表示，从 2 点经过点集 3，到达起始点 0 点；

(4.1.10) 表示，从 1 点经过点集 2，3，到达起始点 0 点；
(4.1.11) 表示，从 2 点经过点集 1，3，到达起始点 0 点；
(4.1.12) 表示，从 3 点经过点集 1，2，到达起始点 0 点；

(4.1.13) 表示，从起始 0 点经过点集1， 2，3，到达起始点 0 点；</code></pre>
<p>显然 (4.1.13) 是我们需要求解的结果,</p>
<p>现在还不适合求解最短路径，因此没有给出任何 ① 中提到的 num 的信息</p>
<p><strong>3、对(4.1.x)的数据进行分解</strong></p>
<pre><code>**1、不经过任何点集**

    1 -&gt; &#123;&#125; -&gt; 0    (4.1.1)
    2 -&gt; &#123;&#125; -&gt; 0    (4.1.2)
    3 -&gt; &#123;&#125; -&gt; 0    (4.1.3)

**2、经过 1 条点集**

    2 -&gt; &#123;1&#125; -&gt; 0    (4.1.4)
        2 -&gt; 1 -&gt; &#123;&#125; -&gt; 0    (4.1.4.1)
    3 -&gt; &#123;1&#125; -&gt; 0    (4.1.5)
        3 -&gt; 1 -&gt; &#123;&#125; -&gt; 0    (4.1.5.1)

    1 -&gt; &#123;2&#125; -&gt; 0    (4.1.6)
        1 -&gt; 2 -&gt; &#123;&#125; -&gt; 0    (4.1.6.1)
    3 -&gt; &#123;2&#125; -&gt; 0    (4.1.7)
        3 -&gt; 2 -&gt; &#123;&#125; -&gt; 0    (4.1.7.1)

    1 -&gt; &#123;3&#125; -&gt; 0    (4.1.8)
        1 -&gt; 3 -&gt; &#123;&#125; -&gt; 0    (4.1.8.1)
    2 -&gt; &#123;3&#125; -&gt; 0    (4.1.9)
        2 -&gt; 3 -&gt; &#123;&#125; -&gt; 0    (4.1.9.1)

**3、经过 2 条点集**

    1 -&gt; &#123;2, 3&#125; -&gt; 0    (4.1.10)
        1 -&gt; 2 -&gt; &#123;3&#125; -&gt; 0    (4.1.10.1)
        1 -&gt; 3 -&gt; &#123;2&#125; -&gt; 0     (4.1.10.2)

    2 -&gt; &#123;1, 3&#125; -&gt; 0    (4.1.11)
        2 -&gt; 1 -&gt; &#123;3&#125; -&gt; 0    (4.1.11.1)
        2 -&gt; 3 -&gt; &#123;1&#125; -&gt; 0    (4.1.11.2)

    3 -&gt; &#123;1, 2&#125; -&gt; 0    (4.1.12)
        3 -&gt; 1 -&gt; &#123;2&#125; -&gt; 0    (4.1.12.1)
        3 -&gt; 2 -&gt; &#123;1&#125; -&gt; 0    (4.1.12.2)

**4、经过 3 条点集**

    0 -&gt; &#123;1, 2, 3&#125; -&gt; 0    (4.1.13)
        0 -&gt; 1 -&gt; &#123;2,3&#125; -&gt; 0    (4.1.13.1)
        0 -&gt; 2 -&gt; &#123;1,3&#125; -&gt; 0    (4.1.13.2)
        0 -&gt; 3 -&gt; &#123;1,2&#125; -&gt; 0    (4.1.13.3)</code></pre>
<p><strong>4、数据分解带来的好处</strong></p>
<p>为什么要对上述数据进行分解呢，因为分解之后我们就可以利用上面已经产生的数据，简化计算。</p>
<p>有了 3 的分解过程，我们就可以求最短路径了，</p>
<p>由图 2 可知，C 表示各个点的路径长度，即 C[i][j] 表示从 i 点到达 j 点的路径长度。</p>
<p>由 ② 可知，d(i,V’) 表示，从点 i 出发，经过点集 V’ 到达零的最短路径</p>
<p>现在对上述的分解过程进行第一部分讲解,<br>    第一部<br>        <strong>1、不经过任何点集</strong></p>
<pre><code>    1 -&gt; &#123;&#125; -&gt; 0    (4.1.1)
    2 -&gt; &#123;&#125; -&gt; 0    (4.1.2)
    3 -&gt; &#123;&#125; -&gt; 0    (4.1.3)</code></pre>
<p>对于这三项不用计算，直接求最短路径</p>
<pre><code>(4.1.1) 表示，从 1 点不经过任何点集，到达起始点 0 点，即 1 到 0 点的距离，可以用矩阵
C[1][0] = 5
(4.1.2) 表示，从 2 点不经过任何点集，到达起始点 0 点,即 2 到 0 点的距离，可以用矩阵
C[2][0] = 6
(4.1.3) 表示，从 3 点不经过任何点集，到达起始点 0 点，即 3 到 0 点的距离，可以用矩阵
C[2][0] = 3</code></pre>
<p>整理得，</p>
<pre><code>**1、不经过任何点集**

1 -&gt; &#123;&#125; -&gt; 0    记为 d(1,&#123;&#125;) = 5        (4.4.1)
2 -&gt; &#123;&#125; -&gt; 0    记为 d(2,&#123;&#125;) = 6        (4.4.2)
3 -&gt; &#123;&#125; -&gt; 0    记为 d(3,&#123;&#125;) = 3        (4.4.3)</code></pre>
<p>进行第二部分的讲解</p>
<pre><code>第二部分
    **2、经过 1 条点集**

        2 -&gt; &#123;1&#125; -&gt; 0    (4.1.4)
            2 -&gt; 1 -&gt; &#123;&#125; -&gt; 0    (4.1.4.1)
        3 -&gt; &#123;1&#125; -&gt; 0    (4.1.5)
            3 -&gt; 1 -&gt; &#123;&#125; -&gt; 0    (4.1.5.1)

        1 -&gt; &#123;2&#125; -&gt; 0    (4.1.6)
            1 -&gt; 2 -&gt; &#123;&#125; -&gt; 0    (4.1.6.1)
        3 -&gt; &#123;2&#125; -&gt; 0    (4.1.7)
            3 -&gt; 2 -&gt; &#123;&#125; -&gt; 0    (4.1.7.1)

        1 -&gt; &#123;3&#125; -&gt; 0    (4.1.8)
            1 -&gt; 3 -&gt; &#123;&#125; -&gt; 0    (4.1.8.1)
        2 -&gt; &#123;3&#125; -&gt; 0    (4.1.9)
            2 -&gt; 3 -&gt; &#123;&#125; -&gt; 0    (4.1.9.1)</code></pre>
<p>对于 (4.1.4.1) 可以看成两部分，第一部分时 2 -&gt; 1，第二部分是 1 -&gt; {} -&gt; 0，</p>
<p>即先从点 2 出发然后经过点 1，然后经过空点集，到达 0 点</p>
<p>第一部分为矩阵 C[2][1] = 4 的值</p>
<p>而第二部分也已经求解，即 (4.4.1) 的结果 d(1,{}) = 5</p>
<p>所以 2 -&gt; {1} -&gt; 0 =&gt; 2 -&gt; 1 -&gt; {} -&gt; 0 =&gt; C[2][1] + d(1,{}) = 4 + 5 = 9</p>
<p>对于 (4.1.5.1) 可以看成两部分，第一部分时 3 -&gt; 1，第二部分是 1 -&gt; {} -&gt; 0，</p>
<p>即先从点 3 出发然后经过点 1，然后经过空点集，到达 0 点</p>
<p>第一部分为矩阵 C[3][1] = 7 的值</p>
<p>可以发现第二部分已经求解，即 (4.4.1) 的结果 d(1,{}) = 5</p>
<p>所以 3 -&gt; {1} -&gt; 0 =&gt; 3 -&gt; 1 -&gt; {} -&gt; 0 =&gt; C[3][1] + d(1,{}) = 7 + 5 = 12</p>
<p>同理，可得其他;</p>
<p>整理得，</p>
<pre><code>(4.1.4.1) : C[2][1] + d(1,&#123;&#125;) = 4 + 5 = 9    记为 d(2,&#123;1&#125;) = 9    (4.4.4)
(4.1.5.1) : C[3][1] + d(1,&#123;&#125;) = 7 + 5 = 12    记为 d(3,&#123;1&#125;) = 12    (4.4.5)
(4.1.6.1) : C[1][2] + d(2,&#123;&#125;) = 2 + 6 = 8    记为 d(1,&#123;2&#125;) = 8    (4.4.6)
(4.1.7.1) : C[3][2] + d(2,&#123;&#125;) = 5 + 6 = 11    记为 d(3,&#123;2&#125;) = 11    (4.4.7)
(4.1.8.1) : C[1][3] + d(3,&#123;&#125;) = 3 + 3 = 6    记为 d(1,&#123;3&#125;) = 6    (4.4.8)
(4.1.9.1) : C[2][3] + d(3,&#123;&#125;) = 2 + 3 = 5    记为 d(2,&#123;3&#125;) = 5    (4.4.9)</code></pre>
<p>进行第三部分得讲解</p>
<pre><code>第三部分
    **3、经过 2 条点集**

        1 -&gt; &#123;2, 3&#125; -&gt; 0    (4.1.10)
            1 -&gt; 2 -&gt; &#123;3&#125; -&gt; 0    (4.1.10.1)
            1 -&gt; 3 -&gt; &#123;2&#125; -&gt; 0     (4.1.10.2)

        2 -&gt; &#123;1, 3&#125; -&gt; 0    (4.1.11)
            2 -&gt; 1 -&gt; &#123;3&#125; -&gt; 0    (4.1.11.1)
            2 -&gt; 3 -&gt; &#123;1&#125; -&gt; 0    (4.1.11.2)

        3 -&gt; &#123;1, 2&#125; -&gt; 0    (4.1.12)
            3 -&gt; 1 -&gt; &#123;2&#125; -&gt; 0    (4.1.12.1)
            3 -&gt; 2 -&gt; &#123;1&#125; -&gt; 0    (4.1.12.2)</code></pre>
<p>1 -&gt; {2, 3} -&gt; 0 ，表示从 点 1 出发，经过点集 2 3，然后达到 0，得最短路径</p>
<p>此时我们有两种方式可以实现该过程，即上式得 (4.1.10.1) 和 (4.1.10.2)</p>
<p>(4.1.10.1) 表示先从点 1 出发，到达点 2，然后经过点集 3，最后到达起始点 0</p>
<p>(4.1.10.2) 表示先从点 1 出发，到达点 3，然后经过点集 2，最后到达起始点 0</p>
<p>因为要实现 1 -&gt; {2, 3} -&gt; 0 这个过程，有两种方式，而我们要求的是最短路径，</p>
<p>所以要求得 1 -&gt; {2, 3} -&gt; 0 最短路径为 1 -&gt; 2 -&gt; {3} -&gt; 0 和 1 -&gt; 3 -&gt; {2} -&gt; 0 得最小值</p>
<p>对于 (4.1.10.1) 可以分解为两个过程，第一个过程为 1 -&gt; 2，第二个过程为 2 -&gt; {3} -&gt; 0</p>
<p>即先从点 1 出发，经过点 2，在经过点集 3，最后到达起始点 0</p>
<p>第一部分为矩阵 C[1][2] = 2 的值</p>
<p>而第二部分也已经求解，即 (4.4.9) 的结果 d(2,{3}) = 5</p>
<p>所以 1 -&gt; 2 -&gt; {3} -&gt; 0 = C[1][2] + d(2,{3}) = 2 + 5 = 7 </p>
<p>对于 (4.1.10.1) 可以分解为两个过程，第一个过程为 1 -&gt; 3，第二个过程为 3 -&gt; {2} -&gt; 0</p>
<p>即先从点 1 出发，经过点 2，在经过点集 3，最后到达起始点 0</p>
<p>第一部分为矩阵 C[1][3] = 3 的值</p>
<p>而第二部分也已经求解，即 (4.4.7) 的结果 d(3,{2}) = 11</p>
<p>所以 1 -&gt; 3 -&gt; {2} -&gt; 0     = C[1][3] + d(3,{2}) = 3 + 11 = 14</p>
<p>所以 1 -&gt; {2, 3} -&gt; 0 需要取两者得最小值为 min(7, 14) = 7</p>
<p>同理，可得其他;</p>
<p>整理得，</p>
<pre><code>(4.1.10.1) : C[1][2] + d(2,&#123;3&#125;) = 2 + 5 = 7    
(4.1.10.2) : C[1][3] + d(3,&#123;2&#125;) = 3 + 11 = 14
(4.1.11.1) : C[2][1] + d(1,&#123;3&#125;) = 4 + 6 = 10
(4.1.11.2) : C[2][3] + d(3,&#123;1&#125;) = 2 + 12 = 14
(4.1.12.1) : C[3][1] + d(1,&#123;2&#125;) = 7 + 8 = 15
(4.1.12.2) : C[3][2] + d(2,&#123;1&#125;) = 5 + 9 = 14
取最小值得
(4.1.10) : min((4.1.10.1),(4.1.10.2)) = 7         记为 d(1,&#123;2,3&#125;) = 7        (4.4.10)
(4.1.11) : min((4.1.11.1),(4.1.11.2)) = 10        记为 d(2,&#123;1,3&#125;) = 10        (4.4.11)
(4.1.12) : min((4.1.12.1),(4.1.12.2)) = 14        记为 d(3,&#123;1,2&#125;) = 14        (4.4.12)</code></pre>
<p>进行第四部分的讲解</p>
<pre><code>第四部
    **4、经过 3 条点集**
        0 -&gt; &#123;1, 2, 3&#125; -&gt; 0    (4.1.13)
            0 -&gt; 1 -&gt; &#123;2,3&#125; -&gt; 0    (4.1.13.1)
            0 -&gt; 2 -&gt; &#123;1,3&#125; -&gt; 0    (4.1.13.2)
            0 -&gt; 3 -&gt; &#123;1,2&#125; -&gt; 0    (4.1.13.3)</code></pre>
<p>0 -&gt; {1, 2, 3} -&gt; 0，表示从点 0 出发，经过点集 1，2，3 最终回到出发点 0，</p>
<p>这就是我们要求的最短路径(因为 TSP 算法要求我们必须从 0 出发，经过多有得点，最后再回到 0 点)</p>
<p>此时我们有三种方式可以实现该过程，即上式得 (4.1.13.1),(4.1.13.2) 和 (4.1.13.3)</p>
<p>(4.1.13.1) 表示先从点 0 出发，到达点 1，然后经过点集 2 3，最后到达起始点 0；</p>
<p>(4.1.13.2) 表示先从点 0 出发，到达点 2，然后经过点集 1 3，最后到达起始点 0；</p>
<p>(4.1.13.2) 表示先从点 0 出发，到达点 3，然后经过点集 1 2，最后到达起始点 0；</p>
<p>因为要实现 0 -&gt; {1, 2, 3} -&gt; 0 这个过程，有三种方式，而我们要求的是最短路径，</p>
<p>所以要求得 0 -&gt; {1, 2, 3} -&gt; 0 最短路径为 </p>
<p>0 -&gt; 1 -&gt; {2,3} -&gt; 0，0 -&gt; 2 -&gt; {1,3} -&gt; 0 和 0 -&gt; 3 -&gt; {1,2} -&gt; 0 得最小值</p>
<p>对于 (4.1.13.1) 可以分解为两个过程，第一个过程为 0 -&gt; 1，第二个过程为 1 -&gt; {2，3} -&gt; 0</p>
<p>即先从点 0 出发，经过点 1，在经过点集 2 3，最后到达起始点 0</p>
<p>第一部分为矩阵 C[0][1] = 3 的值</p>
<p>而第二部分也已经求解，即 (4.4.10) 的结果 d(1,{2,3}) = 7</p>
<p>所以 1 -&gt; 2 -&gt; {3} -&gt; 0 = C[0][1] + d(1,{2,3}) = 3 + 7 = 10 </p>
<p>对于 (4.1.13.2) 可以分解为两个过程，第一个过程为 0 -&gt; 2，第二个过程为 2 -&gt; {1，3} -&gt; 0</p>
<p>即先从点 0 出发，经过点 2，在经过点集 1 3，最后到达起始点 0</p>
<p>第一部分为矩阵 C[0][2] = 6 的值</p>
<p>而第二部分也已经求解，即 (4.4.11) 的结果 d(2,{1,3}) = 10</p>
<p>所以 1 -&gt; 2 -&gt; {3} -&gt; 0 = C[0][2] + d(2,{1,3}) = 6 + 10 = 16</p>
<p>对于 (4.1.13.3) 可以分解为两个过程，第一个过程为 0 -&gt; 3，第二个过程为 3 -&gt; {1，2} -&gt; 0</p>
<p>即先从点 0 出发，经过点 3，在经过点集 1 2，最后到达起始点 0</p>
<p>第一部分为矩阵 C[0][3] = 7 的值</p>
<p>而第二部分也已经求解，即 (4.4.12) 的结果 d(3,{1,2}) = 14</p>
<p>所以 1 -&gt; 2 -&gt; {3} -&gt; 0 = C[0][3] + d(3,{1,2}) = 7 + 14 = 21</p>
<p>整理得，</p>
<pre><code>(4.1.13.1) : C[0][1] + d(1,&#123;2,3&#125;) = 3 + 7 = 10 
(4.1.13.2) : C[0][2] + d(2,&#123;1,3&#125;) = 6 + 10 = 16
(4.1.13.3) : C[0][3] + d(3,&#123;1,2&#125;) = 7 + 14 = 21
取最小值得
(4.1.13) ： min((4.1.13.1),(4.1.13.2),(4.1.13.3)) = 10 记为 d(0,&#123;1,2,3&#125;) = 10(4.4.13)</code></pre>
<p><strong>5、整理数据</strong></p>
<p>整理上方数据我们可以得到图 3 所示得表格，使用 dp 来表示这个二维数组</p>
<p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/o_image_thumb_1.png"></p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;<br>&emsp;&emsp;&emsp;&emsp;图 3</p>
<p>其中 d[i][j] 表示点 i 通过 j 所表示得点集，然后回到 0 所得到的最短路径</p>
<p>dp[1][6](两个下标都是从 0 开始) 其中 1 表示节点 1 ，6 表示点集{2，3}</p>
<p>dp[1][6] 表示从点 1 出发，经过点集 {2,3} 最后到达 0 求得的最短路径为 7</p>
<h1 id="五、分析过程存在的难点"><a href="#五、分析过程存在的难点" class="headerlink" title="五、分析过程存在的难点"></a>五、分析过程存在的难点</h1><p><strong>1、如何表示 j 所代表的数据</strong></p>
<p>j 所代表的数据即,{},{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3}</p>
<p>要想表示上边的点集，我们需要使用二进制形式表示，由于点集最多有三个点，</p>
<p>可以使用三位二进制来表示，具体表示方法为二进制从低位到高位依次表示 1 2 3，</p>
<p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/20201127113614.png"></p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;图 4</p>
<p>第 1 行第 2 列表示每位所代表的数字(行列下标都是从 1 开始)，</p>
<p>如第 5 行(下标从 1 开始) 0 1 1，从<em>右到左以</em>此表示 1 存在，2 存在，3不存在，</p>
<p>所以 011 表示点集{1,2}</p>
<p>这样我们就能用数字表示点集了</p>
<p>由于点集出现的顺序和图 3 的 j 所代表的顺序是不一致的，</p>
<p>所以需要重新绘制图 3 得到图 5，如下</p>
<p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20201123113626.png"></p>
<p>&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;图 5</p>
<p>该图也将那些需要赋值为空的数据，赋值为 ∞，表示该点不可达,</p>
<p>我们将在下面介绍如何找到这些点</p>
<h1 id="六、对图中数据生成的过程进行说明"><a href="#六、对图中数据生成的过程进行说明" class="headerlink" title="六、对图中数据生成的过程进行说明"></a>六、对图中数据生成的过程进行说明</h1><h2 id="1、必备基础知识-以下所有的讨论都是基于图-5-红框中的数据，行列下标都从-0-开始"><a href="#1、必备基础知识-以下所有的讨论都是基于图-5-红框中的数据，行列下标都从-0-开始" class="headerlink" title="1、必备基础知识 (以下所有的讨论都是基于图 5 红框中的数据，行列下标都从 0 开始)##"></a>1、必备基础知识 (以下所有的讨论都是基于图 5 红框中的数据，行列下标都从 0 开始)##</h2><p><strong>1、可以通过整数来表示一个点集</strong></p>
<p>在图 4 中，可以通过一个整数找到对应的点集，对应关系如下</p>
<pre><code>0 = 000 -&gt; &#123;&#125;        1 = 001 -&gt; &#123;1&#125;
2 = 010 -&gt; &#123;2&#125;        3 = 011 -&gt; &#123;1,2&#125;
4 = 100 -&gt; &#123;3&#125;        5 = 101 -&gt; &#123;1,3&#125;
6 = 110 - &gt; &#123;2,3&#125;    7 = 111 -&gt; &#123;1,2,3&#125;

这说明我可以通过一个整数，找到对应的点集
例如 整数 6 就可以找到点集 &#123;2,3&#125;

同样，给定一个点集，也可以找到他在数组中对应的横坐标
如点集 &#123;1,3&#125; 二进制表示形式为 101 = 5,所以他在二维数组的横坐标为 5</code></pre>
<p><strong>2、dp[i][j] = num 表示的是什么</strong></p>
<p>dp[i][j] = num 表示的是从点 i 出发，经过 j 所表示的点集 V’，</p>
<p>到达 0 的最短距离为 num，即 d(i,V’) 的最短距离为 num</p>
<p>如 d[3][3] = 14 表示从 3 出发，经过 3 所表示的点集{1,2}，</p>
<p>到达 0 的最短距离为 14，即 d(3,{1,2}) = 14</p>
<p><strong>3、如何判断整数 num 的二进制表示形式的第 j 位上为 1（以8位二进制数为例）</strong></p>
<pre><code>通过 num &amp; 0000 0001 
    结果为 0000 0001(2的0次方) num 的第 1 位上存在 1，为 0 不存在
通过 num &amp; 0000 0010 
    结果为 0000 0010(2的1次方) num 的第 2 位上存在 1，为 0 不存在
通过 num &amp; 0000 0100 
    结果为 0000 0100(2的2次方) num 的第 3 位上存在 1，为 0 不存在
通过 num &amp; 0000 1000 
    结果为 0000 1000(2的3次方) num 的第 4 位上存在 1，为 0 不存在
通过 num &amp; 0001 0000 
    结果为 0001 0000(2的4次方) num 的第 5 位上存在 1，为 0 不存在
通过 num &amp; 0010 0000 
    结果为 0010 0000(2的5次方) num 的第 6 位上存在 1，为 0 不存在
通过 num &amp; 0100 0000 
    结果为 0100 0000(2的6次方) num 的第 7 位上存在 1，为 0 不存在
通过 num &amp; 1000 0000 
    结果为 1000 0000(2的7次方) num 的第 8 位上存在 1，为 0 不存在</code></pre>
<p>例如对于 252 二进制为 1010 1010</p>
<pre><code>第 1 位    1010 1010 &amp; 0000 0001 = 0000 0000
第 2 位    1010 1010 &amp; 0000 0010 = 0000 0010
第 3 位    1010 1010 &amp; 0000 0100 = 0000 0000
第 4 位    1010 1010 &amp; 0000 1000 = 0000 1000
第 5 位    1010 1010 &amp; 0001 0000 = 0000 0000
第 6 位    1010 1010 &amp; 0010 0000 = 0010 0000
第 7 位    1010 1010 &amp; 0100 0000 = 0000 0000
第 8 位    1010 1010 &amp; 1000 0000 = 1000 0000
可以得出 252 这个整数的第 2，4，6，8 位上面各存在一个 1
                     而 1，3，5，7 位上面没有 1，与 252 的二进制表示形式相符</code></pre>
<p><strong>4、第 j 位上的 1 代表的是哪个点</strong></p>
<p><em>直接说结论，第 j 位上的 1 代表的是 j 这个点</em><br>因为从左到右以此表示的点为 1 2 3 4 ……</p>
<p>例如对于 252 二进制为 1010 1010</p>
<pre><code>1010 1010 第 2 位存在 1 ，代表点 2
1010 1010 第 4 位存在 1 ，代表点 4
1010 1010 第 6 位存在 1 ，代表点 6
1010 1010 第 8 位存在 1 ，代表点 8</code></pre>
<p><strong>5、如何判断点 k 是否存在于点集之中</strong></p>
<p>还是直接说结论，若点集的整数表示形式为 pointNum,则点 k 存在于点集中，</p>
<p>必有如下规则 pointNum &amp; 2^(k - 1) = 2^(k - 1)</p>
<p>例如对于 252 二进制为1010 1010</p>
<pre><code>1010 1010 &amp; 0000 0010 = 0000 0010 表示点 2 在点集中(0000 0010 = 2^(2 - 1))
1010 1010 &amp; 0000 1000 = 0000 1000 表示点 4 在点集中(0000 1000 = 2^(4 - 1))
1010 1010 &amp; 0010 0000 = 0010 0000 表示点 6 在点集中(0010 0000 = 2^(6 - 1))
1010 1010 &amp; 1000 0000 = 1000 0000 表示点 8 在点集中(1000 0000 = 2^(8 - 1))</code></pre>
<p><strong>6、从点集中取出一个点后形成一个新的点集</strong></p>
<p>可以先通过 <strong>5</strong> 判断在点集中是否存在一个点，若存在，则移除该点，</p>
<p>可以通过如下的方法从点集中移除该点</p>
<p>选择点集 V’ 用整数 pointNum 表示，判断一个点是否在点集里面，若存在，取出该点，</p>
<p>获得新的点集 V’’ 用整数 newPointNum 表示，获取一个新的点集的过程如下</p>
<pre><code>1、
    判断点集中是否存在 1 点
    pointNum &amp; 0000 0001(2的 0 次方) = 0000 0001
    获取新的点集(若点 1 存在点集中，则执行下列步骤，否则判断点 2)
    pointNum - 0000 0001 = newPointNum
    newPointNum 就是新的点集的整数表示形式
2、
    判断点集中是否存在 2 点
    pointNum &amp; 0000 0010(2的 1 次方) = 0000 0010
    获取新的点集(若第 2 位上存在 1，则执行下列步骤，否则判断点 3)
    pointNum - 0000 0010 = newPointNum
    newPointNum 就是新的点集的整数表示形式

                    ………………

i、
    判断点集中是否存在 i 点
    pointNum &amp; 1(i 个 0)(2的 i - 1 次方) = 1(i 个 0)
    获取新的点集(若第 i 位上存在 1，则执行下列步骤，否则判断点 i + 1)
    pointNum - 1(i 个 0) = newPointNum
    newPointNum 就是新的点集的整数表示形式</code></pre>
<p>例如对于 252 二进制为 1010 1010</p>
<pre><code>1010 1010 &amp; 0000 0001 = 0000 0000 第 1 位没有 1
1010 1010 &amp; 0000 0010 = 0000 0010 第 2 位存在 1
    1010 1010 - 0000 0010 = 1010 1000 新点集 1010 1000

1010 1010 &amp; 0000 0100 = 0000 0000 第 3 位没有 1
1010 1010 &amp; 0000 1000 = 0000 1000 第 4 位存在 1
    1010 1010 - 0000 1000 = 1010 0010 新点集 1010 0010

1010 1010 &amp; 0001 0000 = 0000 0000 第 5 位没有 1
1010 1010 &amp; 0010 0000 = 0010 0000 第 6 位存在 1
    1010 1010 - 0010 0000 = 1000 1010 新点集 1000 1010

1010 1010 &amp; 0100 0000 = 0000 0000 第 7 位没有 1
1010 1010 &amp; 1000 0000 = 1000 0000 第 8 位存在 1
    1010 1010 - 1000 0000 = 0010 1010 新点集 0010 1010</code></pre>
<p><strong>7、d(i,V’)的另外一种表示</strong></p>
<p>d(i,V’) 表示从点 i 出发,经过点集 V’ （使用整数 pointNum 表示）,</p>
<p>到达点 0 的最短距离（假设在 pointNum 的二进制表示形式中第 j 位上存在 1）</p>
<p>由 2 知，</p>
<p>dp[i][pointNum] 表示的是从点 i 出发，经过 pointNum 所表示的点集 V’，</p>
<p>到达点 0 的最短距离。</p>
<p>因此可以看出 d(i,V’) = dp[i][pointNum] (在 pointNum 能够表示 V’ 的情况下)</p>
<p><strong>8、拆分之后如何求最短路径</strong></p>
<p>最短路径 d(i,V’) 表示从点 i 出发,经过点集 V’ （使用整数 pointNum 表示）,</p>
<p>最后到达 0 的最短距离（假设在 pointNum 的二进制表示形式中第 j 位上存在 1）</p>
<p>我们可以通过 3 判断出在 pointNum 的第 j 位上存在 1,</p>
<p>然后我们通过 6 可以取出点 j </p>
<p>那么取出的点为 k （2 的 j - 1 次方），构成的新点集为 V’’ 用整数 newPointNum 表示</p>
<p>那么我们就获得了一条新的路径，新路径为 i -&gt; j -&gt; V’’ -&gt; 0</p>
<p>表示为 从点 i 出发，然后到达点 j，然后从点 j(2 的 j - 1 次方) 出发，</p>
<p>绕后经过点集 V’’,最后到达 0 点的最短路径，拆分为两部分</p>
<pre><code>第一部分 i -&gt; j 表示从 i 到 j 的距离，可以通过 C[i][j] 表示
第二部分 j -&gt; V&#39;&#39; -&gt; 0 j 经过点集 V&#39;&#39; 最后到达 0 的距离
可以通过 dp[j][newPointNum] 表示

由 2 可知，
dp[i][j] 表示的是从点 i 出发，经过 j 所表示的点集 V&#39;，然后到达 0 的最短距离，
那么 dp[j][newPointNum] 表示的是从点 j 出发，
经过 newPointNum 所表示的点集 V&#39;&#39;，然后到达 0 的最短距离,
所以 j -&gt; V&#39;&#39; -&gt; 0 可以使用 dp[j][newPointNum] 来表示</code></pre>
<p>因此,</p>
<p>d(i,V’) = dp[i][pointNum] = i -&gt; V’ -&gt; 0 = i -&gt; j -&gt; V’’ -&gt; 0 = C[i][j] + dp[j][newPointNum]</p>
<p>其中 i 是已知的不用求，k = 2^(j -1)，newPointNum = pointNum - k</p>
<p>pointNum 也是已知的，它是 V’ 的整数表示形式，</p>
<p>那么我们只需要找出 j 和 newPointNum 就行了</p>
<p>当我们知道了 j 时，k 就已知， newPointNum 也就已知，所以主要求 j</p>
<p>例如，由 6 可知，对于图 5 的 dp[3][3] = d(3,{1,2}) = 14</p>
<pre><code>其中 i = 3，pointNum = 011 = 3
    第 1 位 第 2 位 第 3 位
    011        011        011
   &amp;001       &amp;010       &amp;100
    001        010        000

得到 
    j1 = 1                               j2 = 2(表示第 1 2 位上存在 1)
    k1 = 2^(1-1) = 1                    k2 = 2^(2 - 1) = 2(表示点 1 2 在点集中)
    newPointNum1 = pointNum - k1 = 2    newPointNum2 = pointNum - k2 = 1
    C[3][1] + dp[1][2] = 7 + 8 = 15        C[3][2] + dp[2][1] = 5 + 9 = 14

两者取最小值为14，即为数组 dp[3][3] 所求
dp[1][2],dp[2][1]我们会在下面介绍如何得到的</code></pre>
<p><strong>9、如何找出无效的点</strong></p>
<p>无效的点分为两种情况，</p>
<ul>
<li>没有经过完整的点集</li>
<li>经过了重复的点</li>
</ul>
<p>将 dp 中的无效点进行分类，可以得到下列数据</p>
<pre><code>对于第一种在 dp 数组中有点
    dp[0][0],dp[0][1],dp[0][2]dp[0][3],dp[0][4],dp[0][5],dp[0][6]
    有这 7 个点
对于第 2 种情况在 dp 数组中有点
    dp[1][1],dp[1][3],dp[1][5],dp[1][7]
    dp[2][2],dp[2][3],dp[2][6],dp[2][7]
    dp[3][4],dp[3][5],dp[3][6],dp[3][7]
    有这 12 个点

第一种情况可以通过 if 判断，当在 0 行时，是否为最后一列，若不是最后一列，
就赋值为无效值
对于第二种情况,由于是经过了重复的点，通过 5 我们可以判断在点集中是否存在一个点，
这时候我们只需要知道判断的是哪个点在不在点击就可以了
对于dp[i][j] 来说，只需要判断 i 是不是在点集中就好了，

dp[1][1] k = 2^(1 - 1) = 1        j &amp; k = 001 &amp; 001 = 001
表示在点集中存在 1
dp[1][3] k = 2^(1 - 1) = 1        j &amp; k = 011 &amp; 001 = 001
表示在点击中存在 1
dp[1][5] k = 2^(1 - 1) = 1        j &amp; k = 101 &amp; 001 = 001
表示在点集中存在 1
dp[1][7] k = 2^(1 - 1) = 1        j &amp; k = 111 &amp; 001 = 001
表示在点集中存在 1

dp[2][2] k = 2^(2 - 1) = 2        j &amp; k = 010 &amp; 010 = 010
表示在点集中存在 2
dp[2][3] k = 2^(2 - 1) = 2        j &amp; k = 011 &amp; 010 = 010
表示在点击中存在 2
dp[2][6] k = 2^(2 - 1) = 2        j &amp; k = 110 &amp; 010 = 010
表示在点集中存在 2
dp[2][7] k = 2^(2 - 1) = 2        j &amp; k = 111 &amp; 010 = 010
表示在点集中存在 2

dp[3][4] k = 2^(3 - 1) = 4        j &amp; k = 100 &amp; 100 = 100
表示在点集中存在 3
dp[3][5] k = 2^(3 - 1) = 4        j &amp; k = 101 &amp; 100 = 100
表示在点击中存在 3
dp[3][6] k = 2^(3 - 1) = 4        j &amp; k = 110 &amp; 100 = 100
表示在点集中存在 3
dp[3][7] k = 2^(3 - 1) = 4        j &amp; k = 111 &amp; 100 = 100
表示在点集中存在 3</code></pre>
<h2 id="2、表格数据进行说明-只要我们能够找到-k-就可以解决问题了"><a href="#2、表格数据进行说明-只要我们能够找到-k-就可以解决问题了" class="headerlink" title="2、表格数据进行说明(只要我们能够找到 k 就可以解决问题了)##"></a>2、表格数据进行说明(只要我们能够找到 k 就可以解决问题了)##</h2><p>我们在 9、如何找出无效的点章节已经找出了无效的点，下面的讨论将会一笔带过，</p>
<p>它们分别是 </p>
<pre><code>dp[0][0],dp[0][1],dp[0][2],dp[0][3],
dp[0][4],dp[0][5],dp[0][6]
dp[1][1],dp[1][3],dp[1][5],dp[1][7]
dp[2][2],dp[2][3],dp[2][6],dp[2][7]
dp[3][4],dp[3][5],dp[3][6],dp[3][7]</code></pre>
<p>对红框框起来的数据进行讨论    </p>
<p>第 0 列数据</p>
<pre><code>dp[0][0] = ∞ 已经证明
dp[1][0] = d(1,&#123;&#125;) = C[1][0] = 5
dp[2][0] = d(2,&#123;&#125;) = C[2][0] = 6
dp[3][0] = d(3,&#123;&#125;) = C[3][0] = 3</code></pre>
<p>这一行的数据可以在初始化的时候给表格赋值初始值</p>
<p>第 1 列数据</p>
<pre><code>dp[0][1] = ∞ 已经证明
dp[1][1] = ∞ 已经证明
dp[2][1] = d(2,&#123;1&#125;)
    其中 i = 2，pointNum = 001 = 1
    第 1 位 第 2 位 第 3 位
    001        001        001
   &amp;001       &amp;010       &amp;100
    001        000        000 
得到 
    j = 1                               
    k = 2^(1 - 1) = 1                    
    newPointNum = pointNum - k = 0    
    C[2][1] + dp[1][0] = 4 + 5 = 9

dp[3][1] = d(3,&#123;1&#125;)
    其中 i = 3，pointNum = 001 = 1
    第 1 位 第 2 位 第 3 位
    001        001        001
   &amp;001       &amp;010       &amp;100
    001        000        000 
得到 
    j = 1                               
    k = 2^(1 - 1) = 1                    
    newPointNum = pointNum - k = 0    
    C[3][1] + dp[1][0] = 7 + 5 = 12    </code></pre>
<p>第 2 列数据</p>
<pre><code>dp[0][2] = ∞ 已经证明
dp[1][2] = d(1,&#123;2&#125;)
其中 i = 1，pointNum = 010 = 2
    第 1 位 第 2 位 第 3 位
    010        010        010
   &amp;001       &amp;010       &amp;100
    000        010        000 
得到 
    j = 2                               
    k = 2^(2 - 1) = 2                    
    newPointNum = pointNum - k = 0    
    C[1][2] + dp[2][0] = 2 + 6 = 8    
dp[2][2] = ∞ 已经证明
dp[3][2] = d(3,&#123;2&#125;)
    其中 i = 3，pointNum = 010 = 2
    第 1 位 第 2 位 第 3 位
    010        010        010
   &amp;001       &amp;010       &amp;100
    000        010        000 
得到 
    j = 2                               
    k = 2^(2 - 1) = 2                    
    newPointNum = pointNum - k = 0    
    C[3][2] + dp[2][0] = 5 + 6 = 11    </code></pre>
<p>第 4 列（注意这里是第 4 列，不是第 3 列，先把单个点集的列计算完）</p>
<pre><code>dp[0][4] = ∞ 已经证明
dp[1][4] = d(1,&#123;3&#125;)
    其中 i = 1，pointNum = 100 = 4
    第 1 位 第 2 位 第 3 位
    100        100        100
   &amp;001       &amp;010       &amp;100
    000        000        100 
得到 
    j = 3                               
    k = 2^(3 - 1) = 4                    
    newPointNum = pointNum - k = 0    
    C[1][3] + dp[3][0] = 3 + 3 = 6    
dp[2][4] = d(2,&#123;3&#125;)
    其中 i = 2，pointNum = 100 = 4
    第 1 位 第 2 位 第 3 位
    100        100        100
   &amp;001       &amp;010       &amp;100
    000        000        100 
得到 
    j = 3                               
    k = 2^(3 - 1) = 4                    
    newPointNum = pointNum - k = 0    
    C[2][3] + dp[3][0] = 2 + 3 = 5    
dp[3][4] = ∞ 已经证明</code></pre>
<p>第 3 列数据</p>
<pre><code>dp[0][3] = ∞ 已经证明
dp[1][3] = ∞ 已经证明
dp[2][3] = ∞ 已经证明
dp[3][3] = d(2,&#123;1,2&#125;) (上面也已经证明,摘录一下)
其中 i = 3，pointNum = 011 = 3
    第 1 位 第 2 位 第 3 位
    011        011        011
   &amp;001       &amp;010       &amp;100
    001        010        000

得到 
    j1 = 1                               j2 = 2(表示第 1 2 位上存在 1)
    k1 = 2^(1-1) = 1                    k2 = 2^(2 - 1) = 2(表示点 1 2 在点集中)
    newPointNum1 = pointNum - k1 = 2    newPointNum2 = pointNum - k2 = 1
    C[3][1] + dp[1][2] = 7 + 8 = 15        C[3][2] + dp[2][1] = 5 + 9 = 14

两者取最小值为14，即为数组 dp[3][3] 所求</code></pre>
<p>第 5 列数据</p>
<pre><code>dp[0][5] = ∞ 已经证明
dp[1][5] = ∞ 已经证明
dp[2][5] = d(2,&#123;1,3&#125;)
其中 i = 2，pointNum = 101 = 5
    第 1 位 第 2 位 第 3 位
    101        101        101
   &amp;001       &amp;010       &amp;100
    001        000        100

得到 
    j1 = 1                               j2 = 3(表示第 1 2 位上存在 1)
    k1 = 2^(1 - 1) = 1                    k2 = 2^(3 - 1) = 4(表示点 1 2 在点集中)
    newPointNum1 = pointNum - k1 = 4    newPointNum2 = pointNum - k2 = 1
    C[2][1] + dp[1][4] = 4 + 6 = 10        C[2][3] + dp[3][1] = 2 + 12 = 14
    两者取最小值为10，即为数组 dp[2][5] 所求
dp[3][5] = ∞ 已经证明</code></pre>
<p>第 6 列数据</p>
<pre><code>dp[0][6] = ∞ 已经证明
dp[1][6] = d(1,&#123;2,3&#125;)
其中 i = 1，pointNum = 110 = 6
    第 1 位 第 2 位 第 3 位
    110        110        110
   &amp;001       &amp;010       &amp;100
    000        010        100

得到 
    j1 = 2                               j2 = 3(表示第 1 2 位上存在 1)
    k1 = 2^(2 - 1) = 2                    k2 = 2^(3 - 1) = 4(表示点 1 2 在点集中)
    newPointNum1 = pointNum - k1 = 4    newPointNum2 = pointNum - k2 = 2
    C[1][2] + dp[2][4] = 2 + 5 = 7        C[1][3] + dp[3][2] = 3 + 11 = 14    
    两者取最小值为7，即为数组 dp[1][5] 所求
dp[2][6] = ∞ 已经证明
dp[3][6] = ∞ 已经证明</code></pre>
<p>第 7 列数据</p>
<pre><code>dp[0][7] = d(0,&#123;1,2,3&#125;) 这个是有数据的，因为经过了所有的点集
    其中 i = 0，pointNum = 111 = 7
    第 1 位 第 2 位 第 3 位
    111        111        111
   &amp;001       &amp;010       &amp;100
    001        010        100

得到 
    j1 = 1                   j2 = 2                                                j3 = 3
    k1 = 1                    k2 = 2                                                k3 = 4
    newPointNum1  = 6        newPointNum2 = 5                                    newPointNum2 = 3
    C[0][1] + dp[1][6] = 3 + 7 = 10        C[0][2] + dp[2][5] = 6 + 10 = 16  C[0][3] + dp[3][3] = 7 + 14 = 21
    三者取最小值为10，即为数组 dp[0][7] 所求</code></pre>
<p>整个数组就这样讲解完了</p>
<h1 id="七、代码讲解"><a href="#七、代码讲解" class="headerlink" title="七、代码讲解"></a>七、代码讲解</h1><p>通过上面的讲解，我们知道要先求点的数量为 0 1 2 3 的点集，但是点集在横坐标上并不是按照数量进行排序的，而是按照点集的整数表现形式，从小到大进行排序的。如图 5 的第 3 列和第 4 列(下标从 0 开始)。</p>
<p>因此我们需要找到点的数量为 0 1 2 3 的点集，可以通过下面的代码进行求解，第一张图片显示了如何求解一个整数中 1 的数量，第 2 张图片显示，构造一个列表，该列表存储了有 0 个 1，1 个 1，2 个 1，3 个 1 的都有哪些列表，最终构造的列表如下<br>0    [0]<br>1    [1,2,4]<br>2    [3,5,6]<br>3    [7]</p>
<p>所代表的含义是，</p>
<pre><code>0 个 1 的整数有 0
1 个 1 的整数有 1,2,4
2 个 1 的整数有 3,5,6
3 个 1 的整数有 7</code></pre>
<p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/20201206204556.png"></p>
<p>如何从一个整数中获取 1 的个数，可以查看这篇博客，此处的代码片段也是从该博客中截取的</p>
<p>地址：<a href="https://www.cnblogs.com/graphics/archive/2010/06/21/1752421.html">https://www.cnblogs.com/graphics/archive/2010/06/21/1752421.html</a></p>
<p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/20201204211841.png"></p>
<p>在开始有个初始化的操作，因为在初始的时候我们就已经知道了，点集中点的个数有多少，</p>
<p>对于图 1，有 4 个点，去掉起始点 0(因为起始点不能加入到点集中)，剩余的 3 个点，</p>
<p>共有 4 种情况，0 个 1，1 个 1，2 个 1，3 个 1。</p>
<p><img src="https://clay-nuyoah.github.io/post-images/2020-11-25/20201204212242.png"></p>
<pre><code>1 处就是把无效数据给赋值成 ∞，这里用 Integer.MAX_VALUE 代替
2 处判断一个点是都存在点集中，并且拆出来的这个点能够由 i 点可达
3 处进行点的拆分
4 处当拆分出多个点的时候，取其最小值</code></pre>
<h1 id="八、代码"><a href="#八、代码" class="headerlink" title="八、代码"></a>八、代码</h1><p>代码地址:<a href="https://github.com/clay-nuyoah/TSP" title="TSP代码地址">https://github.com/clay-nuyoah/TSP</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package tsp;</span><br><span class="line"></span><br><span class="line">import java.util.ArrayList;</span><br><span class="line">import java.util.List;</span><br><span class="line"></span><br><span class="line">&#x2F;**</span><br><span class="line"> * TSP 算法旅行推销员问题</span><br><span class="line"> * 给定一系列城市和每对城市之间的距离，求解访问每一座城市一次并回到起始城市的最短回路。</span><br><span class="line"> *&#x2F;</span><br><span class="line">public class TspDp &#123;</span><br><span class="line">	    public static void main(String[] args) &#123;</span><br><span class="line">	        int[][] nums &#x3D; &#123;&#123;Integer.MAX_VALUE, 3, 6, 7&#125;,</span><br><span class="line">	                &#123;5, Integer.MAX_VALUE, 2, 3&#125;,</span><br><span class="line">	                &#123;6, 4, Integer.MAX_VALUE, 2&#125;,</span><br><span class="line">	                &#123;3, 7, 5, Integer.MAX_VALUE&#125;</span><br><span class="line">	        &#125;;</span><br><span class="line">	        TspDp tspDp &#x3D; new TspDp();</span><br><span class="line">	        int minCost &#x3D; tspDp.getMinCost(nums, nums.length);</span><br><span class="line">	        System.out.println(minCost);</span><br><span class="line">	    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 获取最小花费</span><br><span class="line">     *</span><br><span class="line">     * @param nums 节点之间的花费</span><br><span class="line">     * @param n    节点个数</span><br><span class="line">     * @return</span><br><span class="line">     *&#x2F;</span><br><span class="line">    public int getMinCost(int[][] nums, int n) &#123;</span><br><span class="line">        int row &#x3D; n;</span><br><span class="line">        int col &#x3D; (int) Math.pow(2, n - 1);</span><br><span class="line">        int[][] dp &#x3D; new int[row][col];</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; countLists &#x3D; getCountLists(row, col);</span><br><span class="line"></span><br><span class="line">        init(dp, nums, row);</span><br><span class="line"></span><br><span class="line">			&#x2F;&#x2F;遍历数组中的每一个数</span><br><span class="line">        for (int j &#x3D; 1; j &lt; row; j++) &#123; &#x2F;&#x2F;循环遍历 数量 数组</span><br><span class="line">            List&lt;Integer&gt; countList &#x3D; countLists.get(j);</span><br><span class="line">				&#x2F;&#x2F;用来确定横坐标</span><br><span class="line">            for (int k &#x3D; 0; k &lt; countList.size(); k++) &#123;</span><br><span class="line">				&#x2F;&#x2F;纵坐标       </span><br><span class="line">                for (int i &#x3D; 0; i &lt; row; i++) &#123;           </span><br><span class="line">                    Integer num &#x3D; countList.get(k);</span><br><span class="line">                    int currentMinCost &#x3D; getCurrentMinCost(i, num, row, col, nums, dp);</span><br><span class="line">                    dp[i][num] &#x3D; currentMinCost;</span><br><span class="line"></span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        for (int i &#x3D; 0; i &lt; row; i++) &#123;</span><br><span class="line">            for (int j &#x3D; 0; j &lt; col; j++) &#123;</span><br><span class="line">                System.out.print(dp[i][j] + &quot;\t&quot;);</span><br><span class="line">            &#125;</span><br><span class="line">            System.out.println();</span><br><span class="line">        &#125;</span><br><span class="line">        return dp[0][col - 1];</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 获取一个 list</span><br><span class="line">     * list[0] 表示 从 0 ~ col 中二进制 拥有 0 个 1 的数字组成的集合</span><br><span class="line">     * list[1] 表示 从 0 ~ col 中二进制 拥有 1 个 1 的数字组成的集合</span><br><span class="line">     *</span><br><span class="line">     * @param row</span><br><span class="line">     * @param col</span><br><span class="line">     * @return</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private List&lt;List&lt;Integer&gt;&gt; getCountLists(int row, int col) &#123;</span><br><span class="line">        List&lt;List&lt;Integer&gt;&gt; countLists &#x3D; new ArrayList&lt;&gt;();</span><br><span class="line">        for (int i &#x3D; 0; i &lt; row; i++) &#123;</span><br><span class="line">            countLists.add(new ArrayList&lt;&gt;());</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        for (int i &#x3D; 0; i &lt; col; i++) &#123;</span><br><span class="line">            int count &#x3D; bitCount(i);</span><br><span class="line">            countLists.get(count).add(i);</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        return countLists;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 获取一个整数 二进制 1 的个数</span><br><span class="line">     *</span><br><span class="line">     * @param n</span><br><span class="line">     * @return</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private int bitCount(int n) &#123;</span><br><span class="line">        int c;</span><br><span class="line">        for (c &#x3D; 0; n !&#x3D; 0; ++c) &#123;</span><br><span class="line">            n &amp;&#x3D; (n - 1); &#x2F;&#x2F; 清除最低位的1</span><br><span class="line">        &#125;</span><br><span class="line">        return c;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 获取当前条件下的最小花费</span><br><span class="line">     *</span><br><span class="line">     * @param i</span><br><span class="line">     * @param num</span><br><span class="line">     * @param row</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private int getCurrentMinCost(int i, int num, int row, </span><br><span class="line">			int col, int[][] nums, int[][] dp) &#123;</span><br><span class="line">        int pow &#x3D; (int) Math.pow(2, i - 1);</span><br><span class="line"></span><br><span class="line">        if (num !&#x3D; col - 1 &amp;&amp; (i &#x3D;&#x3D; 0 || (pow &amp; num) &#x3D;&#x3D; pow)) &#123;</span><br><span class="line">            return Integer.MAX_VALUE;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        int min &#x3D; Integer.MAX_VALUE;</span><br><span class="line">        for (int j &#x3D; 1; j &lt; row; j++) &#123;</span><br><span class="line">            int pow1 &#x3D; (int) Math.pow(2, j - 1);</span><br><span class="line">            if ((pow1 &amp; num) &#x3D;&#x3D; pow1 &amp;&amp; nums[i][j] !&#x3D; Integer.MAX_VALUE) &#123;</span><br><span class="line">                int y &#x3D; num - pow1;</span><br><span class="line">                int cost &#x3D; nums[i][j] + dp[j][y];</span><br><span class="line">                if (cost &lt; min) &#123;</span><br><span class="line">                    min &#x3D; cost;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return min;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    &#x2F;**</span><br><span class="line">     * 初始化 dp</span><br><span class="line">     *</span><br><span class="line">     * @param dp</span><br><span class="line">     * @param nums</span><br><span class="line">     * @param row</span><br><span class="line">     *&#x2F;</span><br><span class="line">    private void init(int[][] dp, int[][] nums, int row) &#123;</span><br><span class="line">        for (int i &#x3D; 0; i &lt; row; i++) &#123;</span><br><span class="line">            dp[i][0] &#x3D; nums[i][0];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>dp</tag>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>TSP</tag>
      </tags>
  </entry>
  <entry>
    <title>动态规划求解判断子序列问题</title>
    <url>/2020/11/24/dong-tai-gui-hua-qiu-jie-pan-duan-zi-xu-lie-wen-ti/</url>
    <content><![CDATA[<p>leetcode 题库 392 判断子序列</p>
<a id="more"></a>
<p>一、<br>①<br>        <strong>s 中的前 i 个字符是   t 中前 j 个字符的子序列，则 dp[i][j] = true<br>          s 中的前 i 个字符不是 t 中前 j 个字符的子序列，则 dp[i][j] = false</strong></p>
<p>二、<br><strong><em># 1、比较的 s 的第 i 个字符和 t 的第 j 个字符相等</em></strong></p>
<h1 id="s-的前-i-1-项是-t-的前-j-1-项的子序列"><a href="#s-的前-i-1-项是-t-的前-j-1-项的子序列" class="headerlink" title="**   s 的前 i - 1 项是 t 的前 j - 1 项的子序列**"></a>**   s 的前 i - 1 项是 t 的前 j - 1 项的子序列**</h1><pre><code>    s = &quot;abc&quot;
    t = &quot;aebc&quot;
    i = 3
    j = 4
    此时对于字符串 s 和 t 来说，最后两个字符相等
    由于 &quot;ab&quot; 是 &quot;aeb&quot; 的子序列，由 ① 得 dp[2][3] = true
    由于 s 和 t 的最后两个字符相等
    所以 &quot;abc&quot; 是 &quot;aebc&quot; 的子序列，由 ① 的 dp[3][4] = true</code></pre>
<h1 id="s-的前-i-1-项不是-t-的前-j-1-项的子序列"><a href="#s-的前-i-1-项不是-t-的前-j-1-项的子序列" class="headerlink" title="**   s 的前 i - 1 项不是 t 的前 j - 1 项的子序列**"></a>**   s 的前 i - 1 项不是 t 的前 j - 1 项的子序列**</h1><pre><code>    s = &quot;adc&quot;
    t = &quot;aebc&quot;
    i = 3
    j = 4
    此时对于字符串 s 和 t 来说,最后两个字符相等
    由于 &quot;ad&quot; 不是 &quot;aeb&quot; 的子序列，由 ① 得 dp[2][3] = false
    即使 s 和 t 的最后两个字符相等
    &quot;adc&quot; 也不是 &quot;aebc&quot; 的子序列，由 ① 的 dp[3][4] = false</code></pre>
<p><strong><em># 2、比较的 s 的第 i 个字符和 t 的第 j 个字符不相等</em></strong></p>
<h1 id="s-的前-i-项是-t-的前-j-1-项的子序列"><a href="#s-的前-i-项是-t-的前-j-1-项的子序列" class="headerlink" title="**   s 的前 i 项是 t 的前 j - 1 项的子序列**"></a>**   s 的前 i 项是 t 的前 j - 1 项的子序列**</h1><pre><code>    s = &quot;ade&quot;
    t = &quot;abdef&quot;
    i = 3
    j = 5
    此时对于字符串 s 和 t 来说,最后两个字符不相等
    由于 &quot;ade&quot; 是 &quot;abde&quot; 的子序列，由 ① 得 dp[3][4] = true
    那么为 t 添加一个不相等的字符之后，&quot;ade&quot; 也是 &quot;abdef&quot; 的子序列，由 ① 得 dp[3][5] = true</code></pre>
<h1 id="s-的前-i-项不是-t-的前-j-1-项的子序列"><a href="#s-的前-i-项不是-t-的前-j-1-项的子序列" class="headerlink" title="**   s 的前 i 项不是 t 的前 j - 1 项的子序列**"></a>**   s 的前 i 项不是 t 的前 j - 1 项的子序列**</h1><pre><code>    s = &quot;ace&quot;
    t = &quot;abdef&quot;
    i = 3
    j = 5
    此时对于字符串 s 和 t 来说,最后两个字符不相等
    由于 &quot;ace&quot; 不是 &quot;abde&quot; 的子序列，由 ① 得 dp[3][4] = false
    那么为 t 添加一个不相等的字符之后 &quot;ade&quot; 也不是 &quot;abdef&quot; 的子序列，由 ① 得 dp[3][5] = false</code></pre>
<p>三、</p>
<hr>
<p>综上所述：</p>
<pre><code>    当最后两个字符相等的时，即 dp[i][j] = dp[i - 1][j - 1]
            当 s 的长度为 i - 1,t 的长度为 j - 1 时
            若 s 为   t 的子序列，则当 s 的长度为 i ,t 的长度为 j 时，s 也为   t 的子序列
            若 s 不为 t 的子序列，则当 s 的长度为 i ,t 的长度为 j 时，s 也不为 t 的子序列
    当最后两个字符串不相等时，即 dp[i][j] = dp[i][j - 1]
            当 s 的长度为 i,t 的长度为 j - 1 时
            若 s 为   t 的子序列，则为 t 则加一个字符之后，s 的长度为 i ,t 的长度为 j 时，s 也为   t 的子序列
            若 s 不为 t 的子序列，则为 t 则加一个字符之后，s 的长度为 i ,t 的长度为 j 时，s 也为   t 的子序列</code></pre>
<hr>
<p>四、<br><img src="https://raw.githubusercontent.com/clay-nuyoah/clay-nuyoah.github.io/master/post-images/2020-11-25/1607932741-roeLGk-%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20201214155736.png" alt="微信截图_20201214155736.png"></p>
<p>第一步：</p>
<p>初始化：</p>
<pre><code>            将第一行和第一列的数据进行初始化
            第一行表示 t 为空串时，必为 s 的子序列
            第一列表示 s 为空串时，t 必部位 s 的子序列</code></pre>
<p>第二步：</p>
<p>由左到右，由上到下为 dp 数组赋值</p>
<pre><code>            第二行数据：
            dp[1][1] 表示,当 s = &quot;a&quot; t = &quot;a&quot; 时，s 是否为 t 的子序列
            此时两个字符串的最后两个字符相等，所以查看 s=&quot;&quot; t = &quot;&quot; (dp[0][0] 表示)时，s 是否为 t 的子序列
            由于 dp[0][0] = true 所以 s = &quot;a&quot; t = &quot;a&quot; 时 s 为 t 的子序列 即 d[1][1] = true

            dp[1][2] 表示,当 s = &quot;a&quot; t = &quot;ah&quot; 时，s 是否为 t 的子序列
            此时两个字符串的最后两个字符不相等，所以查看 s=&quot;a&quot; t = &quot;a&quot; (dp[1][1] 表示)时，s 是否为 t 的子序列
            由于 dp[0][0] = true s 为 t 的子序列，为 t 添加一个字符之后 s=&quot;a&quot; t = &quot;ah&quot; 此时s 仍为 t 的子序列 即 dp[1][2] = true

            dp[1][3] 表示,当 s = &quot;a&quot; t = &quot;ahb&quot; 时，s 是否为 t 的子序列
            此时两个字符串的最后两个字符不相等，所以查看 s=&quot;a&quot; t = &quot;ah&quot; (dp[1][2] 表示)时，s 是否为 t 的子序列
            由于 dp[1][2] = true s 为 t 的子序列，为 t 添加一个字符之后 s=&quot;a&quot; t = &quot;ahb&quot; 此时s 仍为 t 的子序列 即 dp[1][3] = true

            dp[1][4],dp[1][5],dp[1][6] 同理

            第三行数据：
            dp[2][1] 表示,当 s = &quot;ab&quot; t = &quot;a&quot; 时，s 是否为 t 的子序列
            此时两个字符串的最后两个字符不相等，所以查看 s=&quot;ab&quot; t = &quot;&quot; (dp[2][0] 表示)时，s 是否为 t 的子序列
            由于 dp[2][0] = false s 不为 t 的子序列，为 t 添加一个字符之后 s=&quot;ab&quot; t = &quot;a&quot; 此时s 不为 t 的子序列 即 dp[2][1] = false

            dp[2][2] 表示,当 s = &quot;ab&quot; t = &quot;ah&quot; 时，s 是否为 t 的子序列
            此时两个字符串的最后两个字符不相等，所以查看 s=&quot;ab&quot; t = &quot;a&quot; (dp[2][1] 表示)时，s 是否为 t 的子序列
            由于 dp[2][1] = false s 不为 t 的子序列，为 t 添加一个字符之后 s=&quot;ab&quot; t = &quot;ah&quot; 此时s 不为 t 的子序列 即 dp[2][2] = false

            dp[2][3] 表示 当 s = &quot;ab&quot; t = &quot;ahb&quot; 时，s 是否为 t 的子序列
            此时两个字符串的最后两个字符相等，所以查看 s=&quot;a&quot; t = &quot;ah&quot; (dp[1][2] 表示)时，s 是否为 t 的子序列
            由于 dp[1][2] = true s 为 t 的子序列， 当 s=&quot;ab&quot; t = &quot;ahb&quot; 此时s 为 t 的子序列 即 dp[2][3] = true
            dp[2][4],dp[2][5],dp[2][6] 同理

            第四行也可以用一样的方法进行求解</code></pre>
<p>五、代码如下</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">class Solution &#123;</span><br><span class="line">   public boolean isSubsequence(String s, String t) &#123;</span><br><span class="line">        int sLen &#x3D; s.length(), tLen &#x3D; t.length();</span><br><span class="line">        if (sLen &gt; tLen) return false;</span><br><span class="line">        if (sLen &#x3D;&#x3D; 0) return true;</span><br><span class="line">        boolean[][] dp &#x3D; new boolean[sLen + 1][tLen + 1];</span><br><span class="line">        for (int j &#x3D; 0; j &lt;&#x3D; tLen; j++) &#123;</span><br><span class="line">            dp[0][j] &#x3D; true;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        for (int i &#x3D; 1; i &lt;&#x3D; sLen; i++) &#123;</span><br><span class="line">            for (int j &#x3D; 1; j &lt;&#x3D; tLen; j++) &#123;</span><br><span class="line">                if (s.charAt(i - 1) &#x3D;&#x3D; t.charAt(j - 1)) &#123;</span><br><span class="line">                    dp[i][j] &#x3D; dp[i - 1][j - 1];</span><br><span class="line">                &#125; else &#123;</span><br><span class="line">                    dp[i][j] &#x3D; dp[i][j - 1];</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        return dp[sLen][tLen];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <tags>
        <tag>dp</tag>
        <tag>动态规划</tag>
        <tag>算法</tag>
        <tag>字符串</tag>
      </tags>
  </entry>
  <entry>
    <title>连续子数组的最大和</title>
    <url>/2018/12/12/lian-xu-zi-shu-zu-de-zui-da-he/</url>
    <content><![CDATA[<p>leetcode 题库 43 最大子序和</p>
<a id="more"></a>

<p>题目描述：</p>
<p>给定一个整数数组 nums ，找到一个具有最大和的连续子数组（子数组最少包含一个元素），</p>
<p>返回其最大和。</p>
<p>示例:</p>
<p>输入: [-2,1,-3,4,-1,2,1,-5,4]</p>
<p>输出: 6</p>
<p>解释: 连续子数组 [4,-1,2,1] 的和最大，为 6。</p>
<p>来源：力扣（LeetCode）</p>
<p>链接：<a href="https://leetcode-cn.com/problems/maximum-subarray">https://leetcode-cn.com/problems/maximum-subarray</a></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">public int maxSubArray(int[] nums) &#123;</span><br><span class="line">       int sum &#x3D; 0;</span><br><span class="line">       int max &#x3D; Integer.MIN_VALUE;</span><br><span class="line">       for (int num : nums) &#123;</span><br><span class="line">           sum +&#x3D; num;</span><br><span class="line">           if (max &lt; sum) &#123;</span><br><span class="line">               max &#x3D; sum;</span><br><span class="line">           &#125;</span><br><span class="line"></span><br><span class="line">           if (sum &lt; 0) &#123;</span><br><span class="line">               sum &#x3D; 0;</span><br><span class="line">           &#125;</span><br><span class="line">       &#125;</span><br><span class="line">       return max;</span><br><span class="line">   &#125;</span><br></pre></td></tr></table></figure>
<p>每一次相加的时候，只要是 sum 的值大于零，就相加的数来说就是把相加的数增大，</p>
<p>而相加的数是不知道的，结果也不知道，所以有可能取到最大值。</p>
]]></content>
      <tags>
        <tag>算法</tag>
        <tag>数组</tag>
        <tag>leetcode</tag>
      </tags>
  </entry>
</search>
